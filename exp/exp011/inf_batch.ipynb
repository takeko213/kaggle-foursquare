{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# exp011"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "#MODE = 'local_train'\n",
    "MODE = 'kaggle_inference'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "exp_name = 'exp011'\n",
    "memo = 'bert_vecのsvd'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import sys\n",
    "import gc\n",
    "\n",
    "sys.path.append('/home/kaggler/.local/lib/python3.8/site-packages')\n",
    "from dotenv import load_dotenv\n",
    "load_dotenv\n",
    "sys.path.append(os.getenv('UTILS_PATH'))\n",
    "import line_notify\n",
    "import slack_notify\n",
    "\n",
    "from cuml import ForestInference\n",
    "import treelite\n",
    "    \n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt; plt.style.use(\"ggplot\")\n",
    "import seaborn as sns\n",
    "from sklearn.metrics.pairwise import haversine_distances\n",
    "from sklearn.model_selection import GroupKFold, KFold, StratifiedKFold\n",
    "from sklearn.metrics import accuracy_score\n",
    "from sklearn.decomposition import TruncatedSVD\n",
    "import lightgbm as lgb\n",
    "import itertools\n",
    "from scipy.spatial.distance import canberra\n",
    "from sklearn.neighbors import KNeighborsRegressor\n",
    "import functools\n",
    "import multiprocessing\n",
    "import Levenshtein\n",
    "import difflib\n",
    "import pickle\n",
    "from tqdm import tqdm\n",
    "%load_ext Cython\n",
    "\n",
    "from transformers import DistilBertModel, DistilBertTokenizer\n",
    "from torch.utils.data import DataLoader, Dataset\n",
    "import torch.nn.functional as F\n",
    "import torch.nn as nn\n",
    "import torch\n",
    "\n",
    "from sklearn.metrics.pairwise import cosine_similarity"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "# directry_setting\n",
    "if MODE == 'local_train':\n",
    "    INPUT_DIR = os.getenv('INPUT_DIR')\n",
    "    OUTPUT_DIR = os.getenv('OUTPUT_DIR')\n",
    "    MODEL_DIR = os.getenv('OUTPUT_DIR')\n",
    "    BERT_MODEL = \"distilbert-base-uncased\"\n",
    "    os.makedirs(OUTPUT_DIR + exp_name, exist_ok=True)\n",
    "\n",
    "elif MODE == 'kaggle_inference':\n",
    "    INPUT_DIR = os.getenv('INPUT_DIR')\n",
    "    OUTPUT_DIR = os.getenv('OUTPUT_DIR')\n",
    "    MODEL_DIR = os.getenv('OUTPUT_DIR') + \"exp011/\"\n",
    "    BERT_MODEL = \"distilbert-base-uncased\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "# CONFIG\n",
    "SEED = 42\n",
    "N_NEIGHBORS = 10\n",
    "N_SPLITS = 5\n",
    "PROB_TH = 0.5\n",
    "MAX_LEN = 32\n",
    "BS = 512\n",
    "NW = 2\n",
    "SVD_N_COMP = 50"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "class Cat2VecModel(nn.Module):\n",
    "    def __init__(self):\n",
    "        super(Cat2VecModel, self).__init__()\n",
    "        self.distill_bert = DistilBertModel.from_pretrained(BERT_MODEL)\n",
    "        \n",
    "    def forward(self, ids, mask):\n",
    "        x = self.distill_bert(ids, mask)[0]\n",
    "        x = F.normalize((x[:, 1:, :]*mask[:, 1:, None]).mean(axis=1))\n",
    "        return x\n",
    "\n",
    "class InferenceDataset(Dataset):\n",
    "    \n",
    "    def __init__(self, df, max_len, col):\n",
    "        super().__init__()\n",
    "        self.df = df.reset_index(drop=True)\n",
    "        self.max_len = max_len\n",
    "        self.tokenizer = DistilBertTokenizer.from_pretrained(BERT_MODEL, do_lower_case=True)\n",
    "        self.col = col\n",
    "\n",
    "    def __getitem__(self, index):\n",
    "        row = self.df.iloc[index]\n",
    "        \n",
    "        inputs = self.tokenizer.encode_plus(\n",
    "            row[self.col],\n",
    "            None,\n",
    "            add_special_tokens=True,\n",
    "            max_length=MAX_LEN,\n",
    "            padding=\"max_length\",\n",
    "            return_token_type_ids=True,\n",
    "            truncation=True\n",
    "        )\n",
    "        ids = torch.LongTensor(inputs['input_ids'])\n",
    "        mask = torch.LongTensor(inputs['attention_mask'])\n",
    "\n",
    "        return ids, mask\n",
    "\n",
    "    def __len__(self):\n",
    "        return self.df.shape[0]\n",
    "\n",
    "def inference(ds):\n",
    "    cat2vec_model = Cat2VecModel()\n",
    "    cat2vec_model = cat2vec_model.cuda()\n",
    "    \n",
    "    loader = DataLoader(ds, batch_size=BS, shuffle=False, num_workers=NW,\n",
    "                        pin_memory=False, drop_last=False)\n",
    "    \n",
    "    vs = []\n",
    "    with torch.no_grad():\n",
    "        for idx, (ids, masks) in enumerate(loader):\n",
    "            v = cat2vec_model(ids.cuda(), masks.cuda()).detach().cpu().numpy()\n",
    "            vs.append(v)\n",
    "    return np.concatenate(vs)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "def make_bert_vec(df, col):\n",
    "    cat_df = df[[col]].drop_duplicates()\n",
    "    cat_df[col] = cat_df[col].fillna(\"null\")\n",
    "\n",
    "    cat_ds = InferenceDataset(cat_df, max_len=MAX_LEN, col=col)\n",
    "    V = inference(cat_ds)\n",
    "    svd = TruncatedSVD(n_components=SVD_N_COMP, random_state=SEED)\n",
    "    V = svd.fit_transform(V)\n",
    "    V = V.astype(\"float16\")\n",
    "    bert_vec = {k:v for k,v in zip(cat_df[col].values, V)}\n",
    "    return bert_vec"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "def preprocess(df):\n",
    "    columns = ['id', 'name', 'address', 'city', 'state',\n",
    "        'zip', 'country', 'url', 'phone', 'categories']\n",
    "    for c in columns:\n",
    "        if c != \"id\":\n",
    "            df[c] = df[c].astype(str).str.lower()\n",
    "    return df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "def extract_candidate(df):\n",
    "    dfs = []\n",
    "    candidates = pd.DataFrame()\n",
    "    for country, country_df in tqdm(df.groupby(\"country\")):\n",
    "        country_df = country_df.reset_index(drop=True)\n",
    "        \n",
    "        knn = KNeighborsRegressor(n_neighbors=min(len(country_df), N_NEIGHBORS), \n",
    "                                  metric='haversine', n_jobs=-1)\n",
    "        knn.fit(country_df[['latitude','longitude']], country_df.index)\n",
    "        dists, nears = knn.kneighbors(country_df[['latitude','longitude']], return_distance=True)\n",
    "        \n",
    "        k = min(len(country_df), N_NEIGHBORS)\n",
    "        country_df['match_id'] = country_df['id'].values[nears[:, :k]].tolist()\n",
    "        country_df['d_near'] = dists[:, :k].tolist()\n",
    "        dfs.append(country_df[['id','match_id','d_near']])\n",
    "    df = pd.concat(dfs).reset_index(drop=True)\n",
    "    return df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "def add_orgin_data(df, org_df):\n",
    "    df = df.explode(['match_id','d_near'])\n",
    "    df = df.loc[df['id'] != df['match_id']].copy()\n",
    "    df = df.merge(org_df, on='id')\n",
    "    df = df.merge(org_df.add_prefix('match_'), on='match_id')\n",
    "    df = df.reset_index(drop=True)\n",
    "    return df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "# https://www.kaggle.com/code/columbia2131/foursquare-iou-metrics\n",
    "def get_id2poi(input_df: pd.DataFrame) -> dict:\n",
    "    return dict(zip(input_df['id'], input_df['point_of_interest']))\n",
    "\n",
    "def get_poi2ids(input_df: pd.DataFrame) -> dict:\n",
    "    return input_df.groupby('point_of_interest')['id'].apply(set).to_dict()\n",
    "\n",
    "def get_score(input_df: pd.DataFrame, org_data):\n",
    "    scores = []\n",
    "    id2poi = get_id2poi(org_data)\n",
    "    poi2ids = get_poi2ids(org_data)\n",
    "    for id_str, matches in zip(input_df['id'].to_numpy(), input_df['matches'].to_numpy()):\n",
    "        targets = poi2ids[id2poi[id_str]]\n",
    "        preds = set(matches.split())\n",
    "        score = len((targets & preds)) / len((targets | preds))\n",
    "        scores.append(score)\n",
    "    scores = np.array(scores)\n",
    "    return scores.mean()\n",
    "\n",
    "def calc_max_score(tr_data, org_data):\n",
    "    train_candidate = pd.DataFrame()\n",
    "    train_candidate['id'] = org_data['id'].unique()\n",
    "    train_candidate['matches'] = org_data['id'].unique()\n",
    "    idx = tr_data['point_of_interest']==tr_data['match_point_of_interest']\n",
    "    train_match = tr_data.loc[idx].groupby('id')['match_id'].apply(list).map(\" \".join).reset_index()\n",
    "    train_match.columns = ['id','candidates']\n",
    "    train_candidate = train_candidate.merge(train_match, on = 'id', how = 'left')\n",
    "    idx = ~train_candidate['candidates'].isna()\n",
    "    train_candidate.loc[idx, \"matches\"] += \" \" + train_candidate.loc[idx, \"candidates\"]\n",
    "    score = get_score(train_candidate, org_data)\n",
    "    print('1st_stage_max_score : ' + '{:.5f}'.format(score))\n",
    "    return score"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "%%cython\n",
    "def LCS(str S, str T):\n",
    "    cdef int i, j\n",
    "    cdef list dp = [[0] * (len(T) + 1) for _ in range(len(S) + 1)]\n",
    "    for i in range(len(S)):\n",
    "        for j in range(len(T)):\n",
    "            dp[i + 1][j + 1] = max(dp[i][j] + (S[i] == T[j]), dp[i + 1][j], dp[i][j + 1], dp[i + 1][j + 1])\n",
    "    return dp[len(S)][len(T)]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "def _add_distance_features(args):\n",
    "    _, df = args\n",
    "\n",
    "    columns = ['name', 'address', 'city', 'state',\n",
    "               'zip', 'country', 'url', 'phone', 'categories']\n",
    "\n",
    "    for c in columns:\n",
    "        geshs = []\n",
    "        levens = []\n",
    "        jaros = []\n",
    "        lcss = []\n",
    "\n",
    "        for str1, str2 in df[[f\"{c}\", f\"match_{c}\"]].values.astype(str):\n",
    "            if str1==str1 and str2==str2:\n",
    "                geshs.append(difflib.SequenceMatcher(None, str1, str2).ratio())\n",
    "                levens.append(Levenshtein.distance(str1, str2))\n",
    "                jaros.append(Levenshtein.jaro_winkler(str1, str2))\n",
    "                lcss.append(LCS(str(str1), str(str2)))\n",
    "            else:\n",
    "                geshs.append(-1)\n",
    "                levens.append(-1)\n",
    "                jaros.append(-1)\n",
    "        df[f\"match_{c}_gesh\"] = geshs\n",
    "        df[f\"match_{c}_gesh\"] = df[f\"match_{c}_gesh\"].astype(np.float16)\n",
    "        df[f\"match_{c}_leven\"] = levens\n",
    "        df[f\"match_{c}_leven\"] = df[f\"match_{c}_leven\"].astype(np.float16)\n",
    "        df[f\"match_{c}_jaro\"] = jaros\n",
    "        df[f\"match_{c}_jaro\"] = df[f\"match_{c}_jaro\"].astype(np.float16)\n",
    "        df[f\"match_{c}_lcs\"] = lcss\n",
    "        df[f\"match_{c}_lcs\"] = df[f\"match_{c}_lcs\"].astype(np.float16)\n",
    "            \n",
    "        if not c in ['country', \"phone\", \"zip\"]:\n",
    "            df[f\"match_{c}_len\"] = df[f\"match_{c}\"].astype(str).map(len)\n",
    "            df[f\"match_{c}_nleven\"] = df[f\"match_{c}_leven\"] / df[f\"match_{c}_len\"]\n",
    "            df[f\"match_{c}_nleven\"] = df[f\"match_{c}_nleven\"].astype(np.float16)\n",
    "            df[f\"match_{c}_nlcsi\"] = df[f\"match_{c}_lcs\"] / df[f\"match_{c}_len\"]\n",
    "            df[f\"match_{c}_nlcsi\"].astype(np.float16)\n",
    "\n",
    "    return df\n",
    "\n",
    "\n",
    "def add_distance_features(df):\n",
    "    processes = multiprocessing.cpu_count()\n",
    "    with multiprocessing.Pool(processes=processes) as pool:\n",
    "        dfs = pool.imap_unordered(_add_distance_features, df.groupby('country'))\n",
    "        dfs = tqdm(dfs)\n",
    "        dfs = list(dfs)\n",
    "    df = pd.concat(dfs)\n",
    "    return df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "def add_bert_sim_features(df, bert_vec, col):\n",
    "    bert_sim_list = []\n",
    "    for str1, str2 in df[[f\"{col}\", f\"match_{col}\"]].values.astype(str):\n",
    "        if str1==str1 and str2==str2:\n",
    "            bert_sim = cosine_similarity(bert_vec[str1].reshape(1, -1), bert_vec[str2].reshape(1, -1))[0][0]\n",
    "        else:\n",
    "            bert_sim_name = -1\n",
    "        bert_sim_list.append(bert_sim)\n",
    "    df[f\"bert_sim_{col}\"] = bert_sim_list\n",
    "    df[f\"bert_sim_{col}\"] = df[f\"bert_sim_{col}\"].astype(np.float16)\n",
    "    return df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [],
   "source": [
    "def reduce_data_size(df, features):\n",
    "    if MODE == 'local_train':\n",
    "        df = df[features + ['target', 'id', 'match_id']].copy()\n",
    "    elif MODE == 'kaggle_inference':\n",
    "        df = df[features + ['id', 'match_id']].copy()\n",
    "\n",
    "\n",
    "    df[features] = df[features].astype(np.float16)\n",
    "    for _ in range(5):\n",
    "        gc.collect()\n",
    "        \n",
    "    return df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [],
   "source": [
    "def train_model(df, features):\n",
    "    params = {'objective': 'binary', \n",
    "              'boosting': 'gbdt',\n",
    "              'learning_rate': 0.1, \n",
    "              'metric': 'binary_logloss', \n",
    "              'seed': SEED, \n",
    "              'feature_pre_filter': False, \n",
    "              'lambda_l1': 0.5745709668124809, \n",
    "              'lambda_l2': 0.5123383865042099, \n",
    "              'num_leaves': 239, \n",
    "              'feature_fraction': 0.784, \n",
    "              'bagging_fraction': 1.0, \n",
    "              'bagging_freq': 0, \n",
    "              'min_child_samples': 5\n",
    "              }\n",
    "\n",
    "    # split folds\n",
    "    kf = StratifiedKFold(n_splits=N_SPLITS, shuffle=True, random_state=SEED)\n",
    "    for i, (trn_idx, val_idx) in enumerate(kf.split(df, df[\"target\"], df[\"target\"])):\n",
    "        df.loc[val_idx, \"fold\"] = i\n",
    "    \n",
    "    fi = pd.DataFrame()\n",
    "    oof = df[['id', 'match_id', 'target']].copy()\n",
    "    oof['prob'] = 0.0\n",
    "    scores = []\n",
    "\n",
    "    for i in range(N_SPLITS):\n",
    "        print('fold : ' + str(i))\n",
    "        tr_idx = df[df['fold'] != i].index\n",
    "        vl_idx = df[df['fold'] == i].index\n",
    "        tr_x, tr_y = df.loc[tr_idx, features], df.loc[tr_idx, 'target']\n",
    "        vl_x, vl_y = df.loc[vl_idx, features], df.loc[vl_idx, 'target']\n",
    "        tr_data = lgb.Dataset(tr_x, label=tr_y)\n",
    "        vl_data = lgb.Dataset(vl_x, label=vl_y)\n",
    "\n",
    "        model = lgb.train(params, tr_data, valid_sets=[tr_data, vl_data],\n",
    "                        num_boost_round=20000, early_stopping_rounds=100, verbose_eval=1000)\n",
    "\n",
    "        # 特徴量重要度\n",
    "        fi_tmp = pd.DataFrame()\n",
    "        fi_tmp['feature'] = model.feature_name()\n",
    "        fi_tmp['importance'] = model.feature_importance(importance_type='gain')\n",
    "        fi_tmp['iter'] = i\n",
    "        fi = fi.append(fi_tmp)\n",
    "\n",
    "        pred = model.predict(vl_x, num_iteration=model.best_iteration)\n",
    "        oof.loc[vl_idx, 'prob'] = pred\n",
    "\n",
    "        score = accuracy_score((pred > PROB_TH).astype(int), vl_y)\n",
    "        scores.append(score)\n",
    "        print(f'fold{i} | accuracy = ' + '{:.5f}'.format(score))\n",
    "\n",
    "        with open(OUTPUT_DIR + f'{exp_name}/model{i}.pickle', 'wb') as f:\n",
    "            pickle.dump(model, f)\n",
    "        \n",
    "    oof.to_csv(OUTPUT_DIR + f'{exp_name}/{exp_name}_oof.csv', index=False)\n",
    "\n",
    "    print('accuracy(mean) : ' + '{:.5f}'.format(np.mean(scores)))\n",
    "    print(scores)\n",
    "\n",
    "    fi_n = fi['feature'].nunique()\n",
    "    order = list(fi.groupby(\"feature\").mean().sort_values(\"importance\", ascending=False).index)\n",
    "    plt.figure(figsize=(10, fi_n*0.2))\n",
    "    sns.barplot(x=\"importance\", y=\"feature\", data=fi, order=order)\n",
    "    plt.title(f\"LGBM importance\")\n",
    "    plt.tight_layout()\n",
    "    plt.savefig(OUTPUT_DIR + f'{exp_name}/lgbm_importance.png')\n",
    "\n",
    "    return oof, np.mean(scores)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [],
   "source": [
    "def model_inference(df, features):\n",
    "    pred = np.zeros(len(df))\n",
    "    for i in range(N_SPLITS):\n",
    "        with open(MODEL_DIR + f'model{i}.pickle', 'rb') as f:\n",
    "            model = pickle.load(f)\n",
    "        model.save_model(\"test.lgb\")\n",
    "        fi = ForestInference()\n",
    "        fi = ForestInference.load(\"test.lgb\", output_class=True, model_type=\"lightgbm\")\n",
    "        pred += fi.predict(df[features]) / N_SPLITS\n",
    "    return pred"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [],
   "source": [
    "def transform_data(df, org_data):\n",
    "    train_candidate = pd.DataFrame()\n",
    "    train_candidate['id'] = org_data['id'].unique()\n",
    "    train_candidate['matches'] = org_data['id'].unique()\n",
    "\n",
    "    train_match = df[df['prob'] >= PROB_TH].copy()\n",
    "    train_match = train_match.groupby('id')['match_id'].apply(list).map(\" \".join).reset_index()\n",
    "    train_match.columns = ['id','candidates']\n",
    "    train_candidate = train_candidate.merge(train_match, on = 'id', how = 'left')\n",
    "    idx = ~train_candidate['candidates'].isna()\n",
    "    train_candidate.loc[idx, \"matches\"] += \" \" + train_candidate.loc[idx, \"candidates\"]\n",
    "    return train_candidate[['id', 'matches']]\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [],
   "source": [
    "def postprocess(df):\n",
    "    id2match = dict(zip(df[\"id\"].values, df[\"matches\"].str.split()))\n",
    "\n",
    "    for match in tqdm(df[\"matches\"]):\n",
    "        match = match.split()\n",
    "        if len(match) == 1:        \n",
    "            continue\n",
    "\n",
    "        base = match[0]\n",
    "        for m in match[1:]:\n",
    "            if not base in id2match[m]:\n",
    "                id2match[m].append(base)\n",
    "    df[\"matches\"] = df[\"id\"].map(id2match).map(\" \".join)\n",
    "    return df "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [],
   "source": [
    "def run_train():\n",
    "    train_origin = pd.read_csv(INPUT_DIR + \"train.csv\")\n",
    "    train_origin = preprocess(train_origin)\n",
    "\n",
    "    # trainデータの分割\n",
    "    kf = GroupKFold(n_splits=2)\n",
    "    for i, (trn_idx, val_idx) in enumerate(kf.split(train_origin, train_origin['point_of_interest'], train_origin['point_of_interest'])):\n",
    "        train_origin.loc[val_idx, \"set\"] = i\n",
    "\n",
    "    # 1st stage\n",
    "    train = pd.concat([\n",
    "        extract_candidate(train_origin[train_origin[\"set\"]==0]), \n",
    "        extract_candidate(train_origin[train_origin[\"set\"]==1]), \n",
    "    ])\n",
    "    train = add_orgin_data(train, train_origin)\n",
    "    stage1_max_score = calc_max_score(train, train_origin)\n",
    "\n",
    "    # 2nd stage\n",
    "    # create target\n",
    "    train['target'] = (train['point_of_interest'] == train['match_point_of_interest']).values.astype(int)\n",
    "    train[\"target\"] = train[\"target\"].fillna(0)\n",
    "\n",
    "    train = add_distance_features(train)\n",
    "    \n",
    "    # reduce memory\n",
    "    train = train.drop(columns=['latitude', 'longitude', 'address', 'city', 'state', 'zip', 'country', 'url', 'phone',\n",
    "                                'match_latitude', 'match_longitude', 'match_address', 'match_city', 'match_state', \n",
    "                                'match_zip', 'match_country', 'match_url', 'match_phone'])\n",
    "    gc.collect()\n",
    "\n",
    "    # bert類似度\n",
    "    bert_vec_categories = make_bert_vec(train_origin[[\"categories\"]], \"categories\")\n",
    "    train = add_bert_sim_features(train, bert_vec_categories, \"categories\")\n",
    "    del bert_vec_categories\n",
    "    gc.collect()\n",
    "\n",
    "    print(\"add_bert_sim_name\")\n",
    "    bert_vec_name = make_bert_vec(train_origin[[\"name\"]], \"name\")\n",
    "    train = add_bert_sim_features(train, bert_vec_name, \"name\")\n",
    "    del bert_vec_name\n",
    "    gc.collect()\n",
    "\n",
    "    not_use_cols = ['match_state_leven', 'address', 'categories', 'point_of_interest', 'match_address_leven',\n",
    "                    'city', 'match_point_of_interest', 'match_name', 'match_categories_leven', 'match_longitude',\n",
    "                    'target', 'match_city_leven', 'zip', 'match_categories', 'match_city', 'match_latitude',\n",
    "                    'match_zip', 'match_url', 'id', 'match_set', 'country', 'match_state', 'match_address',\n",
    "                    'match_name_leven', 'match_id', 'latitude', 'url', 'set', 'name', 'phone', 'longitude',\n",
    "                    'match_url_leven', 'state', 'match_phone', 'match_country']\n",
    "    features = [c for c in train.columns if c not in not_use_cols]\n",
    "    with open(OUTPUT_DIR + f'{exp_name}/features.pickle', 'wb') as f:\n",
    "        pickle.dump(features, f)\n",
    "\n",
    "    train = reduce_data_size(train, features)\n",
    "\n",
    "    oof, stage2_mean_accuracy = train_model(train, features)\n",
    "    oof = transform_data(oof, train_origin)\n",
    "\n",
    "    cv_score = get_score(oof, train_origin)\n",
    "    print(f'cv_score = ' + '{:.5f}'.format(cv_score))\n",
    "\n",
    "    oof = postprocess(oof)\n",
    "    cv_score_after_pp = get_score(oof, train_origin)\n",
    "    print(f'cv_score(after_pp) = ' + '{:.5f}'.format(cv_score_after_pp))\n",
    "\n",
    "\n",
    "    report = f'{exp_name}\\n'\n",
    "    report += memo + '\\n'\n",
    "    report += 'stage1_max_score : ' + '{:.5f}'.format(stage1_max_score) + '\\n'\n",
    "    report += 'stage2_mean_accuracy : ' + '{:.5f}'.format(stage2_mean_accuracy) + '\\n'\n",
    "    report += 'cv_score : ' + '{:.5f}'.format(cv_score) + '\\n'\n",
    "    report += 'cv_score_after_pp : ' + '{:.5f}'.format(cv_score_after_pp) + '\\n'\n",
    "    print(report)\n",
    "    line_notify.send(report)\n",
    "    slack_notify.send(report)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 44/44 [00:01<00:00, 31.23it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "add_org\n",
      "add_dist_feat\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "42it [00:13,  3.12it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "add_bert_sim_cat\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Some weights of the model checkpoint at distilbert-base-uncased were not used when initializing DistilBertModel: ['vocab_transform.bias', 'vocab_transform.weight', 'vocab_layer_norm.weight', 'vocab_layer_norm.bias', 'vocab_projector.bias', 'vocab_projector.weight']\n",
      "- This IS expected if you are initializing DistilBertModel from the checkpoint of a model trained on another task or with another architecture (e.g. initializing a BertForSequenceClassification model from a BertForPreTraining model).\n",
      "- This IS NOT expected if you are initializing DistilBertModel from the checkpoint of a model that you expect to be exactly identical (initializing a BertForSequenceClassification model from a BertForSequenceClassification model).\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "add_bert_sim_name\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Some weights of the model checkpoint at distilbert-base-uncased were not used when initializing DistilBertModel: ['vocab_transform.bias', 'vocab_transform.weight', 'vocab_layer_norm.weight', 'vocab_layer_norm.bias', 'vocab_projector.bias', 'vocab_projector.weight']\n",
      "- This IS expected if you are initializing DistilBertModel from the checkpoint of a model trained on another task or with another architecture (e.g. initializing a BertForSequenceClassification model from a BertForPreTraining model).\n",
      "- This IS NOT expected if you are initializing DistilBertModel from the checkpoint of a model that you expect to be exactly identical (initializing a BertForSequenceClassification model from a BertForSequenceClassification model).\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "test_dataset_info\n",
      "<class 'pandas.core.frame.DataFrame'>\n",
      "Int64Index: 200711 entries, 0 to 196024\n",
      "Data columns (total 53 columns):\n",
      " #   Column                   Non-Null Count   Dtype  \n",
      "---  ------                   --------------   -----  \n",
      " 0   d_near                   200711 non-null  float16\n",
      " 1   match_name_gesh          200711 non-null  float16\n",
      " 2   match_name_jaro          200711 non-null  float16\n",
      " 3   match_name_lcs           200711 non-null  float16\n",
      " 4   match_name_len           200711 non-null  float16\n",
      " 5   match_name_nleven        200711 non-null  float16\n",
      " 6   match_name_nlcsi         200711 non-null  float16\n",
      " 7   match_address_gesh       200711 non-null  float16\n",
      " 8   match_address_jaro       200711 non-null  float16\n",
      " 9   match_address_lcs        200711 non-null  float16\n",
      " 10  match_address_len        200711 non-null  float16\n",
      " 11  match_address_nleven     200711 non-null  float16\n",
      " 12  match_address_nlcsi      200711 non-null  float16\n",
      " 13  match_city_gesh          200711 non-null  float16\n",
      " 14  match_city_jaro          200711 non-null  float16\n",
      " 15  match_city_lcs           200711 non-null  float16\n",
      " 16  match_city_len           200711 non-null  float16\n",
      " 17  match_city_nleven        200711 non-null  float16\n",
      " 18  match_city_nlcsi         200711 non-null  float16\n",
      " 19  match_state_gesh         200711 non-null  float16\n",
      " 20  match_state_jaro         200711 non-null  float16\n",
      " 21  match_state_lcs          200711 non-null  float16\n",
      " 22  match_state_len          200711 non-null  float16\n",
      " 23  match_state_nleven       200711 non-null  float16\n",
      " 24  match_state_nlcsi        200711 non-null  float16\n",
      " 25  match_zip_gesh           200711 non-null  float16\n",
      " 26  match_zip_leven          200711 non-null  float16\n",
      " 27  match_zip_jaro           200711 non-null  float16\n",
      " 28  match_zip_lcs            200711 non-null  float16\n",
      " 29  match_country_gesh       200711 non-null  float16\n",
      " 30  match_country_leven      200711 non-null  float16\n",
      " 31  match_country_jaro       200711 non-null  float16\n",
      " 32  match_country_lcs        200711 non-null  float16\n",
      " 33  match_url_gesh           200711 non-null  float16\n",
      " 34  match_url_jaro           200711 non-null  float16\n",
      " 35  match_url_lcs            200711 non-null  float16\n",
      " 36  match_url_len            200711 non-null  float16\n",
      " 37  match_url_nleven         200711 non-null  float16\n",
      " 38  match_url_nlcsi          200711 non-null  float16\n",
      " 39  match_phone_gesh         200711 non-null  float16\n",
      " 40  match_phone_leven        200711 non-null  float16\n",
      " 41  match_phone_jaro         200711 non-null  float16\n",
      " 42  match_phone_lcs          200711 non-null  float16\n",
      " 43  match_categories_gesh    200711 non-null  float16\n",
      " 44  match_categories_jaro    200711 non-null  float16\n",
      " 45  match_categories_lcs     200711 non-null  float16\n",
      " 46  match_categories_len     200711 non-null  float16\n",
      " 47  match_categories_nleven  200711 non-null  float16\n",
      " 48  match_categories_nlcsi   200711 non-null  float16\n",
      " 49  bert_sim_categories      200711 non-null  float16\n",
      " 50  bert_sim_name            200711 non-null  float16\n",
      " 51  id                       200711 non-null  object \n",
      " 52  match_id                 200711 non-null  object \n",
      "dtypes: float16(51), object(2)\n",
      "memory usage: 24.1+ MB\n",
      "None\n",
      "[W] [20:35:41.638571] Treelite currently does not support float64 model parameters. Accuracy may degrade slightly relative to native LightGBM invocation.\n",
      "[W] [20:35:43.094814] Casting all thresholds and leaf values to float32, as FIL currently doesn't support inferencing models with float64 values. This may lead to predictions with reduced accuracy.\n",
      "[W] [20:35:44.647769] Treelite currently does not support float64 model parameters. Accuracy may degrade slightly relative to native LightGBM invocation.\n",
      "[W] [20:35:46.189884] Casting all thresholds and leaf values to float32, as FIL currently doesn't support inferencing models with float64 values. This may lead to predictions with reduced accuracy.\n",
      "[W] [20:35:47.857914] Treelite currently does not support float64 model parameters. Accuracy may degrade slightly relative to native LightGBM invocation.\n",
      "[W] [20:35:49.414459] Casting all thresholds and leaf values to float32, as FIL currently doesn't support inferencing models with float64 values. This may lead to predictions with reduced accuracy.\n",
      "[W] [20:35:51.099729] Treelite currently does not support float64 model parameters. Accuracy may degrade slightly relative to native LightGBM invocation.\n",
      "[W] [20:35:52.626299] Casting all thresholds and leaf values to float32, as FIL currently doesn't support inferencing models with float64 values. This may lead to predictions with reduced accuracy.\n",
      "[W] [20:35:54.340713] Treelite currently does not support float64 model parameters. Accuracy may degrade slightly relative to native LightGBM invocation.\n",
      "[W] [20:35:55.924289] Casting all thresholds and leaf values to float32, as FIL currently doesn't support inferencing models with float64 values. This may lead to predictions with reduced accuracy.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 45/45 [00:01<00:00, 30.61it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "add_org\n",
      "add_dist_feat\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "42it [00:14,  2.82it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "add_bert_sim_cat\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Some weights of the model checkpoint at distilbert-base-uncased were not used when initializing DistilBertModel: ['vocab_transform.bias', 'vocab_transform.weight', 'vocab_layer_norm.weight', 'vocab_layer_norm.bias', 'vocab_projector.bias', 'vocab_projector.weight']\n",
      "- This IS expected if you are initializing DistilBertModel from the checkpoint of a model trained on another task or with another architecture (e.g. initializing a BertForSequenceClassification model from a BertForPreTraining model).\n",
      "- This IS NOT expected if you are initializing DistilBertModel from the checkpoint of a model that you expect to be exactly identical (initializing a BertForSequenceClassification model from a BertForSequenceClassification model).\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "add_bert_sim_name\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Some weights of the model checkpoint at distilbert-base-uncased were not used when initializing DistilBertModel: ['vocab_transform.bias', 'vocab_transform.weight', 'vocab_layer_norm.weight', 'vocab_layer_norm.bias', 'vocab_projector.bias', 'vocab_projector.weight']\n",
      "- This IS expected if you are initializing DistilBertModel from the checkpoint of a model trained on another task or with another architecture (e.g. initializing a BertForSequenceClassification model from a BertForPreTraining model).\n",
      "- This IS NOT expected if you are initializing DistilBertModel from the checkpoint of a model that you expect to be exactly identical (initializing a BertForSequenceClassification model from a BertForSequenceClassification model).\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "test_dataset_info\n",
      "<class 'pandas.core.frame.DataFrame'>\n",
      "Int64Index: 200697 entries, 0 to 139436\n",
      "Data columns (total 53 columns):\n",
      " #   Column                   Non-Null Count   Dtype  \n",
      "---  ------                   --------------   -----  \n",
      " 0   d_near                   200697 non-null  float16\n",
      " 1   match_name_gesh          200697 non-null  float16\n",
      " 2   match_name_jaro          200697 non-null  float16\n",
      " 3   match_name_lcs           200697 non-null  float16\n",
      " 4   match_name_len           200697 non-null  float16\n",
      " 5   match_name_nleven        200697 non-null  float16\n",
      " 6   match_name_nlcsi         200697 non-null  float16\n",
      " 7   match_address_gesh       200697 non-null  float16\n",
      " 8   match_address_jaro       200697 non-null  float16\n",
      " 9   match_address_lcs        200697 non-null  float16\n",
      " 10  match_address_len        200697 non-null  float16\n",
      " 11  match_address_nleven     200697 non-null  float16\n",
      " 12  match_address_nlcsi      200697 non-null  float16\n",
      " 13  match_city_gesh          200697 non-null  float16\n",
      " 14  match_city_jaro          200697 non-null  float16\n",
      " 15  match_city_lcs           200697 non-null  float16\n",
      " 16  match_city_len           200697 non-null  float16\n",
      " 17  match_city_nleven        200697 non-null  float16\n",
      " 18  match_city_nlcsi         200697 non-null  float16\n",
      " 19  match_state_gesh         200697 non-null  float16\n",
      " 20  match_state_jaro         200697 non-null  float16\n",
      " 21  match_state_lcs          200697 non-null  float16\n",
      " 22  match_state_len          200697 non-null  float16\n",
      " 23  match_state_nleven       200697 non-null  float16\n",
      " 24  match_state_nlcsi        200697 non-null  float16\n",
      " 25  match_zip_gesh           200697 non-null  float16\n",
      " 26  match_zip_leven          200697 non-null  float16\n",
      " 27  match_zip_jaro           200697 non-null  float16\n",
      " 28  match_zip_lcs            200697 non-null  float16\n",
      " 29  match_country_gesh       200697 non-null  float16\n",
      " 30  match_country_leven      200697 non-null  float16\n",
      " 31  match_country_jaro       200697 non-null  float16\n",
      " 32  match_country_lcs        200697 non-null  float16\n",
      " 33  match_url_gesh           200697 non-null  float16\n",
      " 34  match_url_jaro           200697 non-null  float16\n",
      " 35  match_url_lcs            200697 non-null  float16\n",
      " 36  match_url_len            200697 non-null  float16\n",
      " 37  match_url_nleven         200697 non-null  float16\n",
      " 38  match_url_nlcsi          200697 non-null  float16\n",
      " 39  match_phone_gesh         200697 non-null  float16\n",
      " 40  match_phone_leven        200697 non-null  float16\n",
      " 41  match_phone_jaro         200697 non-null  float16\n",
      " 42  match_phone_lcs          200697 non-null  float16\n",
      " 43  match_categories_gesh    200697 non-null  float16\n",
      " 44  match_categories_jaro    200697 non-null  float16\n",
      " 45  match_categories_lcs     200697 non-null  float16\n",
      " 46  match_categories_len     200697 non-null  float16\n",
      " 47  match_categories_nleven  200697 non-null  float16\n",
      " 48  match_categories_nlcsi   200697 non-null  float16\n",
      " 49  bert_sim_categories      200697 non-null  float16\n",
      " 50  bert_sim_name            200697 non-null  float16\n",
      " 51  id                       200697 non-null  object \n",
      " 52  match_id                 200697 non-null  object \n",
      "dtypes: float16(51), object(2)\n",
      "memory usage: 24.1+ MB\n",
      "None\n",
      "[W] [20:37:17.534343] Treelite currently does not support float64 model parameters. Accuracy may degrade slightly relative to native LightGBM invocation.\n",
      "[W] [20:37:18.935662] Casting all thresholds and leaf values to float32, as FIL currently doesn't support inferencing models with float64 values. This may lead to predictions with reduced accuracy.\n",
      "[W] [20:37:20.371553] Treelite currently does not support float64 model parameters. Accuracy may degrade slightly relative to native LightGBM invocation.\n",
      "[W] [20:37:21.859767] Casting all thresholds and leaf values to float32, as FIL currently doesn't support inferencing models with float64 values. This may lead to predictions with reduced accuracy.\n",
      "[W] [20:37:23.591950] Treelite currently does not support float64 model parameters. Accuracy may degrade slightly relative to native LightGBM invocation.\n",
      "[W] [20:37:25.216482] Casting all thresholds and leaf values to float32, as FIL currently doesn't support inferencing models with float64 values. This may lead to predictions with reduced accuracy.\n",
      "[W] [20:37:26.986781] Treelite currently does not support float64 model parameters. Accuracy may degrade slightly relative to native LightGBM invocation.\n",
      "[W] [20:37:28.513745] Casting all thresholds and leaf values to float32, as FIL currently doesn't support inferencing models with float64 values. This may lead to predictions with reduced accuracy.\n",
      "[W] [20:37:30.206662] Treelite currently does not support float64 model parameters. Accuracy may degrade slightly relative to native LightGBM invocation.\n",
      "[W] [20:37:31.777912] Casting all thresholds and leaf values to float32, as FIL currently doesn't support inferencing models with float64 values. This may lead to predictions with reduced accuracy.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 44/44 [00:01<00:00, 37.97it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "add_org\n",
      "add_dist_feat\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "40it [00:27,  1.47it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "add_bert_sim_cat\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Some weights of the model checkpoint at distilbert-base-uncased were not used when initializing DistilBertModel: ['vocab_transform.bias', 'vocab_transform.weight', 'vocab_layer_norm.weight', 'vocab_layer_norm.bias', 'vocab_projector.bias', 'vocab_projector.weight']\n",
      "- This IS expected if you are initializing DistilBertModel from the checkpoint of a model trained on another task or with another architecture (e.g. initializing a BertForSequenceClassification model from a BertForPreTraining model).\n",
      "- This IS NOT expected if you are initializing DistilBertModel from the checkpoint of a model that you expect to be exactly identical (initializing a BertForSequenceClassification model from a BertForSequenceClassification model).\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "add_bert_sim_name\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Some weights of the model checkpoint at distilbert-base-uncased were not used when initializing DistilBertModel: ['vocab_transform.bias', 'vocab_transform.weight', 'vocab_layer_norm.weight', 'vocab_layer_norm.bias', 'vocab_projector.bias', 'vocab_projector.weight']\n",
      "- This IS expected if you are initializing DistilBertModel from the checkpoint of a model trained on another task or with another architecture (e.g. initializing a BertForSequenceClassification model from a BertForPreTraining model).\n",
      "- This IS NOT expected if you are initializing DistilBertModel from the checkpoint of a model that you expect to be exactly identical (initializing a BertForSequenceClassification model from a BertForSequenceClassification model).\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "test_dataset_info\n",
      "<class 'pandas.core.frame.DataFrame'>\n",
      "Int64Index: 200705 entries, 3528 to 200353\n",
      "Data columns (total 53 columns):\n",
      " #   Column                   Non-Null Count   Dtype  \n",
      "---  ------                   --------------   -----  \n",
      " 0   d_near                   200705 non-null  float16\n",
      " 1   match_name_gesh          200705 non-null  float16\n",
      " 2   match_name_jaro          200705 non-null  float16\n",
      " 3   match_name_lcs           200705 non-null  float16\n",
      " 4   match_name_len           200705 non-null  float16\n",
      " 5   match_name_nleven        200705 non-null  float16\n",
      " 6   match_name_nlcsi         200705 non-null  float16\n",
      " 7   match_address_gesh       200705 non-null  float16\n",
      " 8   match_address_jaro       200705 non-null  float16\n",
      " 9   match_address_lcs        200705 non-null  float16\n",
      " 10  match_address_len        200705 non-null  float16\n",
      " 11  match_address_nleven     200705 non-null  float16\n",
      " 12  match_address_nlcsi      200705 non-null  float16\n",
      " 13  match_city_gesh          200705 non-null  float16\n",
      " 14  match_city_jaro          200705 non-null  float16\n",
      " 15  match_city_lcs           200705 non-null  float16\n",
      " 16  match_city_len           200705 non-null  float16\n",
      " 17  match_city_nleven        200705 non-null  float16\n",
      " 18  match_city_nlcsi         200705 non-null  float16\n",
      " 19  match_state_gesh         200705 non-null  float16\n",
      " 20  match_state_jaro         200705 non-null  float16\n",
      " 21  match_state_lcs          200705 non-null  float16\n",
      " 22  match_state_len          200705 non-null  float16\n",
      " 23  match_state_nleven       200705 non-null  float16\n",
      " 24  match_state_nlcsi        200705 non-null  float16\n",
      " 25  match_zip_gesh           200705 non-null  float16\n",
      " 26  match_zip_leven          200705 non-null  float16\n",
      " 27  match_zip_jaro           200705 non-null  float16\n",
      " 28  match_zip_lcs            200705 non-null  float16\n",
      " 29  match_country_gesh       200705 non-null  float16\n",
      " 30  match_country_leven      200705 non-null  float16\n",
      " 31  match_country_jaro       200705 non-null  float16\n",
      " 32  match_country_lcs        200705 non-null  float16\n",
      " 33  match_url_gesh           200705 non-null  float16\n",
      " 34  match_url_jaro           200705 non-null  float16\n",
      " 35  match_url_lcs            200705 non-null  float16\n",
      " 36  match_url_len            200705 non-null  float16\n",
      " 37  match_url_nleven         200705 non-null  float16\n",
      " 38  match_url_nlcsi          200705 non-null  float16\n",
      " 39  match_phone_gesh         200705 non-null  float16\n",
      " 40  match_phone_leven        200705 non-null  float16\n",
      " 41  match_phone_jaro         200705 non-null  float16\n",
      " 42  match_phone_lcs          200705 non-null  float16\n",
      " 43  match_categories_gesh    200705 non-null  float16\n",
      " 44  match_categories_jaro    200705 non-null  float16\n",
      " 45  match_categories_lcs     200705 non-null  float16\n",
      " 46  match_categories_len     200705 non-null  float16\n",
      " 47  match_categories_nleven  200705 non-null  float16\n",
      " 48  match_categories_nlcsi   200705 non-null  float16\n",
      " 49  bert_sim_categories      200705 non-null  float16\n",
      " 50  bert_sim_name            200705 non-null  float16\n",
      " 51  id                       200705 non-null  object \n",
      " 52  match_id                 200705 non-null  object \n",
      "dtypes: float16(51), object(2)\n",
      "memory usage: 24.1+ MB\n",
      "None\n",
      "[W] [20:39:05.240660] Treelite currently does not support float64 model parameters. Accuracy may degrade slightly relative to native LightGBM invocation.\n",
      "[W] [20:39:06.638398] Casting all thresholds and leaf values to float32, as FIL currently doesn't support inferencing models with float64 values. This may lead to predictions with reduced accuracy.\n",
      "[W] [20:39:08.154198] Treelite currently does not support float64 model parameters. Accuracy may degrade slightly relative to native LightGBM invocation.\n",
      "[W] [20:39:09.651187] Casting all thresholds and leaf values to float32, as FIL currently doesn't support inferencing models with float64 values. This may lead to predictions with reduced accuracy.\n",
      "[W] [20:39:11.298004] Treelite currently does not support float64 model parameters. Accuracy may degrade slightly relative to native LightGBM invocation.\n",
      "[W] [20:39:12.839520] Casting all thresholds and leaf values to float32, as FIL currently doesn't support inferencing models with float64 values. This may lead to predictions with reduced accuracy.\n",
      "[W] [20:39:14.453329] Treelite currently does not support float64 model parameters. Accuracy may degrade slightly relative to native LightGBM invocation.\n",
      "[W] [20:39:15.936252] Casting all thresholds and leaf values to float32, as FIL currently doesn't support inferencing models with float64 values. This may lead to predictions with reduced accuracy.\n",
      "[W] [20:39:17.675768] Treelite currently does not support float64 model parameters. Accuracy may degrade slightly relative to native LightGBM invocation.\n",
      "[W] [20:39:19.191930] Casting all thresholds and leaf values to float32, as FIL currently doesn't support inferencing models with float64 values. This may lead to predictions with reduced accuracy.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 1/1 [00:02<00:00,  2.57s/it]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "add_org\n",
      "add_dist_feat\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "1it [01:17, 77.14s/it]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "add_bert_sim_cat\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Some weights of the model checkpoint at distilbert-base-uncased were not used when initializing DistilBertModel: ['vocab_transform.bias', 'vocab_transform.weight', 'vocab_layer_norm.weight', 'vocab_layer_norm.bias', 'vocab_projector.bias', 'vocab_projector.weight']\n",
      "- This IS expected if you are initializing DistilBertModel from the checkpoint of a model trained on another task or with another architecture (e.g. initializing a BertForSequenceClassification model from a BertForPreTraining model).\n",
      "- This IS NOT expected if you are initializing DistilBertModel from the checkpoint of a model that you expect to be exactly identical (initializing a BertForSequenceClassification model from a BertForSequenceClassification model).\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "add_bert_sim_name\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Some weights of the model checkpoint at distilbert-base-uncased were not used when initializing DistilBertModel: ['vocab_transform.bias', 'vocab_transform.weight', 'vocab_layer_norm.weight', 'vocab_layer_norm.bias', 'vocab_projector.bias', 'vocab_projector.weight']\n",
      "- This IS expected if you are initializing DistilBertModel from the checkpoint of a model trained on another task or with another architecture (e.g. initializing a BertForSequenceClassification model from a BertForPreTraining model).\n",
      "- This IS NOT expected if you are initializing DistilBertModel from the checkpoint of a model that you expect to be exactly identical (initializing a BertForSequenceClassification model from a BertForSequenceClassification model).\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "test_dataset_info\n",
      "<class 'pandas.core.frame.DataFrame'>\n",
      "Int64Index: 221123 entries, 0 to 221122\n",
      "Data columns (total 53 columns):\n",
      " #   Column                   Non-Null Count   Dtype  \n",
      "---  ------                   --------------   -----  \n",
      " 0   d_near                   221123 non-null  float16\n",
      " 1   match_name_gesh          221123 non-null  float16\n",
      " 2   match_name_jaro          221123 non-null  float16\n",
      " 3   match_name_lcs           221123 non-null  float16\n",
      " 4   match_name_len           221123 non-null  float16\n",
      " 5   match_name_nleven        221123 non-null  float16\n",
      " 6   match_name_nlcsi         221123 non-null  float16\n",
      " 7   match_address_gesh       221123 non-null  float16\n",
      " 8   match_address_jaro       221123 non-null  float16\n",
      " 9   match_address_lcs        221123 non-null  float16\n",
      " 10  match_address_len        221123 non-null  float16\n",
      " 11  match_address_nleven     221123 non-null  float16\n",
      " 12  match_address_nlcsi      221123 non-null  float16\n",
      " 13  match_city_gesh          221123 non-null  float16\n",
      " 14  match_city_jaro          221123 non-null  float16\n",
      " 15  match_city_lcs           221123 non-null  float16\n",
      " 16  match_city_len           221123 non-null  float16\n",
      " 17  match_city_nleven        221123 non-null  float16\n",
      " 18  match_city_nlcsi         221123 non-null  float16\n",
      " 19  match_state_gesh         221123 non-null  float16\n",
      " 20  match_state_jaro         221123 non-null  float16\n",
      " 21  match_state_lcs          221123 non-null  float16\n",
      " 22  match_state_len          221123 non-null  float16\n",
      " 23  match_state_nleven       221123 non-null  float16\n",
      " 24  match_state_nlcsi        221123 non-null  float16\n",
      " 25  match_zip_gesh           221123 non-null  float16\n",
      " 26  match_zip_leven          221123 non-null  float16\n",
      " 27  match_zip_jaro           221123 non-null  float16\n",
      " 28  match_zip_lcs            221123 non-null  float16\n",
      " 29  match_country_gesh       221123 non-null  float16\n",
      " 30  match_country_leven      221123 non-null  float16\n",
      " 31  match_country_jaro       221123 non-null  float16\n",
      " 32  match_country_lcs        221123 non-null  float16\n",
      " 33  match_url_gesh           221123 non-null  float16\n",
      " 34  match_url_jaro           221123 non-null  float16\n",
      " 35  match_url_lcs            221123 non-null  float16\n",
      " 36  match_url_len            221123 non-null  float16\n",
      " 37  match_url_nleven         221123 non-null  float16\n",
      " 38  match_url_nlcsi          221123 non-null  float16\n",
      " 39  match_phone_gesh         221123 non-null  float16\n",
      " 40  match_phone_leven        221123 non-null  float16\n",
      " 41  match_phone_jaro         221123 non-null  float16\n",
      " 42  match_phone_lcs          221123 non-null  float16\n",
      " 43  match_categories_gesh    221123 non-null  float16\n",
      " 44  match_categories_jaro    221123 non-null  float16\n",
      " 45  match_categories_lcs     221123 non-null  float16\n",
      " 46  match_categories_len     221123 non-null  float16\n",
      " 47  match_categories_nleven  221123 non-null  float16\n",
      " 48  match_categories_nlcsi   221123 non-null  float16\n",
      " 49  bert_sim_categories      221123 non-null  float16\n",
      " 50  bert_sim_name            221123 non-null  float16\n",
      " 51  id                       221123 non-null  object \n",
      " 52  match_id                 221123 non-null  object \n",
      "dtypes: float16(51), object(2)\n",
      "memory usage: 26.6+ MB\n",
      "None\n",
      "[W] [20:41:46.636036] Treelite currently does not support float64 model parameters. Accuracy may degrade slightly relative to native LightGBM invocation.\n",
      "[W] [20:41:48.021826] Casting all thresholds and leaf values to float32, as FIL currently doesn't support inferencing models with float64 values. This may lead to predictions with reduced accuracy.\n",
      "[W] [20:41:49.496999] Treelite currently does not support float64 model parameters. Accuracy may degrade slightly relative to native LightGBM invocation.\n",
      "[W] [20:41:50.989012] Casting all thresholds and leaf values to float32, as FIL currently doesn't support inferencing models with float64 values. This may lead to predictions with reduced accuracy.\n",
      "[W] [20:41:52.771692] Treelite currently does not support float64 model parameters. Accuracy may degrade slightly relative to native LightGBM invocation.\n",
      "[W] [20:41:54.326383] Casting all thresholds and leaf values to float32, as FIL currently doesn't support inferencing models with float64 values. This may lead to predictions with reduced accuracy.\n",
      "[W] [20:41:56.071670] Treelite currently does not support float64 model parameters. Accuracy may degrade slightly relative to native LightGBM invocation.\n",
      "[W] [20:41:57.552858] Casting all thresholds and leaf values to float32, as FIL currently doesn't support inferencing models with float64 values. This may lead to predictions with reduced accuracy.\n",
      "[W] [20:41:59.269560] Treelite currently does not support float64 model parameters. Accuracy may degrade slightly relative to native LightGBM invocation.\n",
      "[W] [20:42:00.776835] Casting all thresholds and leaf values to float32, as FIL currently doesn't support inferencing models with float64 values. This may lead to predictions with reduced accuracy.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 43/43 [00:01<00:00, 34.47it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "add_org\n",
      "add_dist_feat\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "39it [00:26,  1.48it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "add_bert_sim_cat\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Some weights of the model checkpoint at distilbert-base-uncased were not used when initializing DistilBertModel: ['vocab_transform.bias', 'vocab_transform.weight', 'vocab_layer_norm.weight', 'vocab_layer_norm.bias', 'vocab_projector.bias', 'vocab_projector.weight']\n",
      "- This IS expected if you are initializing DistilBertModel from the checkpoint of a model trained on another task or with another architecture (e.g. initializing a BertForSequenceClassification model from a BertForPreTraining model).\n",
      "- This IS NOT expected if you are initializing DistilBertModel from the checkpoint of a model that you expect to be exactly identical (initializing a BertForSequenceClassification model from a BertForSequenceClassification model).\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "add_bert_sim_name\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Some weights of the model checkpoint at distilbert-base-uncased were not used when initializing DistilBertModel: ['vocab_transform.bias', 'vocab_transform.weight', 'vocab_layer_norm.weight', 'vocab_layer_norm.bias', 'vocab_projector.bias', 'vocab_projector.weight']\n",
      "- This IS expected if you are initializing DistilBertModel from the checkpoint of a model trained on another task or with another architecture (e.g. initializing a BertForSequenceClassification model from a BertForPreTraining model).\n",
      "- This IS NOT expected if you are initializing DistilBertModel from the checkpoint of a model that you expect to be exactly identical (initializing a BertForSequenceClassification model from a BertForSequenceClassification model).\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "test_dataset_info\n",
      "<class 'pandas.core.frame.DataFrame'>\n",
      "Int64Index: 200710 entries, 5300 to 136019\n",
      "Data columns (total 53 columns):\n",
      " #   Column                   Non-Null Count   Dtype  \n",
      "---  ------                   --------------   -----  \n",
      " 0   d_near                   200710 non-null  float16\n",
      " 1   match_name_gesh          200710 non-null  float16\n",
      " 2   match_name_jaro          200710 non-null  float16\n",
      " 3   match_name_lcs           200710 non-null  float16\n",
      " 4   match_name_len           200710 non-null  float16\n",
      " 5   match_name_nleven        200710 non-null  float16\n",
      " 6   match_name_nlcsi         200710 non-null  float16\n",
      " 7   match_address_gesh       200710 non-null  float16\n",
      " 8   match_address_jaro       200710 non-null  float16\n",
      " 9   match_address_lcs        200710 non-null  float16\n",
      " 10  match_address_len        200710 non-null  float16\n",
      " 11  match_address_nleven     200710 non-null  float16\n",
      " 12  match_address_nlcsi      200710 non-null  float16\n",
      " 13  match_city_gesh          200710 non-null  float16\n",
      " 14  match_city_jaro          200710 non-null  float16\n",
      " 15  match_city_lcs           200710 non-null  float16\n",
      " 16  match_city_len           200710 non-null  float16\n",
      " 17  match_city_nleven        200710 non-null  float16\n",
      " 18  match_city_nlcsi         200710 non-null  float16\n",
      " 19  match_state_gesh         200710 non-null  float16\n",
      " 20  match_state_jaro         200710 non-null  float16\n",
      " 21  match_state_lcs          200710 non-null  float16\n",
      " 22  match_state_len          200710 non-null  float16\n",
      " 23  match_state_nleven       200710 non-null  float16\n",
      " 24  match_state_nlcsi        200710 non-null  float16\n",
      " 25  match_zip_gesh           200710 non-null  float16\n",
      " 26  match_zip_leven          200710 non-null  float16\n",
      " 27  match_zip_jaro           200710 non-null  float16\n",
      " 28  match_zip_lcs            200710 non-null  float16\n",
      " 29  match_country_gesh       200710 non-null  float16\n",
      " 30  match_country_leven      200710 non-null  float16\n",
      " 31  match_country_jaro       200710 non-null  float16\n",
      " 32  match_country_lcs        200710 non-null  float16\n",
      " 33  match_url_gesh           200710 non-null  float16\n",
      " 34  match_url_jaro           200710 non-null  float16\n",
      " 35  match_url_lcs            200710 non-null  float16\n",
      " 36  match_url_len            200710 non-null  float16\n",
      " 37  match_url_nleven         200710 non-null  float16\n",
      " 38  match_url_nlcsi          200710 non-null  float16\n",
      " 39  match_phone_gesh         200710 non-null  float16\n",
      " 40  match_phone_leven        200710 non-null  float16\n",
      " 41  match_phone_jaro         200710 non-null  float16\n",
      " 42  match_phone_lcs          200710 non-null  float16\n",
      " 43  match_categories_gesh    200710 non-null  float16\n",
      " 44  match_categories_jaro    200710 non-null  float16\n",
      " 45  match_categories_lcs     200710 non-null  float16\n",
      " 46  match_categories_len     200710 non-null  float16\n",
      " 47  match_categories_nleven  200710 non-null  float16\n",
      " 48  match_categories_nlcsi   200710 non-null  float16\n",
      " 49  bert_sim_categories      200710 non-null  float16\n",
      " 50  bert_sim_name            200710 non-null  float16\n",
      " 51  id                       200710 non-null  object \n",
      " 52  match_id                 200710 non-null  object \n",
      "dtypes: float16(51), object(2)\n",
      "memory usage: 24.1+ MB\n",
      "None\n",
      "[W] [20:43:33.560654] Treelite currently does not support float64 model parameters. Accuracy may degrade slightly relative to native LightGBM invocation.\n",
      "[W] [20:43:34.963854] Casting all thresholds and leaf values to float32, as FIL currently doesn't support inferencing models with float64 values. This may lead to predictions with reduced accuracy.\n",
      "[W] [20:43:36.512510] Treelite currently does not support float64 model parameters. Accuracy may degrade slightly relative to native LightGBM invocation.\n",
      "[W] [20:43:38.010244] Casting all thresholds and leaf values to float32, as FIL currently doesn't support inferencing models with float64 values. This may lead to predictions with reduced accuracy.\n",
      "[W] [20:43:39.686029] Treelite currently does not support float64 model parameters. Accuracy may degrade slightly relative to native LightGBM invocation.\n",
      "[W] [20:43:41.234617] Casting all thresholds and leaf values to float32, as FIL currently doesn't support inferencing models with float64 values. This may lead to predictions with reduced accuracy.\n",
      "[W] [20:43:42.939809] Treelite currently does not support float64 model parameters. Accuracy may degrade slightly relative to native LightGBM invocation.\n",
      "[W] [20:43:44.424241] Casting all thresholds and leaf values to float32, as FIL currently doesn't support inferencing models with float64 values. This may lead to predictions with reduced accuracy.\n",
      "[W] [20:43:46.168626] Treelite currently does not support float64 model parameters. Accuracy may degrade slightly relative to native LightGBM invocation.\n",
      "[W] [20:43:47.684864] Casting all thresholds and leaf values to float32, as FIL currently doesn't support inferencing models with float64 values. This may lead to predictions with reduced accuracy.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 113882/113882 [00:00<00:00, 1068726.89it/s]\n"
     ]
    }
   ],
   "source": [
    "test_origin = pd.read_csv(INPUT_DIR + \"test.csv\")\n",
    "sub = pd.read_csv(INPUT_DIR + \"sample_submission.csv\", usecols=[\"id\"])\n",
    "\n",
    "if len(test_origin) == 5:\n",
    "    test_origin = pd.read_csv(INPUT_DIR + \"train.csv\")\n",
    "    kf = GroupKFold(n_splits=10)\n",
    "    for i, (trn_idx, val_idx) in enumerate(kf.split(test_origin, test_origin['point_of_interest'], test_origin['point_of_interest'])):\n",
    "        test_origin = test_origin.loc[val_idx]\n",
    "        test_origin = test_origin.reset_index(drop=True)\n",
    "        sub = test_origin[[\"id\"]].copy()\n",
    "        break\n",
    "test_origin = preprocess(test_origin)\n",
    "\n",
    "kf = GroupKFold(n_splits=5)\n",
    "for i, (trn_idx, val_idx) in enumerate(kf.split(test_origin, test_origin['country'].fillna(\"\"), test_origin['country'].fillna(\"\"))):\n",
    "    test_origin.loc[val_idx, \"c_fold\"] = int(i)\n",
    "\n",
    "country_tests = []\n",
    "for c_fold in test_origin[\"c_fold\"].unique():\n",
    "    test_origin_batch = test_origin[test_origin[\"c_fold\"]==c_fold].copy()\n",
    "    test_origin_batch = test_origin_batch.reset_index(drop=True)\n",
    "    test_origin_batch =test_origin_batch.drop(columns=[\"c_fold\"])\n",
    "\n",
    "    # 1st stage\n",
    "    test = extract_candidate(test_origin_batch)\n",
    "\n",
    "    # 2nd stage\n",
    "    print(\"add_org\")\n",
    "    test = add_orgin_data(test, test_origin_batch)\n",
    "    \n",
    "    print(\"add_dist_feat\")\n",
    "    test = add_distance_features(test)\n",
    "\n",
    "    # reduce memory\n",
    "    test = test.drop(columns=['latitude', 'longitude', 'address', 'city', 'state', 'zip', 'country', 'url', 'phone',\n",
    "                                'match_latitude', 'match_longitude', 'match_address', 'match_city', 'match_state', \n",
    "                                'match_zip', 'match_country', 'match_url', 'match_phone'])\n",
    "    gc.collect()\n",
    "\n",
    "    print(\"add_bert_sim_cat\")\n",
    "    bert_vec_categories = make_bert_vec(test_origin_batch[[\"categories\"]], \"categories\")\n",
    "    test = add_bert_sim_features(test, bert_vec_categories, \"categories\")\n",
    "    del bert_vec_categories\n",
    "    gc.collect()\n",
    "\n",
    "    print(\"add_bert_sim_name\")\n",
    "    bert_vec_name = make_bert_vec(test_origin_batch[[\"name\"]], \"name\")\n",
    "    test = add_bert_sim_features(test, bert_vec_name, \"name\")\n",
    "    del bert_vec_name\n",
    "    gc.collect()\n",
    "    \n",
    "    with open(MODEL_DIR + 'features.pickle', 'rb') as f:\n",
    "        features = pickle.load(f)\n",
    "    test = reduce_data_size(test, features)\n",
    "    \n",
    "    print('test_dataset_info')\n",
    "    print(test.info())\n",
    "    \n",
    "    test['prob'] = model_inference(test, features)\n",
    "    test = transform_data(test, test_origin_batch)\n",
    "    country_tests.append(test)\n",
    "\n",
    "    del test, test_origin_batch\n",
    "    gc.collect()\n",
    "\n",
    "test = pd.concat(country_tests)\n",
    "test = postprocess(test)\n",
    "\n",
    "sub = sub.merge(test, on=\"id\", how=\"left\") \n",
    "sub.to_csv(OUTPUT_DIR + 'exp011/submission_batch.csv', index=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [],
   "source": [
    "sub = pd.read_csv(OUTPUT_DIR + \"exp011/submission_org.csv\")\n",
    "sub_batch = pd.read_csv(OUTPUT_DIR + \"exp011/submission_batch.csv\")\n",
    "\n",
    "test_origin = pd.read_csv(INPUT_DIR + \"train.csv\")\n",
    "kf = GroupKFold(n_splits=10)\n",
    "for i, (trn_idx, val_idx) in enumerate(kf.split(test_origin, test_origin['point_of_interest'], test_origin['point_of_interest'])):\n",
    "    test_origin = test_origin.loc[val_idx]\n",
    "    test_origin = test_origin.reset_index(drop=True)\n",
    "    break"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(113882, 2)\n",
      "(113882, 2)\n"
     ]
    }
   ],
   "source": [
    "print(sub.shape)\n",
    "print(sub_batch.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0.9196188041297237\n",
      "0.9165237983832976\n"
     ]
    }
   ],
   "source": [
    "print(get_score(sub, test_origin))\n",
    "print(get_score(sub_batch, test_origin))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>id</th>\n",
       "      <th>matches</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "Empty DataFrame\n",
       "Columns: [id, matches]\n",
       "Index: []"
      ]
     },
     "execution_count": 25,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "sub[sub[\"id\"] != sub_batch[\"id\"]]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def run_inference():\n",
    "    test_origin = pd.read_csv(INPUT_DIR + \"test.csv\")\n",
    "    sub = pd.read_csv(INPUT_DIR + \"sample_submission.csv\", usecols=[\"id\"])\n",
    "\n",
    "    if len(test_origin) == 5:\n",
    "        test_origin = pd.read_csv(INPUT_DIR + \"train.csv\")\n",
    "        kf = GroupKFold(n_splits=10)\n",
    "        for i, (trn_idx, val_idx) in enumerate(kf.split(test_origin, test_origin['point_of_interest'], test_origin['point_of_interest'])):\n",
    "            test_origin = test_origin.loc[val_idx]\n",
    "            test_origin = test_origin.reset_index(drop=True)\n",
    "            sub = test_origin[[\"id\"]].copy()\n",
    "            break\n",
    "    test_origin = preprocess(test_origin)\n",
    "\n",
    "    kf = GroupKFold(n_splits=5)\n",
    "    for i, (trn_idx, val_idx) in enumerate(kf.split(test_origin, test_origin['country'].fillna(\"\"), test_origin['country'].fillna(\"\"))):\n",
    "        test_origin.loc[val_idx, \"c_fold\"] = int(i)\n",
    "    \n",
    "    country_tests = []\n",
    "    for c_fold in test_origin[\"c_fold\"].unique():\n",
    "        test_origin_batch = test_origin[test_origin[\"c_fold\"]==c_fold].copy()\n",
    "        test_origin_batch = test_origin_batch.reset_index(drop=True)\n",
    "        test_origin_batch =test_origin_batch.drop(columns=[\"c_fold\"])\n",
    "    \n",
    "        # 1st stage\n",
    "        test = extract_candidate(test_origin_batch)\n",
    "\n",
    "        # 2nd stage\n",
    "        print(\"add_org\")\n",
    "        test = add_orgin_data(test, test_origin_batch)\n",
    "        \n",
    "        print(\"add_dist_feat\")\n",
    "        test = add_distance_features(test)\n",
    "\n",
    "        # reduce memory\n",
    "        test = test.drop(columns=['latitude', 'longitude', 'address', 'city', 'state', 'zip', 'country', 'url', 'phone',\n",
    "                                    'match_latitude', 'match_longitude', 'match_address', 'match_city', 'match_state', \n",
    "                                    'match_zip', 'match_country', 'match_url', 'match_phone'])\n",
    "        gc.collect()\n",
    "\n",
    "        print(\"add_bert_sim_cat\")\n",
    "        bert_vec_categories = make_bert_vec(test_origin_batch[[\"categories\"]], \"categories\")\n",
    "        test = add_bert_sim_features(test, bert_vec_categories, \"categories\")\n",
    "        del bert_vec_categories\n",
    "        gc.collect()\n",
    "\n",
    "        print(\"add_bert_sim_name\")\n",
    "        bert_vec_name = make_bert_vec(test_origin_batch[[\"name\"]], \"name\")\n",
    "        test = add_bert_sim_features(test, bert_vec_name, \"name\")\n",
    "        del bert_vec_name\n",
    "        gc.collect()\n",
    "        \n",
    "        with open(MODEL_DIR + 'features.pickle', 'rb') as f:\n",
    "            features = pickle.load(f)\n",
    "        test = reduce_data_size(test, features)\n",
    "        \n",
    "        print('test_dataset_info')\n",
    "        print(test.info())\n",
    "        \n",
    "        test['prob'] = model_inference(test, features)\n",
    "        test = transform_data(test, test_origin)\n",
    "        country_tests.append(test)\n",
    "\n",
    "        del test, test_origin_batch\n",
    "        gc.collect()\n",
    "\n",
    "    test = pd.concat(country_tests)\n",
    "    test = postprocess(test)\n",
    "    \n",
    "    sub = sub.merge(test, on=\"id\", how=\"left\") \n",
    "    test.to_csv(OUTPUT_DIR + 'submission_batch.csv', index=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "if MODE == 'local_train':\n",
    "    run_train()\n",
    "elif MODE == 'kaggle_inference':\n",
    "    run_inference()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "sub = pd.read_csv(OUTPUT_DIR + \"exp011/submission_batch.csv\")\n",
    "\n",
    "test_origin = pd.read_csv(INPUT_DIR + \"train.csv\")\n",
    "kf = GroupKFold(n_splits=10)\n",
    "for i, (trn_idx, val_idx) in enumerate(kf.split(test_origin, test_origin['point_of_interest'], test_origin['point_of_interest'])):\n",
    "    test_origin = test_origin.loc[val_idx]\n",
    "    test_origin = test_origin.reset_index(drop=True)\n",
    "    break"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "get_score(sub, test_origin)"
   ]
  }
 ],
 "metadata": {
  "interpreter": {
   "hash": "d4d1e4263499bec80672ea0156c357c1ee493ec2b1c70f0acce89fc37c4a6abe"
  },
  "kernelspec": {
   "display_name": "Python 3.8.12 ('base')",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.12"
  },
  "orig_nbformat": 4
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
