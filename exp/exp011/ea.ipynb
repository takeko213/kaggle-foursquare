{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# exp011_ea"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "MODE = 'local_train'\n",
    "#MODE = 'kaggle_inference'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "exp_name = 'exp011'\n",
    "memo = 'bert_vecのsvd'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import sys\n",
    "import gc\n",
    "\n",
    "if MODE == 'local_train':\n",
    "    sys.path.append('/home/kaggler/.local/lib/python3.8/site-packages')\n",
    "    from dotenv import load_dotenv\n",
    "    load_dotenv\n",
    "    sys.path.append(os.getenv('UTILS_PATH'))\n",
    "    import line_notify\n",
    "    import slack_notify\n",
    "    \n",
    "if MODE == \"kaggle_inference\":\n",
    "    from cuml import ForestInference\n",
    "    import treelite\n",
    "    \n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt; plt.style.use(\"ggplot\")\n",
    "import seaborn as sns\n",
    "from sklearn.metrics.pairwise import haversine_distances\n",
    "from sklearn.model_selection import GroupKFold, KFold, StratifiedKFold\n",
    "from sklearn.metrics import accuracy_score\n",
    "from sklearn.decomposition import TruncatedSVD\n",
    "import lightgbm as lgb\n",
    "import itertools\n",
    "from scipy.spatial.distance import canberra\n",
    "from sklearn.neighbors import KNeighborsRegressor\n",
    "import functools\n",
    "import multiprocessing\n",
    "import Levenshtein\n",
    "import difflib\n",
    "import pickle\n",
    "from tqdm import tqdm\n",
    "%load_ext Cython\n",
    "\n",
    "from transformers import DistilBertModel, DistilBertTokenizer\n",
    "from torch.utils.data import DataLoader, Dataset\n",
    "import torch.nn.functional as F\n",
    "import torch.nn as nn\n",
    "import torch\n",
    "\n",
    "from sklearn.metrics.pairwise import cosine_similarity"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "# directry_setting\n",
    "if MODE == 'local_train':\n",
    "    INPUT_DIR = os.getenv('INPUT_DIR')\n",
    "    OUTPUT_DIR = os.getenv('OUTPUT_DIR')\n",
    "    MODEL_DIR = os.getenv('OUTPUT_DIR')\n",
    "    BERT_MODEL = \"distilbert-base-uncased\"\n",
    "    os.makedirs(OUTPUT_DIR + exp_name, exist_ok=True)\n",
    "\n",
    "elif MODE == 'kaggle_inference':\n",
    "    INPUT_DIR = '/kaggle/input/foursquare-location-matching/'\n",
    "    OUTPUT_DIR = './'\n",
    "    MODEL_DIR = f'../input/fs{exp_name}/'\n",
    "    BERT_MODEL = \"../input/distilbertbaseuncased\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "# CONFIG\n",
    "SEED = 42\n",
    "N_NEIGHBORS = 10\n",
    "N_SPLITS = 5\n",
    "PROB_TH = 0.5\n",
    "MAX_LEN = 32\n",
    "BS = 512\n",
    "NW = 2\n",
    "SVD_N_COMP = 50"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "class Cat2VecModel(nn.Module):\n",
    "    def __init__(self):\n",
    "        super(Cat2VecModel, self).__init__()\n",
    "        self.distill_bert = DistilBertModel.from_pretrained(BERT_MODEL)\n",
    "        \n",
    "    def forward(self, ids, mask):\n",
    "        x = self.distill_bert(ids, mask)[0]\n",
    "        x = F.normalize((x[:, 1:, :]*mask[:, 1:, None]).mean(axis=1))\n",
    "        return x\n",
    "\n",
    "class InferenceDataset(Dataset):\n",
    "    \n",
    "    def __init__(self, df, max_len, col):\n",
    "        super().__init__()\n",
    "        self.df = df.reset_index(drop=True)\n",
    "        self.max_len = max_len\n",
    "        self.tokenizer = DistilBertTokenizer.from_pretrained(BERT_MODEL, do_lower_case=True)\n",
    "        self.col = col\n",
    "\n",
    "    def __getitem__(self, index):\n",
    "        row = self.df.iloc[index]\n",
    "        \n",
    "        inputs = self.tokenizer.encode_plus(\n",
    "            row[self.col],\n",
    "            None,\n",
    "            add_special_tokens=True,\n",
    "            max_length=MAX_LEN,\n",
    "            padding=\"max_length\",\n",
    "            return_token_type_ids=True,\n",
    "            truncation=True\n",
    "        )\n",
    "        ids = torch.LongTensor(inputs['input_ids'])\n",
    "        mask = torch.LongTensor(inputs['attention_mask'])\n",
    "\n",
    "        return ids, mask\n",
    "\n",
    "    def __len__(self):\n",
    "        return self.df.shape[0]\n",
    "\n",
    "def inference(ds):\n",
    "    cat2vec_model = Cat2VecModel()\n",
    "    cat2vec_model = cat2vec_model.cuda()\n",
    "    \n",
    "    loader = DataLoader(ds, batch_size=BS, shuffle=False, num_workers=NW,\n",
    "                        pin_memory=False, drop_last=False)\n",
    "    \n",
    "    vs = []\n",
    "    with torch.no_grad():\n",
    "        for idx, (ids, masks) in enumerate(loader):\n",
    "            v = cat2vec_model(ids.cuda(), masks.cuda()).detach().cpu().numpy()\n",
    "            vs.append(v)\n",
    "    return np.concatenate(vs)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "def make_bert_vec(df, col):\n",
    "    cat_df = df[[col]].drop_duplicates()\n",
    "    cat_df[col] = cat_df[col].fillna(\"null\")\n",
    "\n",
    "    cat_ds = InferenceDataset(cat_df, max_len=MAX_LEN, col=col)\n",
    "    V = inference(cat_ds)\n",
    "    svd = TruncatedSVD(n_components=SVD_N_COMP, random_state=SEED)\n",
    "    V = svd.fit_transform(V)\n",
    "    V = V.astype(\"float16\")\n",
    "    bert_vec = {k:v for k,v in zip(cat_df[col].values, V)}\n",
    "    return bert_vec"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "def preprocess(df):\n",
    "    columns = ['id', 'name', 'address', 'city', 'state',\n",
    "        'zip', 'country', 'url', 'phone', 'categories']\n",
    "    for c in columns:\n",
    "        if c != \"id\":\n",
    "            df[c] = df[c].astype(str).str.lower()\n",
    "    return df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "def extract_candidate(df):\n",
    "    dfs = []\n",
    "    candidates = pd.DataFrame()\n",
    "    for country, country_df in tqdm(df.groupby(\"country\")):\n",
    "        country_df = country_df.reset_index(drop=True)\n",
    "        \n",
    "        knn = KNeighborsRegressor(n_neighbors=min(len(country_df), N_NEIGHBORS), \n",
    "                                  metric='haversine', n_jobs=-1)\n",
    "        knn.fit(country_df[['latitude','longitude']], country_df.index)\n",
    "        dists, nears = knn.kneighbors(country_df[['latitude','longitude']], return_distance=True)\n",
    "        \n",
    "        k = min(len(country_df), N_NEIGHBORS)\n",
    "        country_df['match_id'] = country_df['id'].values[nears[:, :k]].tolist()\n",
    "        country_df['d_near'] = dists[:, :k].tolist()\n",
    "        dfs.append(country_df[['id','match_id','d_near']])\n",
    "    df = pd.concat(dfs).reset_index(drop=True)\n",
    "    return df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "def add_orgin_data(df, org_df):\n",
    "    df = df.explode(['match_id','d_near'])\n",
    "    df = df.loc[df['id'] != df['match_id']].copy()\n",
    "    df = df.merge(org_df, on='id')\n",
    "    df = df.merge(org_df.add_prefix('match_'), on='match_id')\n",
    "    df = df.reset_index(drop=True)\n",
    "    return df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "# https://www.kaggle.com/code/columbia2131/foursquare-iou-metrics\n",
    "def get_id2poi(input_df: pd.DataFrame) -> dict:\n",
    "    return dict(zip(input_df['id'], input_df['point_of_interest']))\n",
    "\n",
    "def get_poi2ids(input_df: pd.DataFrame) -> dict:\n",
    "    return input_df.groupby('point_of_interest')['id'].apply(set).to_dict()\n",
    "\n",
    "def get_score(input_df: pd.DataFrame, org_data):\n",
    "    scores = []\n",
    "    id2poi = get_id2poi(org_data)\n",
    "    poi2ids = get_poi2ids(org_data)\n",
    "    for id_str, matches in zip(input_df['id'].to_numpy(), input_df['matches'].to_numpy()):\n",
    "        targets = poi2ids[id2poi[id_str]]\n",
    "        preds = set(matches.split())\n",
    "        score = len((targets & preds)) / len((targets | preds))\n",
    "        scores.append(score)\n",
    "    scores = np.array(scores)\n",
    "    return scores.mean()\n",
    "\n",
    "def calc_max_score(tr_data, org_data):\n",
    "    train_candidate = pd.DataFrame()\n",
    "    train_candidate['id'] = org_data['id'].unique()\n",
    "    train_candidate['matches'] = org_data['id'].unique()\n",
    "    idx = tr_data['point_of_interest']==tr_data['match_point_of_interest']\n",
    "    train_match = tr_data.loc[idx].groupby('id')['match_id'].apply(list).map(\" \".join).reset_index()\n",
    "    train_match.columns = ['id','candidates']\n",
    "    train_candidate = train_candidate.merge(train_match, on = 'id', how = 'left')\n",
    "    idx = ~train_candidate['candidates'].isna()\n",
    "    train_candidate.loc[idx, \"matches\"] += \" \" + train_candidate.loc[idx, \"candidates\"]\n",
    "    score = get_score(train_candidate, org_data)\n",
    "    print('1st_stage_max_score : ' + '{:.5f}'.format(score))\n",
    "    return score"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "%%cython\n",
    "def LCS(str S, str T):\n",
    "    cdef int i, j\n",
    "    cdef list dp = [[0] * (len(T) + 1) for _ in range(len(S) + 1)]\n",
    "    for i in range(len(S)):\n",
    "        for j in range(len(T)):\n",
    "            dp[i + 1][j + 1] = max(dp[i][j] + (S[i] == T[j]), dp[i + 1][j], dp[i][j + 1], dp[i + 1][j + 1])\n",
    "    return dp[len(S)][len(T)]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "def _add_distance_features(args):\n",
    "    _, df = args\n",
    "\n",
    "    columns = ['name', 'address', 'city', 'state',\n",
    "               'zip', 'country', 'url', 'phone', 'categories']\n",
    "\n",
    "    for c in columns:\n",
    "        geshs = []\n",
    "        levens = []\n",
    "        jaros = []\n",
    "        lcss = []\n",
    "\n",
    "        for str1, str2 in df[[f\"{c}\", f\"match_{c}\"]].values.astype(str):\n",
    "            if str1==str1 and str2==str2:\n",
    "                geshs.append(difflib.SequenceMatcher(None, str1, str2).ratio())\n",
    "                levens.append(Levenshtein.distance(str1, str2))\n",
    "                jaros.append(Levenshtein.jaro_winkler(str1, str2))\n",
    "                lcss.append(LCS(str(str1), str(str2)))\n",
    "            else:\n",
    "                geshs.append(-1)\n",
    "                levens.append(-1)\n",
    "                jaros.append(-1)\n",
    "        df[f\"match_{c}_gesh\"] = geshs\n",
    "        df[f\"match_{c}_gesh\"] = df[f\"match_{c}_gesh\"].astype(np.float16)\n",
    "        df[f\"match_{c}_leven\"] = levens\n",
    "        df[f\"match_{c}_leven\"] = df[f\"match_{c}_leven\"].astype(np.float16)\n",
    "        df[f\"match_{c}_jaro\"] = jaros\n",
    "        df[f\"match_{c}_jaro\"] = df[f\"match_{c}_jaro\"].astype(np.float16)\n",
    "        df[f\"match_{c}_lcs\"] = lcss\n",
    "        df[f\"match_{c}_lcs\"] = df[f\"match_{c}_lcs\"].astype(np.float16)\n",
    "            \n",
    "        if not c in ['country', \"phone\", \"zip\"]:\n",
    "            df[f\"match_{c}_len\"] = df[f\"match_{c}\"].astype(str).map(len)\n",
    "            df[f\"match_{c}_nleven\"] = df[f\"match_{c}_leven\"] / df[f\"match_{c}_len\"]\n",
    "            df[f\"match_{c}_nleven\"] = df[f\"match_{c}_nleven\"].astype(np.float16)\n",
    "            df[f\"match_{c}_nlcsi\"] = df[f\"match_{c}_lcs\"] / df[f\"match_{c}_len\"]\n",
    "            df[f\"match_{c}_nlcsi\"].astype(np.float16)\n",
    "\n",
    "    return df\n",
    "\n",
    "\n",
    "def add_distance_features(df):\n",
    "    processes = multiprocessing.cpu_count()\n",
    "    with multiprocessing.Pool(processes=processes) as pool:\n",
    "        dfs = pool.imap_unordered(_add_distance_features, df.groupby('country'))\n",
    "        dfs = tqdm(dfs)\n",
    "        dfs = list(dfs)\n",
    "    df = pd.concat(dfs)\n",
    "    return df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "def add_bert_sim_features(df, bert_vec, col):\n",
    "    bert_sim_list = []\n",
    "    for str1, str2 in df[[f\"{col}\", f\"match_{col}\"]].values.astype(str):\n",
    "        if str1==str1 and str2==str2:\n",
    "            bert_sim = cosine_similarity(bert_vec[str1].reshape(1, -1), bert_vec[str2].reshape(1, -1))[0][0]\n",
    "        else:\n",
    "            bert_sim_name = -1\n",
    "        bert_sim_list.append(bert_sim)\n",
    "    df[f\"bert_sim_{col}\"] = bert_sim_list\n",
    "    df[f\"bert_sim_{col}\"] = df[f\"bert_sim_{col}\"].astype(np.float16)\n",
    "    return df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [],
   "source": [
    "def reduce_data_size(df, features):\n",
    "    if MODE == 'local_train':\n",
    "        df = df[features + ['target', 'id', 'match_id']].copy()\n",
    "    elif MODE == 'kaggle_inference':\n",
    "        df = df[features + ['id', 'match_id']].copy()\n",
    "\n",
    "\n",
    "    df[features] = df[features].astype(np.float16)\n",
    "    for _ in range(5):\n",
    "        gc.collect()\n",
    "        \n",
    "    return df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [],
   "source": [
    "def train_model(df, features):\n",
    "    params = {'objective': 'binary', \n",
    "              'boosting': 'gbdt',\n",
    "              'learning_rate': 0.1, \n",
    "              'metric': 'binary_logloss', \n",
    "              'seed': SEED, \n",
    "              'feature_pre_filter': False, \n",
    "              'lambda_l1': 0.5745709668124809, \n",
    "              'lambda_l2': 0.5123383865042099, \n",
    "              'num_leaves': 239, \n",
    "              'feature_fraction': 0.784, \n",
    "              'bagging_fraction': 1.0, \n",
    "              'bagging_freq': 0, \n",
    "              'min_child_samples': 5\n",
    "              }\n",
    "\n",
    "    # split folds\n",
    "    kf = StratifiedKFold(n_splits=N_SPLITS, shuffle=True, random_state=SEED)\n",
    "    for i, (trn_idx, val_idx) in enumerate(kf.split(df, df[\"target\"], df[\"target\"])):\n",
    "        df.loc[val_idx, \"fold\"] = i\n",
    "    \n",
    "    fi = pd.DataFrame()\n",
    "    oof = df[['id', 'match_id', 'target']].copy()\n",
    "    oof['prob'] = 0.0\n",
    "    scores = []\n",
    "\n",
    "    for i in range(N_SPLITS):\n",
    "        print('fold : ' + str(i))\n",
    "        tr_idx = df[df['fold'] != i].index\n",
    "        vl_idx = df[df['fold'] == i].index\n",
    "        tr_x, tr_y = df.loc[tr_idx, features], df.loc[tr_idx, 'target']\n",
    "        vl_x, vl_y = df.loc[vl_idx, features], df.loc[vl_idx, 'target']\n",
    "        tr_data = lgb.Dataset(tr_x, label=tr_y)\n",
    "        vl_data = lgb.Dataset(vl_x, label=vl_y)\n",
    "\n",
    "        model = lgb.train(params, tr_data, valid_sets=[tr_data, vl_data],\n",
    "                        num_boost_round=20000, early_stopping_rounds=100, verbose_eval=1000)\n",
    "\n",
    "        # 特徴量重要度\n",
    "        fi_tmp = pd.DataFrame()\n",
    "        fi_tmp['feature'] = model.feature_name()\n",
    "        fi_tmp['importance'] = model.feature_importance(importance_type='gain')\n",
    "        fi_tmp['iter'] = i\n",
    "        fi = fi.append(fi_tmp)\n",
    "\n",
    "        pred = model.predict(vl_x, num_iteration=model.best_iteration)\n",
    "        oof.loc[vl_idx, 'prob'] = pred\n",
    "\n",
    "        score = accuracy_score((pred > PROB_TH).astype(int), vl_y)\n",
    "        scores.append(score)\n",
    "        print(f'fold{i} | accuracy = ' + '{:.5f}'.format(score))\n",
    "\n",
    "        with open(OUTPUT_DIR + f'{exp_name}/model{i}.pickle', 'wb') as f:\n",
    "            pickle.dump(model, f)\n",
    "        \n",
    "    oof.to_csv(OUTPUT_DIR + f'{exp_name}/{exp_name}_oof.csv', index=False)\n",
    "\n",
    "    print('accuracy(mean) : ' + '{:.5f}'.format(np.mean(scores)))\n",
    "    print(scores)\n",
    "\n",
    "    fi_n = fi['feature'].nunique()\n",
    "    order = list(fi.groupby(\"feature\").mean().sort_values(\"importance\", ascending=False).index)\n",
    "    plt.figure(figsize=(10, fi_n*0.2))\n",
    "    sns.barplot(x=\"importance\", y=\"feature\", data=fi, order=order)\n",
    "    plt.title(f\"LGBM importance\")\n",
    "    plt.tight_layout()\n",
    "    plt.savefig(OUTPUT_DIR + f'{exp_name}/lgbm_importance.png')\n",
    "\n",
    "    return oof, np.mean(scores)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [],
   "source": [
    "def model_inference(df, features):\n",
    "    pred = np.zeros(len(df))\n",
    "    for i in range(N_SPLITS):\n",
    "        with open(MODEL_DIR + f'model{i}.pickle', 'rb') as f:\n",
    "            model = pickle.load(f)\n",
    "        model.save_model(\"test.lgb\")\n",
    "        fi = ForestInference()\n",
    "        fi = ForestInference.load(\"test.lgb\", output_class=True, model_type=\"lightgbm\")\n",
    "        pred += fi.predict(df[features]) / N_SPLITS\n",
    "    return pred"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [],
   "source": [
    "def transform_data(df, org_data):\n",
    "    train_candidate = pd.DataFrame()\n",
    "    train_candidate['id'] = org_data['id'].unique()\n",
    "    train_candidate['matches'] = org_data['id'].unique()\n",
    "\n",
    "    train_match = df[df['prob'] >= PROB_TH].copy()\n",
    "    train_match = train_match.groupby('id')['match_id'].apply(list).map(\" \".join).reset_index()\n",
    "    train_match.columns = ['id','candidates']\n",
    "    train_candidate = train_candidate.merge(train_match, on = 'id', how = 'left')\n",
    "    idx = ~train_candidate['candidates'].isna()\n",
    "    train_candidate.loc[idx, \"matches\"] += \" \" + train_candidate.loc[idx, \"candidates\"]\n",
    "    return train_candidate[['id', 'matches']]\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [],
   "source": [
    "def postprocess(df):\n",
    "    id2match = dict(zip(df[\"id\"].values, df[\"matches\"].str.split()))\n",
    "\n",
    "    for match in tqdm(df[\"matches\"]):\n",
    "        match = match.split()\n",
    "        if len(match) == 1:        \n",
    "            continue\n",
    "\n",
    "        base = match[0]\n",
    "        for m in match[1:]:\n",
    "            if not base in id2match[m]:\n",
    "                id2match[m].append(base)\n",
    "    df[\"matches\"] = df[\"id\"].map(id2match).map(\" \".join)\n",
    "    return df "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [],
   "source": [
    "def run_train():\n",
    "    train_origin = pd.read_csv(INPUT_DIR + \"train.csv\")\n",
    "    train_origin = preprocess(train_origin)\n",
    "\n",
    "    # trainデータの分割\n",
    "    kf = GroupKFold(n_splits=2)\n",
    "    for i, (trn_idx, val_idx) in enumerate(kf.split(train_origin, train_origin['point_of_interest'], train_origin['point_of_interest'])):\n",
    "        train_origin.loc[val_idx, \"set\"] = i\n",
    "\n",
    "    # 1st stage\n",
    "    train = pd.concat([\n",
    "        extract_candidate(train_origin[train_origin[\"set\"]==0]), \n",
    "        extract_candidate(train_origin[train_origin[\"set\"]==1]), \n",
    "    ])\n",
    "    train = add_orgin_data(train, train_origin)\n",
    "    stage1_max_score = calc_max_score(train, train_origin)\n",
    "\n",
    "    # 2nd stage\n",
    "    # create target\n",
    "    train['target'] = (train['point_of_interest'] == train['match_point_of_interest']).values.astype(int)\n",
    "    train[\"target\"] = train[\"target\"].fillna(0)\n",
    "\n",
    "    train = add_distance_features(train)\n",
    "    \n",
    "    # reduce memory\n",
    "    train = train.drop(columns=['latitude', 'longitude', 'address', 'city', 'state', 'zip', 'country', 'url', 'phone',\n",
    "                                'match_latitude', 'match_longitude', 'match_address', 'match_city', 'match_state', \n",
    "                                'match_zip', 'match_country', 'match_url', 'match_phone'])\n",
    "    gc.collect()\n",
    "\n",
    "    # bert類似度\n",
    "    bert_vec_categories = make_bert_vec(train_origin[[\"categories\"]], \"categories\")\n",
    "    train = add_bert_sim_features(train, bert_vec_categories, \"categories\")\n",
    "    del bert_vec_categories\n",
    "    gc.collect()\n",
    "\n",
    "    print(\"add_bert_sim_name\")\n",
    "    bert_vec_name = make_bert_vec(train_origin[[\"name\"]], \"name\")\n",
    "    train = add_bert_sim_features(train, bert_vec_name, \"name\")\n",
    "    del bert_vec_name\n",
    "    gc.collect()\n",
    "\n",
    "    not_use_cols = ['match_state_leven', 'address', 'categories', 'point_of_interest', 'match_address_leven',\n",
    "                    'city', 'match_point_of_interest', 'match_name', 'match_categories_leven', 'match_longitude',\n",
    "                    'target', 'match_city_leven', 'zip', 'match_categories', 'match_city', 'match_latitude',\n",
    "                    'match_zip', 'match_url', 'id', 'match_set', 'country', 'match_state', 'match_address',\n",
    "                    'match_name_leven', 'match_id', 'latitude', 'url', 'set', 'name', 'phone', 'longitude',\n",
    "                    'match_url_leven', 'state', 'match_phone', 'match_country']\n",
    "    features = [c for c in train.columns if c not in not_use_cols]\n",
    "    with open(OUTPUT_DIR + f'{exp_name}/features.pickle', 'wb') as f:\n",
    "        pickle.dump(features, f)\n",
    "\n",
    "    train = reduce_data_size(train, features)\n",
    "\n",
    "    oof, stage2_mean_accuracy = train_model(train, features)\n",
    "    oof = transform_data(oof, train_origin)\n",
    "\n",
    "    cv_score = get_score(oof, train_origin)\n",
    "    print(f'cv_score = ' + '{:.5f}'.format(cv_score))\n",
    "\n",
    "    oof = postprocess(oof)\n",
    "    cv_score_after_pp = get_score(oof, train_origin)\n",
    "    print(f'cv_score(after_pp) = ' + '{:.5f}'.format(cv_score_after_pp))\n",
    "\n",
    "\n",
    "    report = f'{exp_name}\\n'\n",
    "    report += memo + '\\n'\n",
    "    report += 'stage1_max_score : ' + '{:.5f}'.format(stage1_max_score) + '\\n'\n",
    "    report += 'stage2_mean_accuracy : ' + '{:.5f}'.format(stage2_mean_accuracy) + '\\n'\n",
    "    report += 'cv_score : ' + '{:.5f}'.format(cv_score) + '\\n'\n",
    "    report += 'cv_score_after_pp : ' + '{:.5f}'.format(cv_score_after_pp) + '\\n'\n",
    "    print(report)\n",
    "    line_notify.send(report)\n",
    "    slack_notify.send(report)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [],
   "source": [
    "def run_inference():\n",
    "    test_origin = pd.read_csv(INPUT_DIR + \"test.csv\")\n",
    "\n",
    "    if len(test_origin) == 5:\n",
    "        test_origin = pd.read_csv(INPUT_DIR + \"train.csv\")\n",
    "        kf = GroupKFold(n_splits=2)\n",
    "        for i, (trn_idx, val_idx) in enumerate(kf.split(test_origin, test_origin['point_of_interest'], test_origin['point_of_interest'])):\n",
    "            test_origin = test_origin.loc[trn_idx]\n",
    "            break\n",
    "    \n",
    "    test_origin = preprocess(test_origin)\n",
    "\n",
    "    # 1st stage\n",
    "    test = extract_candidate(test_origin)\n",
    "\n",
    "    # 2nd stage\n",
    "    print(\"add_org\")\n",
    "    test = add_orgin_data(test, test_origin)\n",
    "    \n",
    "    print(\"add_dist_feat\")\n",
    "    test = add_distance_features(test)\n",
    "\n",
    "    # reduce memory\n",
    "    test = test.drop(columns=['latitude', 'longitude', 'address', 'city', 'state', 'zip', 'country', 'url', 'phone',\n",
    "                                'match_latitude', 'match_longitude', 'match_address', 'match_city', 'match_state', \n",
    "                                'match_zip', 'match_country', 'match_url', 'match_phone'])\n",
    "    gc.collect()\n",
    "\n",
    "    print(\"add_bert_sim_cat\")\n",
    "    bert_vec_categories = make_bert_vec(test_origin[[\"categories\"]], \"categories\")\n",
    "    test = add_bert_sim_features(test, bert_vec_categories, \"categories\")\n",
    "    del bert_vec_categories\n",
    "    gc.collect()\n",
    "\n",
    "    print(\"add_bert_sim_name\")\n",
    "    bert_vec_name = make_bert_vec(test_origin[[\"name\"]], \"name\")\n",
    "    test = add_bert_sim_features(test, bert_vec_name, \"name\")\n",
    "    del bert_vec_name\n",
    "    gc.collect()\n",
    "    \n",
    "    with open(MODEL_DIR + 'features.pickle', 'rb') as f:\n",
    "        features = pickle.load(f)\n",
    "    test = reduce_data_size(test, features)\n",
    "    \n",
    "    print('test_dataset_info')\n",
    "    print(test.info())\n",
    "    \n",
    "    test['prob'] = model_inference(test, features)\n",
    "    test = transform_data(test, test_origin)\n",
    "    test = postprocess(test)\n",
    "    test.to_csv('submission.csv', index=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [],
   "source": [
    "oof = pd.read_csv(OUTPUT_DIR + \"exp011/exp011_oof.csv\")\n",
    "train_origin = pd.read_csv(INPUT_DIR + \"train.csv\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [],
   "source": [
    "oof = oof.merge(train_origin, on='id')\n",
    "oof = oof.merge(train_origin.add_prefix('match_'), on='match_id')\n",
    "oof = oof.reset_index(drop=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "metadata": {},
   "outputs": [],
   "source": [
    "oof[\"pred\"] = (oof[\"prob\"] > 0.5).astype(int)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "metadata": {},
   "outputs": [],
   "source": [
    "oof[\"correct\"] = (oof[\"target\"] == oof[\"pred\"]).astype(int)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "metadata": {},
   "outputs": [],
   "source": [
    "agg_df = oof.groupby(\"country\")[\"correct\"].agg([\"mean\", \"count\"]).reset_index()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "metadata": {},
   "outputs": [],
   "source": [
    "agg_df = agg_df.sort_values(\"count\", ascending=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 46,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>id</th>\n",
       "      <th>match_id</th>\n",
       "      <th>target</th>\n",
       "      <th>prob</th>\n",
       "      <th>name</th>\n",
       "      <th>latitude</th>\n",
       "      <th>longitude</th>\n",
       "      <th>address</th>\n",
       "      <th>city</th>\n",
       "      <th>state</th>\n",
       "      <th>...</th>\n",
       "      <th>match_city</th>\n",
       "      <th>match_state</th>\n",
       "      <th>match_zip</th>\n",
       "      <th>match_country</th>\n",
       "      <th>match_url</th>\n",
       "      <th>match_phone</th>\n",
       "      <th>match_categories</th>\n",
       "      <th>match_point_of_interest</th>\n",
       "      <th>pred</th>\n",
       "      <th>correct</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>7002958</th>\n",
       "      <td>E_000008a8ba4f48</td>\n",
       "      <td>E_c03acb4032c33d</td>\n",
       "      <td>0</td>\n",
       "      <td>0.002576</td>\n",
       "      <td>Turkcell</td>\n",
       "      <td>37.844510</td>\n",
       "      <td>27.844202</td>\n",
       "      <td>Adnan Menderes Bulvarı</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>...</td>\n",
       "      <td>Aydın</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>TR</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>Recreation Centers</td>\n",
       "      <td>P_0d28feefc667bc</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>7002959</th>\n",
       "      <td>E_0665b13bcc35e8</td>\n",
       "      <td>E_c03acb4032c33d</td>\n",
       "      <td>0</td>\n",
       "      <td>0.000204</td>\n",
       "      <td>Tığ Ekmek</td>\n",
       "      <td>37.843413</td>\n",
       "      <td>27.846491</td>\n",
       "      <td>NaN</td>\n",
       "      <td>Aydın</td>\n",
       "      <td>NaN</td>\n",
       "      <td>...</td>\n",
       "      <td>Aydın</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>TR</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>Recreation Centers</td>\n",
       "      <td>P_0d28feefc667bc</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>7002960</th>\n",
       "      <td>E_2995bbffade020</td>\n",
       "      <td>E_c03acb4032c33d</td>\n",
       "      <td>0</td>\n",
       "      <td>0.000225</td>\n",
       "      <td>Sarıoğlu Apartmanı</td>\n",
       "      <td>37.843295</td>\n",
       "      <td>27.845953</td>\n",
       "      <td>Efeler Mahallesi</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>...</td>\n",
       "      <td>Aydın</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>TR</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>Recreation Centers</td>\n",
       "      <td>P_0d28feefc667bc</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>7002961</th>\n",
       "      <td>E_40c486192f6105</td>\n",
       "      <td>E_c03acb4032c33d</td>\n",
       "      <td>0</td>\n",
       "      <td>0.003286</td>\n",
       "      <td>ANker travel</td>\n",
       "      <td>37.844558</td>\n",
       "      <td>27.844448</td>\n",
       "      <td>Adnan Menderes Bulvarı No 25</td>\n",
       "      <td>Aydın</td>\n",
       "      <td>NaN</td>\n",
       "      <td>...</td>\n",
       "      <td>Aydın</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>TR</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>Recreation Centers</td>\n",
       "      <td>P_0d28feefc667bc</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>7002962</th>\n",
       "      <td>E_84f4a52f7f843f</td>\n",
       "      <td>E_c03acb4032c33d</td>\n",
       "      <td>0</td>\n",
       "      <td>0.000170</td>\n",
       "      <td>Aydın Kadın Doğum ve Çocuk Hastalıkları Hastanesi</td>\n",
       "      <td>37.843430</td>\n",
       "      <td>27.847287</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>...</td>\n",
       "      <td>Aydın</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>TR</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>Recreation Centers</td>\n",
       "      <td>P_0d28feefc667bc</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>...</th>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>8039574</th>\n",
       "      <td>E_a9ff5ff26a64ab</td>\n",
       "      <td>E_a5533a74044a01</td>\n",
       "      <td>0</td>\n",
       "      <td>0.000081</td>\n",
       "      <td>Aheste</td>\n",
       "      <td>40.989272</td>\n",
       "      <td>29.134955</td>\n",
       "      <td>Mozaik Carsi</td>\n",
       "      <td>Istanbul</td>\n",
       "      <td>NaN</td>\n",
       "      <td>...</td>\n",
       "      <td>İstanbul</td>\n",
       "      <td>Ataşehir</td>\n",
       "      <td>NaN</td>\n",
       "      <td>TR</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>Steakhouses, Turkish Restaurants</td>\n",
       "      <td>P_199580249383d3</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>8039575</th>\n",
       "      <td>E_65b35aac7cc6c0</td>\n",
       "      <td>E_7f629cdf44b1b5</td>\n",
       "      <td>0</td>\n",
       "      <td>0.000513</td>\n",
       "      <td>Sevda Home</td>\n",
       "      <td>39.020108</td>\n",
       "      <td>31.791042</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>...</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>TR</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>P_fa5b307bde2213</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>8039576</th>\n",
       "      <td>E_9fa0173c51db20</td>\n",
       "      <td>E_7f629cdf44b1b5</td>\n",
       "      <td>1</td>\n",
       "      <td>0.993440</td>\n",
       "      <td>Mehmetoğulları Termal Otel</td>\n",
       "      <td>39.497564</td>\n",
       "      <td>35.373566</td>\n",
       "      <td>NaN</td>\n",
       "      <td>Sarıkaya</td>\n",
       "      <td>Yozgat</td>\n",
       "      <td>...</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>TR</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>P_fa5b307bde2213</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>8039577</th>\n",
       "      <td>E_65b35aac7cc6c0</td>\n",
       "      <td>E_9fa0173c51db20</td>\n",
       "      <td>0</td>\n",
       "      <td>0.000020</td>\n",
       "      <td>Sevda Home</td>\n",
       "      <td>39.020108</td>\n",
       "      <td>31.791042</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>...</td>\n",
       "      <td>Sarıkaya</td>\n",
       "      <td>Yozgat</td>\n",
       "      <td>NaN</td>\n",
       "      <td>TR</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>Hotels</td>\n",
       "      <td>P_fa5b307bde2213</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>8039578</th>\n",
       "      <td>E_7f629cdf44b1b5</td>\n",
       "      <td>E_9fa0173c51db20</td>\n",
       "      <td>1</td>\n",
       "      <td>0.948723</td>\n",
       "      <td>Mehmetoğulları Thermal Resort</td>\n",
       "      <td>39.497831</td>\n",
       "      <td>35.373512</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>...</td>\n",
       "      <td>Sarıkaya</td>\n",
       "      <td>Yozgat</td>\n",
       "      <td>NaN</td>\n",
       "      <td>TR</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>Hotels</td>\n",
       "      <td>P_fa5b307bde2213</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>1036621 rows × 30 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "                       id          match_id  target      prob  \\\n",
       "7002958  E_000008a8ba4f48  E_c03acb4032c33d       0  0.002576   \n",
       "7002959  E_0665b13bcc35e8  E_c03acb4032c33d       0  0.000204   \n",
       "7002960  E_2995bbffade020  E_c03acb4032c33d       0  0.000225   \n",
       "7002961  E_40c486192f6105  E_c03acb4032c33d       0  0.003286   \n",
       "7002962  E_84f4a52f7f843f  E_c03acb4032c33d       0  0.000170   \n",
       "...                   ...               ...     ...       ...   \n",
       "8039574  E_a9ff5ff26a64ab  E_a5533a74044a01       0  0.000081   \n",
       "8039575  E_65b35aac7cc6c0  E_7f629cdf44b1b5       0  0.000513   \n",
       "8039576  E_9fa0173c51db20  E_7f629cdf44b1b5       1  0.993440   \n",
       "8039577  E_65b35aac7cc6c0  E_9fa0173c51db20       0  0.000020   \n",
       "8039578  E_7f629cdf44b1b5  E_9fa0173c51db20       1  0.948723   \n",
       "\n",
       "                                                      name   latitude  \\\n",
       "7002958                                           Turkcell  37.844510   \n",
       "7002959                                          Tığ Ekmek  37.843413   \n",
       "7002960                                 Sarıoğlu Apartmanı  37.843295   \n",
       "7002961                                       ANker travel  37.844558   \n",
       "7002962  Aydın Kadın Doğum ve Çocuk Hastalıkları Hastanesi  37.843430   \n",
       "...                                                    ...        ...   \n",
       "8039574                                             Aheste  40.989272   \n",
       "8039575                                         Sevda Home  39.020108   \n",
       "8039576                         Mehmetoğulları Termal Otel  39.497564   \n",
       "8039577                                         Sevda Home  39.020108   \n",
       "8039578                      Mehmetoğulları Thermal Resort  39.497831   \n",
       "\n",
       "         longitude                       address      city   state  ...  \\\n",
       "7002958  27.844202        Adnan Menderes Bulvarı       NaN     NaN  ...   \n",
       "7002959  27.846491                           NaN     Aydın     NaN  ...   \n",
       "7002960  27.845953              Efeler Mahallesi       NaN     NaN  ...   \n",
       "7002961  27.844448  Adnan Menderes Bulvarı No 25     Aydın     NaN  ...   \n",
       "7002962  27.847287                           NaN       NaN     NaN  ...   \n",
       "...            ...                           ...       ...     ...  ...   \n",
       "8039574  29.134955                  Mozaik Carsi  Istanbul     NaN  ...   \n",
       "8039575  31.791042                           NaN       NaN     NaN  ...   \n",
       "8039576  35.373566                           NaN  Sarıkaya  Yozgat  ...   \n",
       "8039577  31.791042                           NaN       NaN     NaN  ...   \n",
       "8039578  35.373512                           NaN       NaN     NaN  ...   \n",
       "\n",
       "        match_city match_state match_zip match_country match_url match_phone  \\\n",
       "7002958      Aydın         NaN       NaN            TR       NaN         NaN   \n",
       "7002959      Aydın         NaN       NaN            TR       NaN         NaN   \n",
       "7002960      Aydın         NaN       NaN            TR       NaN         NaN   \n",
       "7002961      Aydın         NaN       NaN            TR       NaN         NaN   \n",
       "7002962      Aydın         NaN       NaN            TR       NaN         NaN   \n",
       "...            ...         ...       ...           ...       ...         ...   \n",
       "8039574   İstanbul    Ataşehir       NaN            TR       NaN         NaN   \n",
       "8039575        NaN         NaN       NaN            TR       NaN         NaN   \n",
       "8039576        NaN         NaN       NaN            TR       NaN         NaN   \n",
       "8039577   Sarıkaya      Yozgat       NaN            TR       NaN         NaN   \n",
       "8039578   Sarıkaya      Yozgat       NaN            TR       NaN         NaN   \n",
       "\n",
       "                         match_categories  match_point_of_interest  pred  \\\n",
       "7002958                Recreation Centers         P_0d28feefc667bc     0   \n",
       "7002959                Recreation Centers         P_0d28feefc667bc     0   \n",
       "7002960                Recreation Centers         P_0d28feefc667bc     0   \n",
       "7002961                Recreation Centers         P_0d28feefc667bc     0   \n",
       "7002962                Recreation Centers         P_0d28feefc667bc     0   \n",
       "...                                   ...                      ...   ...   \n",
       "8039574  Steakhouses, Turkish Restaurants         P_199580249383d3     0   \n",
       "8039575                               NaN         P_fa5b307bde2213     0   \n",
       "8039576                               NaN         P_fa5b307bde2213     1   \n",
       "8039577                            Hotels         P_fa5b307bde2213     0   \n",
       "8039578                            Hotels         P_fa5b307bde2213     1   \n",
       "\n",
       "        correct  \n",
       "7002958       1  \n",
       "7002959       1  \n",
       "7002960       1  \n",
       "7002961       1  \n",
       "7002962       1  \n",
       "...         ...  \n",
       "8039574       1  \n",
       "8039575       1  \n",
       "8039576       1  \n",
       "8039577       1  \n",
       "8039578       1  \n",
       "\n",
       "[1036621 rows x 30 columns]"
      ]
     },
     "execution_count": 46,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "oof[oof[\"country\"]==\"TR\"]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 45,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>country</th>\n",
       "      <th>mean</th>\n",
       "      <th>count</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>196</th>\n",
       "      <td>US</td>\n",
       "      <td>0.989904</td>\n",
       "      <td>2209034</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>190</th>\n",
       "      <td>TR</td>\n",
       "      <td>0.971223</td>\n",
       "      <td>1036621</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>86</th>\n",
       "      <td>ID</td>\n",
       "      <td>0.973492</td>\n",
       "      <td>997193</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>98</th>\n",
       "      <td>JP</td>\n",
       "      <td>0.985911</td>\n",
       "      <td>630288</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>184</th>\n",
       "      <td>TH</td>\n",
       "      <td>0.974816</td>\n",
       "      <td>529272</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>162</th>\n",
       "      <td>RU</td>\n",
       "      <td>0.961461</td>\n",
       "      <td>513270</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>30</th>\n",
       "      <td>BR</td>\n",
       "      <td>0.988041</td>\n",
       "      <td>461169</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>135</th>\n",
       "      <td>MY</td>\n",
       "      <td>0.982151</td>\n",
       "      <td>416934</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>19</th>\n",
       "      <td>BE</td>\n",
       "      <td>0.985176</td>\n",
       "      <td>232392</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>66</th>\n",
       "      <td>GB</td>\n",
       "      <td>0.990787</td>\n",
       "      <td>229896</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>150</th>\n",
       "      <td>PH</td>\n",
       "      <td>0.985593</td>\n",
       "      <td>199620</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>134</th>\n",
       "      <td>MX</td>\n",
       "      <td>0.981689</td>\n",
       "      <td>192564</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>168</th>\n",
       "      <td>SG</td>\n",
       "      <td>0.979063</td>\n",
       "      <td>189045</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>104</th>\n",
       "      <td>KR</td>\n",
       "      <td>0.983392</td>\n",
       "      <td>171783</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>50</th>\n",
       "      <td>DE</td>\n",
       "      <td>0.987256</td>\n",
       "      <td>160317</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>65</th>\n",
       "      <td>FR</td>\n",
       "      <td>0.982887</td>\n",
       "      <td>127566</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>60</th>\n",
       "      <td>ES</td>\n",
       "      <td>0.987946</td>\n",
       "      <td>116307</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>36</th>\n",
       "      <td>CA</td>\n",
       "      <td>0.991392</td>\n",
       "      <td>107343</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>94</th>\n",
       "      <td>IT</td>\n",
       "      <td>0.985091</td>\n",
       "      <td>102555</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>164</th>\n",
       "      <td>SA</td>\n",
       "      <td>0.975911</td>\n",
       "      <td>97803</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "    country      mean    count\n",
       "196      US  0.989904  2209034\n",
       "190      TR  0.971223  1036621\n",
       "86       ID  0.973492   997193\n",
       "98       JP  0.985911   630288\n",
       "184      TH  0.974816   529272\n",
       "162      RU  0.961461   513270\n",
       "30       BR  0.988041   461169\n",
       "135      MY  0.982151   416934\n",
       "19       BE  0.985176   232392\n",
       "66       GB  0.990787   229896\n",
       "150      PH  0.985593   199620\n",
       "134      MX  0.981689   192564\n",
       "168      SG  0.979063   189045\n",
       "104      KR  0.983392   171783\n",
       "50       DE  0.987256   160317\n",
       "65       FR  0.982887   127566\n",
       "60       ES  0.987946   116307\n",
       "36       CA  0.991392   107343\n",
       "94       IT  0.985091   102555\n",
       "164      SA  0.975911    97803"
      ]
     },
     "execution_count": 45,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "agg_df.head(20)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 44,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>country</th>\n",
       "      <th>mean</th>\n",
       "      <th>count</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>196</th>\n",
       "      <td>US</td>\n",
       "      <td>0.989904</td>\n",
       "      <td>2209034</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>190</th>\n",
       "      <td>TR</td>\n",
       "      <td>0.971223</td>\n",
       "      <td>1036621</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>86</th>\n",
       "      <td>ID</td>\n",
       "      <td>0.973492</td>\n",
       "      <td>997193</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>98</th>\n",
       "      <td>JP</td>\n",
       "      <td>0.985911</td>\n",
       "      <td>630288</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>184</th>\n",
       "      <td>TH</td>\n",
       "      <td>0.974816</td>\n",
       "      <td>529272</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>...</th>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>64</th>\n",
       "      <td>FO</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>2</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>177</th>\n",
       "      <td>ST</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>2</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>183</th>\n",
       "      <td>TG</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>2</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>24</th>\n",
       "      <td>BJ</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>2</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>59</th>\n",
       "      <td>EH</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>2</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>212 rows × 3 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "    country      mean    count\n",
       "196      US  0.989904  2209034\n",
       "190      TR  0.971223  1036621\n",
       "86       ID  0.973492   997193\n",
       "98       JP  0.985911   630288\n",
       "184      TH  0.974816   529272\n",
       "..      ...       ...      ...\n",
       "64       FO  0.000000        2\n",
       "177      ST  1.000000        2\n",
       "183      TG  1.000000        2\n",
       "24       BJ  1.000000        2\n",
       "59       EH  1.000000        2\n",
       "\n",
       "[212 rows x 3 columns]"
      ]
     },
     "execution_count": 44,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "agg_df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "interpreter": {
   "hash": "d4d1e4263499bec80672ea0156c357c1ee493ec2b1c70f0acce89fc37c4a6abe"
  },
  "kernelspec": {
   "display_name": "Python 3.8.12 ('base')",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.12"
  },
  "orig_nbformat": 4
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
