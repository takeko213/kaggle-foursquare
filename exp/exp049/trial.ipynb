{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# exp049_trial"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "MODE = 'local_train'\n",
    "#MODE = 'kaggle_inference'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "exp_name = 'exp049'\n",
    "memo = '1st改善'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import sys\n",
    "import gc\n",
    "\n",
    "if MODE == 'local_train':\n",
    "    sys.path.append('/home/kaggler/.local/lib/python3.8/site-packages')\n",
    "    from dotenv import load_dotenv\n",
    "    load_dotenv\n",
    "    sys.path.append(os.getenv('UTILS_PATH'))\n",
    "    import line_notify\n",
    "    import slack_notify\n",
    "    \n",
    "if MODE == \"kaggle_inference\":\n",
    "    from cuml import ForestInference\n",
    "    import treelite\n",
    "    \n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt; plt.style.use(\"ggplot\")\n",
    "import seaborn as sns\n",
    "from sklearn.metrics.pairwise import haversine_distances\n",
    "from sklearn.model_selection import GroupKFold, KFold, StratifiedKFold\n",
    "from sklearn.metrics import accuracy_score\n",
    "from sklearn.decomposition import TruncatedSVD\n",
    "import lightgbm as lgb\n",
    "import itertools\n",
    "from scipy.spatial.distance import canberra\n",
    "from cuml.neighbors import KNeighborsRegressor\n",
    "import functools\n",
    "import multiprocessing\n",
    "import Levenshtein\n",
    "import difflib\n",
    "import pickle\n",
    "from tqdm import tqdm\n",
    "%load_ext Cython\n",
    "\n",
    "from transformers import DistilBertModel, DistilBertTokenizer\n",
    "from torch.utils.data import DataLoader, Dataset\n",
    "import torch.nn.functional as F\n",
    "import torch.nn as nn\n",
    "import torch\n",
    "\n",
    "from sklearn.metrics.pairwise import cosine_similarity\n",
    "from sklearn.feature_extraction.text import TfidfVectorizer\n",
    "\n",
    "from unidecode import unidecode\n",
    "import pykakasi"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "# directry_setting\n",
    "if MODE == 'local_train':\n",
    "    INPUT_DIR = os.getenv('INPUT_DIR')\n",
    "    OUTPUT_DIR = os.getenv('OUTPUT_DIR')\n",
    "    MODEL_DIR = os.getenv('OUTPUT_DIR')\n",
    "    BERT_MODEL = \"distilbert-base-multilingual-cased\"\n",
    "    os.makedirs(OUTPUT_DIR + exp_name, exist_ok=True)\n",
    "\n",
    "elif MODE == 'kaggle_inference':\n",
    "    INPUT_DIR = '/kaggle/input/foursquare-location-matching/'\n",
    "    OUTPUT_DIR = './'\n",
    "    MODEL_DIR = f'../input/fs{exp_name}/'\n",
    "    BERT_MODEL = \"../input/distilbertbaseuncased\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "# CONFIG\n",
    "SEED = 42\n",
    "N_NEIGHBORS = 10\n",
    "N_SPLITS = 5\n",
    "PROB_TH = 0.5\n",
    "MAX_LEN = 32\n",
    "BS = 512\n",
    "NW = 2\n",
    "SVD_N_COMP = 50"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "class Cat2VecModel(nn.Module):\n",
    "    def __init__(self):\n",
    "        super(Cat2VecModel, self).__init__()\n",
    "        self.distill_bert = DistilBertModel.from_pretrained(BERT_MODEL)\n",
    "        \n",
    "    def forward(self, ids, mask):\n",
    "        x = self.distill_bert(ids, mask)[0]\n",
    "        x = F.normalize((x[:, 1:, :]*mask[:, 1:, None]).mean(axis=1))\n",
    "        return x\n",
    "\n",
    "class InferenceDataset(Dataset):\n",
    "    \n",
    "    def __init__(self, df, max_len, col):\n",
    "        super().__init__()\n",
    "        self.df = df.reset_index(drop=True)\n",
    "        self.max_len = max_len\n",
    "        self.tokenizer = DistilBertTokenizer.from_pretrained(BERT_MODEL, do_lower_case=True)\n",
    "        self.col = col\n",
    "\n",
    "    def __getitem__(self, index):\n",
    "        row = self.df.iloc[index]\n",
    "        \n",
    "        inputs = self.tokenizer.encode_plus(\n",
    "            row[self.col],\n",
    "            None,\n",
    "            add_special_tokens=True,\n",
    "            max_length=MAX_LEN,\n",
    "            padding=\"max_length\",\n",
    "            return_token_type_ids=True,\n",
    "            truncation=True\n",
    "        )\n",
    "        ids = torch.LongTensor(inputs['input_ids'])\n",
    "        mask = torch.LongTensor(inputs['attention_mask'])\n",
    "\n",
    "        return ids, mask\n",
    "\n",
    "    def __len__(self):\n",
    "        return self.df.shape[0]\n",
    "\n",
    "def inference(ds):\n",
    "    cat2vec_model = Cat2VecModel()\n",
    "    cat2vec_model = cat2vec_model.cuda()\n",
    "    \n",
    "    loader = DataLoader(ds, batch_size=BS, shuffle=False, num_workers=NW,\n",
    "                        pin_memory=False, drop_last=False)\n",
    "    \n",
    "    vs = []\n",
    "    with torch.no_grad():\n",
    "        for idx, (ids, masks) in enumerate(loader):\n",
    "            v = cat2vec_model(ids.cuda(), masks.cuda()).detach().cpu().numpy()\n",
    "            vs.append(v)\n",
    "    return np.concatenate(vs)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "def make_bert_vec(df, col):\n",
    "    cat_df = df[[col]].drop_duplicates()\n",
    "    cat_df[col] = cat_df[col].fillna(\"null\")\n",
    "\n",
    "    cat_ds = InferenceDataset(cat_df, max_len=MAX_LEN, col=col)\n",
    "    V = inference(cat_ds)\n",
    "    #svd = TruncatedSVD(n_components=SVD_N_COMP, random_state=SEED)\n",
    "    #V = svd.fit_transform(V)\n",
    "    V = V.astype(\"float16\")\n",
    "    bert_vec = {k:v for k,v in zip(cat_df[col].values, V)}\n",
    "    return bert_vec"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "def preprocess(df):\n",
    "    columns = ['id', 'name', 'address', 'city', 'state',\n",
    "        'zip', 'country', 'url', 'phone', 'categories']\n",
    "    for c in columns:\n",
    "        if c != \"id\":\n",
    "            df[c] = df[c].astype(str).str.lower()\n",
    "\n",
    "    df[[\"latitude\", \"longitude\"]] = np.deg2rad(df[[\"latitude\", \"longitude\"]])\n",
    "    \n",
    "    return df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "from cuml.feature_extraction.text import TfidfVectorizer as TfidfVectorizer_gpu\n",
    "import cudf, cuml, cupy\n",
    "from cuml.neighbors import NearestNeighbors as NearestNeighbors_gpu"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "def extract_candidate_dist(df):\n",
    "    dfs = []\n",
    "    for country, country_df in tqdm(df.groupby(\"country\")):\n",
    "        country_df = country_df.reset_index(drop=True)\n",
    "        \n",
    "        knn = KNeighborsRegressor(n_neighbors=min(len(country_df), N_NEIGHBORS), \n",
    "                                    metric='haversine', algorithm=\"brute\")\n",
    "        knn.fit(country_df[['latitude','longitude']], country_df.index.values)\n",
    "        nears = knn.kneighbors(country_df[['latitude','longitude']], return_distance=False)\n",
    "        \n",
    "        k = min(len(country_df), N_NEIGHBORS)\n",
    "        country_df['match_id'] = country_df['id'].values[nears[:, :k]].tolist()\n",
    "        country_df = country_df.explode(['match_id'])\n",
    "        country_df = country_df.loc[country_df['id'] != country_df['match_id']].copy()\n",
    "        dfs.append(country_df)\n",
    "    df = pd.concat(dfs).reset_index(drop=True)\n",
    "    return df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "def extract_candidate_tfidf_sim(df, col):\n",
    "    dfs = []\n",
    "    for country, country_df in tqdm(df.groupby(\"country\")):\n",
    "        country_df = country_df[country_df[col]!=\"nan\"].copy()\n",
    "        if len(country_df) < 2:\n",
    "            continue\n",
    "\n",
    "        country_df = country_df.reset_index(drop=True)\n",
    "        \n",
    "        model = TfidfVectorizer(ngram_range=(3,3), analyzer=\"char_wb\", use_idf=True)\n",
    "        text_embeddings = model.fit_transform(country_df[col].tolist())\n",
    "\n",
    "        model = NearestNeighbors_gpu(n_neighbors=min(len(country_df), N_NEIGHBORS), algorithm=\"brute\")\n",
    "        model.fit(text_embeddings)\n",
    "        nears = model.kneighbors(text_embeddings, return_distance=False)\n",
    "        \n",
    "        k = min(len(country_df), N_NEIGHBORS)\n",
    "        country_df['match_id'] = country_df['id'].values[nears[:, :k]].tolist()\n",
    "        country_df = country_df.explode(['match_id'])\n",
    "        country_df = country_df.loc[country_df['id'] != country_df['match_id']].copy()\n",
    "        dfs.append(country_df)\n",
    "    df = pd.concat(dfs).reset_index(drop=True)\n",
    "    return df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "def extract_candidate_bert_sim(df, col):\n",
    "    dfs = []\n",
    "    vecs = make_bert_vec(df,col)\n",
    "    for country, country_df in tqdm(df.groupby(\"country\")):\n",
    "        country_df = country_df[country_df[col]!=\"nan\"].copy()\n",
    "        if len(country_df) < 2:\n",
    "            continue\n",
    "\n",
    "        country_df = country_df.reset_index(drop=True)\n",
    "        text_embeddings = np.vstack([vecs[str_] for str_ in country_df[col].values])\n",
    "        \n",
    "        model = NearestNeighbors_gpu(n_neighbors=min(len(country_df), N_NEIGHBORS), algorithm=\"brute\")\n",
    "        model.fit(text_embeddings)\n",
    "        nears = model.kneighbors(text_embeddings, return_distance=False)\n",
    "        \n",
    "        k = min(len(country_df), N_NEIGHBORS)\n",
    "        country_df['match_id'] = country_df['id'].values[nears[:, :k]].tolist()\n",
    "        country_df = country_df.explode(['match_id'])\n",
    "        country_df = country_df.loc[country_df['id'] != country_df['match_id']].copy()\n",
    "        dfs.append(country_df)\n",
    "    df = pd.concat(dfs).reset_index(drop=True)\n",
    "    return df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "def add_orgin_data(df, org_df):\n",
    "    df = df.merge(org_df.add_prefix('match_'), on='match_id')\n",
    "    df = df.reset_index(drop=True)\n",
    "    return df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "# https://www.kaggle.com/code/columbia2131/foursquare-iou-metrics\n",
    "def get_id2poi(input_df: pd.DataFrame) -> dict:\n",
    "    return dict(zip(input_df['id'], input_df['point_of_interest']))\n",
    "\n",
    "def get_poi2ids(input_df: pd.DataFrame) -> dict:\n",
    "    return input_df.groupby('point_of_interest')['id'].apply(set).to_dict()\n",
    "\n",
    "def get_score(input_df: pd.DataFrame, org_data):\n",
    "    scores = []\n",
    "    id2poi = get_id2poi(org_data)\n",
    "    poi2ids = get_poi2ids(org_data)\n",
    "    for id_str, matches in zip(input_df['id'].to_numpy(), input_df['matches'].to_numpy()):\n",
    "        targets = poi2ids[id2poi[id_str]]\n",
    "        preds = set(matches.split())\n",
    "        score = len((targets & preds)) / len((targets | preds))\n",
    "        scores.append(score)\n",
    "    scores = np.array(scores)\n",
    "    return scores.mean()\n",
    "\n",
    "def calc_max_score(tr_data, org_data):\n",
    "    train_candidate = pd.DataFrame()\n",
    "    train_candidate['id'] = org_data['id'].unique()\n",
    "    train_candidate['matches'] = org_data['id'].unique()\n",
    "    idx = tr_data['point_of_interest']==tr_data['match_point_of_interest']\n",
    "    train_match = tr_data.loc[idx].groupby('id')['match_id'].apply(list).map(\" \".join).reset_index()\n",
    "    train_match.columns = ['id','candidates']\n",
    "    train_candidate = train_candidate.merge(train_match, on = 'id', how = 'left')\n",
    "    idx = ~train_candidate['candidates'].isna()\n",
    "    train_candidate.loc[idx, \"matches\"] += \" \" + train_candidate.loc[idx, \"candidates\"]\n",
    "    score = get_score(train_candidate, org_data)\n",
    "    print('1st_stage_max_score : ' + '{:.5f}'.format(score))\n",
    "    return score"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_origin = pd.read_csv(INPUT_DIR + \"train.csv\")\n",
    "train_origin = preprocess(train_origin)\n",
    "\n",
    "# trainデータの分割\n",
    "kf = GroupKFold(n_splits=2)\n",
    "for i, (trn_idx, val_idx) in enumerate(kf.split(train_origin, train_origin['point_of_interest'], train_origin['point_of_interest'])):\n",
    "    train_origin = train_origin.loc[val_idx].copy()\n",
    "    break\n",
    "train_origin = train_origin.reset_index(drop=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [],
   "source": [
    "conds = []\n",
    "scores = []"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 210/210 [00:36<00:00,  5.80it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "1st_stage_max_score : 0.88423\n"
     ]
    }
   ],
   "source": [
    "cond = \"nameのみ\"\n",
    "train = extract_candidate_tfidf_sim(train_origin, \"name\")\n",
    "train = train.merge(train_origin[[\"id\", \"point_of_interest\"]].add_prefix(\"match_\"), on=\"match_id\", how=\"left\")\n",
    "score = calc_max_score(train, train_origin)\n",
    "conds.append(cond)\n",
    "scores.append(score)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 210/210 [00:43<00:00,  4.86it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "1st_stage_max_score : 0.90294\n"
     ]
    }
   ],
   "source": [
    "cond = \"name + address\"\n",
    "train_origin[\"text\"] = train_origin[\"name\"].replace(\"nan\", \"\") + \" \" + \\\n",
    "                       train_origin[\"address\"].replace(\"nan\", \"\")\n",
    "train = extract_candidate_tfidf_sim(train_origin, \"text\")\n",
    "train = train.merge(train_origin[[\"id\", \"point_of_interest\"]].add_prefix(\"match_\"), on=\"match_id\", how=\"left\")\n",
    "score = calc_max_score(train, train_origin)\n",
    "conds.append(cond)\n",
    "scores.append(score)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 210/210 [00:38<00:00,  5.44it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "1st_stage_max_score : 0.90803\n"
     ]
    }
   ],
   "source": [
    "cond = \"name + city\"\n",
    "train_origin[\"text\"] = train_origin[\"name\"].replace(\"nan\", \"\") + \" \" + \\\n",
    "                       train_origin[\"city\"].replace(\"nan\", \"\")\n",
    "train = extract_candidate_tfidf_sim(train_origin, \"text\")\n",
    "train = train.merge(train_origin[[\"id\", \"point_of_interest\"]].add_prefix(\"match_\"), on=\"match_id\", how=\"left\")\n",
    "score = calc_max_score(train, train_origin)\n",
    "conds.append(cond)\n",
    "scores.append(score)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 210/210 [00:37<00:00,  5.66it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "1st_stage_max_score : 0.89184\n"
     ]
    }
   ],
   "source": [
    "cond = \"name + state\"\n",
    "train_origin[\"text\"] = train_origin[\"name\"].replace(\"nan\", \"\") + \" \" + \\\n",
    "                       train_origin[\"state\"].replace(\"nan\", \"\")\n",
    "train = extract_candidate_tfidf_sim(train_origin, \"text\")\n",
    "train = train.merge(train_origin[[\"id\", \"point_of_interest\"]].add_prefix(\"match_\"), on=\"match_id\", how=\"left\")\n",
    "score = calc_max_score(train, train_origin)\n",
    "conds.append(cond)\n",
    "scores.append(score)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 210/210 [00:42<00:00,  4.91it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "1st_stage_max_score : 0.88436\n"
     ]
    }
   ],
   "source": [
    "cond = \"name + url\"\n",
    "train_origin[\"text\"] = train_origin[\"name\"].replace(\"nan\", \"\") + \" \" + \\\n",
    "                       train_origin[\"url\"].replace(\"nan\", \"\")\n",
    "train = extract_candidate_tfidf_sim(train_origin, \"text\")\n",
    "train = train.merge(train_origin[[\"id\", \"point_of_interest\"]].add_prefix(\"match_\"), on=\"match_id\", how=\"left\")\n",
    "score = calc_max_score(train, train_origin)\n",
    "conds.append(cond)\n",
    "scores.append(score)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 210/210 [00:46<00:00,  4.51it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "1st_stage_max_score : 0.86467\n"
     ]
    }
   ],
   "source": [
    "cond = \"name + categories\"\n",
    "train_origin[\"text\"] = train_origin[\"name\"].replace(\"nan\", \"\") + \" \" + \\\n",
    "                       train_origin[\"categories\"].replace(\"nan\", \"\")\n",
    "train = extract_candidate_tfidf_sim(train_origin, \"text\")\n",
    "train = train.merge(train_origin[[\"id\", \"point_of_interest\"]].add_prefix(\"match_\"), on=\"match_id\", how=\"left\")\n",
    "score = calc_max_score(train, train_origin)\n",
    "conds.append(cond)\n",
    "scores.append(score)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 210/210 [00:49<00:00,  4.26it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "1st_stage_max_score : 0.91179\n"
     ]
    }
   ],
   "source": [
    "cond = \"name + address + city\"\n",
    "train_origin[\"text\"] = train_origin[\"name\"].replace(\"nan\", \"\") + \" \" + \\\n",
    "                       train_origin[\"address\"].replace(\"nan\", \"\") + \" \" + \\\n",
    "                       train_origin[\"city\"].replace(\"nan\", \"\")\n",
    "train = extract_candidate_tfidf_sim(train_origin, \"text\")\n",
    "train = train.merge(train_origin[[\"id\", \"point_of_interest\"]].add_prefix(\"match_\"), on=\"match_id\", how=\"left\")\n",
    "score = calc_max_score(train, train_origin)\n",
    "conds.append(cond)\n",
    "scores.append(score)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 210/210 [00:52<00:00,  4.02it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "1st_stage_max_score : 0.90675\n"
     ]
    }
   ],
   "source": [
    "cond = \"name + address + city + state\"\n",
    "train_origin[\"text\"] = train_origin[\"name\"].replace(\"nan\", \"\") + \" \" + \\\n",
    "                       train_origin[\"address\"].replace(\"nan\", \"\") + \" \" + \\\n",
    "                       train_origin[\"city\"].replace(\"nan\", \"\") + \" \" + \\\n",
    "                       train_origin[\"state\"].replace(\"nan\", \"\") \n",
    "train = extract_candidate_tfidf_sim(train_origin, \"text\")\n",
    "train = train.merge(train_origin[[\"id\", \"point_of_interest\"]].add_prefix(\"match_\"), on=\"match_id\", how=\"left\")\n",
    "score = calc_max_score(train, train_origin)\n",
    "conds.append(cond)\n",
    "scores.append(score)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>条件</th>\n",
       "      <th>1st_max_score</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>nameのみ</td>\n",
       "      <td>0.884232</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>name + address</td>\n",
       "      <td>0.902943</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>name + city</td>\n",
       "      <td>0.908033</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>name + state</td>\n",
       "      <td>0.891844</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>name + url</td>\n",
       "      <td>0.884360</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>5</th>\n",
       "      <td>name + categories</td>\n",
       "      <td>0.864667</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>6</th>\n",
       "      <td>name + address + city</td>\n",
       "      <td>0.911794</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>7</th>\n",
       "      <td>name + address + city + state</td>\n",
       "      <td>0.906749</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                              条件  1st_max_score\n",
       "0                         nameのみ       0.884232\n",
       "1                 name + address       0.902943\n",
       "2                    name + city       0.908033\n",
       "3                   name + state       0.891844\n",
       "4                     name + url       0.884360\n",
       "5              name + categories       0.864667\n",
       "6          name + address + city       0.911794\n",
       "7  name + address + city + state       0.906749"
      ]
     },
     "execution_count": 29,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "result = pd.DataFrame()\n",
    "result[\"条件\"] = conds\n",
    "result[\"1st_max_score\"] = scores\n",
    "result"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "tfidf条件"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [],
   "source": [
    "def extract_candidate_tfidf_sim2(df, col, ngram_range, analyzer, use_idf):\n",
    "    dfs = []\n",
    "    for country, country_df in tqdm(df.groupby(\"country\")):\n",
    "        country_df = country_df[country_df[col]!=\"nan\"].copy()\n",
    "        if len(country_df) < 2:\n",
    "            continue\n",
    "\n",
    "        country_df = country_df.reset_index(drop=True)\n",
    "        \n",
    "        model = TfidfVectorizer(ngram_range=ngram_range, analyzer=analyzer, use_idf=use_idf)\n",
    "        text_embeddings = model.fit_transform(country_df[col].tolist())\n",
    "\n",
    "        model = NearestNeighbors_gpu(n_neighbors=min(len(country_df), N_NEIGHBORS), algorithm=\"brute\")\n",
    "        model.fit(text_embeddings)\n",
    "        nears = model.kneighbors(text_embeddings, return_distance=False)\n",
    "        \n",
    "        k = min(len(country_df), N_NEIGHBORS)\n",
    "        country_df['match_id'] = country_df['id'].values[nears[:, :k]].tolist()\n",
    "        country_df = country_df.explode(['match_id'])\n",
    "        country_df = country_df.loc[country_df['id'] != country_df['match_id']].copy()\n",
    "        dfs.append(country_df)\n",
    "    df = pd.concat(dfs).reset_index(drop=True)\n",
    "    return df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_origin[\"text\"] = train_origin[\"name\"].replace(\"nan\", \"\") + \" \" + \\\n",
    "                       train_origin[\"address\"].replace(\"nan\", \"\") + \" \" + \\\n",
    "                       train_origin[\"city\"].replace(\"nan\", \"\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "metadata": {},
   "outputs": [],
   "source": [
    "ngrams = [(1,1), (1,2), (1,3), (2,2), (2,3), (3,3)]\n",
    "analyzers = [\"word\", \"char\", \"char_wb\"]\n",
    "use_idfs = [True, False]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "metadata": {},
   "outputs": [],
   "source": [
    "import datetime"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 210/210 [00:28<00:00,  7.49it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "1st_stage_max_score : 0.87565\n",
      "(1, 1) word True 0.8756546269287908 32\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 210/210 [00:26<00:00,  7.84it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "1st_stage_max_score : 0.86014\n",
      "(1, 1) word False 0.8601403807249306 31\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 210/210 [00:31<00:00,  6.67it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "1st_stage_max_score : 0.81241\n",
      "(1, 1) char True 0.8124075356143731 35\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 210/210 [00:31<00:00,  6.67it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "1st_stage_max_score : 0.80656\n",
      "(1, 1) char False 0.806555327536221 36\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 210/210 [00:37<00:00,  5.64it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "1st_stage_max_score : 0.80972\n",
      "(1, 1) char_wb True 0.8097237420495128 41\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 210/210 [00:36<00:00,  5.81it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "1st_stage_max_score : 0.80159\n",
      "(1, 1) char_wb False 0.8015947364919042 40\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 210/210 [00:34<00:00,  6.04it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "1st_stage_max_score : 0.84407\n",
      "(1, 2) word True 0.8440746486946918 39\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 210/210 [00:34<00:00,  6.15it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "1st_stage_max_score : 0.84055\n",
      "(1, 2) word False 0.8405519556668388 38\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 210/210 [00:53<00:00,  3.96it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "1st_stage_max_score : 0.88938\n",
      "(1, 2) char True 0.8893813452852076 57\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 210/210 [00:52<00:00,  4.01it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "1st_stage_max_score : 0.85952\n",
      "(1, 2) char False 0.8595237243217336 56\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 210/210 [00:59<00:00,  3.52it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "1st_stage_max_score : 0.88777\n",
      "(1, 2) char_wb True 0.8877744349540051 64\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 210/210 [00:58<00:00,  3.57it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "1st_stage_max_score : 0.85429\n",
      "(1, 2) char_wb False 0.85429435487618 63\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 210/210 [00:42<00:00,  4.97it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "1st_stage_max_score : 0.83067\n",
      "(1, 3) word True 0.830667381701086 46\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 210/210 [00:41<00:00,  5.09it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "1st_stage_max_score : 0.82717\n",
      "(1, 3) word False 0.8271708325805838 45\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 210/210 [01:30<00:00,  2.32it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "1st_stage_max_score : 0.90554\n",
      "(1, 3) char True 0.9055365439915224 95\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 210/210 [01:29<00:00,  2.35it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "1st_stage_max_score : 0.87515\n",
      "(1, 3) char False 0.875148037949855 93\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 210/210 [01:36<00:00,  2.18it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "1st_stage_max_score : 0.90653\n",
      "(1, 3) char_wb True 0.9065344007427815 100\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 210/210 [01:34<00:00,  2.22it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "1st_stage_max_score : 0.86942\n",
      "(1, 3) char_wb False 0.8694207742507833 99\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 210/210 [00:30<00:00,  6.98it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "1st_stage_max_score : 0.74698\n",
      "(2, 2) word True 0.7469842611517388 34\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 210/210 [00:29<00:00,  7.14it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "1st_stage_max_score : 0.75106\n",
      "(2, 2) word False 0.7510552019406153 33\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 210/210 [00:42<00:00,  5.00it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "1st_stage_max_score : 0.89770\n",
      "(2, 2) char True 0.8977027705323352 46\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 210/210 [00:41<00:00,  5.08it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "1st_stage_max_score : 0.88224\n",
      "(2, 2) char False 0.8822405478818399 45\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 210/210 [00:44<00:00,  4.67it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "1st_stage_max_score : 0.89997\n",
      "(2, 2) char_wb True 0.8999721369042379 49\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 210/210 [00:44<00:00,  4.70it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "1st_stage_max_score : 0.88538\n",
      "(2, 2) char_wb False 0.8853826318848681 49\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 210/210 [00:37<00:00,  5.63it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "1st_stage_max_score : 0.73044\n",
      "(2, 3) word True 0.7304437101309482 41\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 210/210 [00:37<00:00,  5.66it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "1st_stage_max_score : 0.73801\n",
      "(2, 3) word False 0.73800967949315 41\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 210/210 [01:15<00:00,  2.78it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "1st_stage_max_score : 0.90768\n",
      "(2, 3) char True 0.9076845081205892 79\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 210/210 [01:14<00:00,  2.83it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "1st_stage_max_score : 0.88958\n",
      "(2, 3) char False 0.889578661825101 78\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 210/210 [01:16<00:00,  2.74it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "1st_stage_max_score : 0.91032\n",
      "(2, 3) char_wb True 0.9103175165368081 81\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 210/210 [01:15<00:00,  2.78it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "1st_stage_max_score : 0.89274\n",
      "(2, 3) char_wb False 0.8927355327239233 80\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 80%|███████▉  | 167/210 [00:15<00:03, 11.12it/s]\n"
     ]
    },
    {
     "ename": "ValueError",
     "evalue": "empty vocabulary; perhaps the documents only contain stop words",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mValueError\u001b[0m                                Traceback (most recent call last)",
      "\u001b[0;32m/tmp/ipykernel_11849/2667733167.py\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[1;32m      2\u001b[0m \u001b[0;32mfor\u001b[0m \u001b[0mngram\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0manalyzer\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0muse_idf\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mitertools\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mproduct\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mngrams\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0manalyzers\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0muse_idfs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      3\u001b[0m     \u001b[0mst_time\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mdatetime\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mdatetime\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mnow\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m----> 4\u001b[0;31m     \u001b[0mtrain\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mextract_candidate_tfidf_sim2\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mtrain_origin\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m\"text\"\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mngram\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0manalyzer\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0muse_idf\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m      5\u001b[0m     \u001b[0med_time\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mdatetime\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mdatetime\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mnow\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      6\u001b[0m     \u001b[0mt\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;34m(\u001b[0m\u001b[0med_time\u001b[0m \u001b[0;34m-\u001b[0m \u001b[0mst_time\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mseconds\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/tmp/ipykernel_11849/3838276476.py\u001b[0m in \u001b[0;36mextract_candidate_tfidf_sim2\u001b[0;34m(df, col, ngram_range, analyzer, use_idf)\u001b[0m\n\u001b[1;32m      9\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     10\u001b[0m         \u001b[0mmodel\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mTfidfVectorizer\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mngram_range\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mngram_range\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0manalyzer\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0manalyzer\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0muse_idf\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0muse_idf\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 11\u001b[0;31m         \u001b[0mtext_embeddings\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mmodel\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mfit_transform\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mcountry_df\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mcol\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mtolist\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     12\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     13\u001b[0m         \u001b[0mmodel\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mNearestNeighbors_gpu\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mn_neighbors\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mmin\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mlen\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mcountry_df\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mN_NEIGHBORS\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0malgorithm\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;34m\"brute\"\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/opt/conda/lib/python3.8/site-packages/sklearn/feature_extraction/text.py\u001b[0m in \u001b[0;36mfit_transform\u001b[0;34m(self, raw_documents, y)\u001b[0m\n\u001b[1;32m   1848\u001b[0m         \"\"\"\n\u001b[1;32m   1849\u001b[0m         \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_check_params\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 1850\u001b[0;31m         \u001b[0mX\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0msuper\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mfit_transform\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mraw_documents\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   1851\u001b[0m         \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_tfidf\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mfit\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mX\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1852\u001b[0m         \u001b[0;31m# X is already a transformed view of raw_documents so\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/opt/conda/lib/python3.8/site-packages/sklearn/feature_extraction/text.py\u001b[0m in \u001b[0;36mfit_transform\u001b[0;34m(self, raw_documents, y)\u001b[0m\n\u001b[1;32m   1201\u001b[0m         \u001b[0mmax_features\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mmax_features\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1202\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 1203\u001b[0;31m         vocabulary, X = self._count_vocab(raw_documents,\n\u001b[0m\u001b[1;32m   1204\u001b[0m                                           self.fixed_vocabulary_)\n\u001b[1;32m   1205\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/opt/conda/lib/python3.8/site-packages/sklearn/feature_extraction/text.py\u001b[0m in \u001b[0;36m_count_vocab\u001b[0;34m(self, raw_documents, fixed_vocab)\u001b[0m\n\u001b[1;32m   1132\u001b[0m             \u001b[0mvocabulary\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mdict\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mvocabulary\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1133\u001b[0m             \u001b[0;32mif\u001b[0m \u001b[0;32mnot\u001b[0m \u001b[0mvocabulary\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 1134\u001b[0;31m                 raise ValueError(\"empty vocabulary; perhaps the documents only\"\n\u001b[0m\u001b[1;32m   1135\u001b[0m                                  \" contain stop words\")\n\u001b[1;32m   1136\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mValueError\u001b[0m: empty vocabulary; perhaps the documents only contain stop words"
     ]
    }
   ],
   "source": [
    "results = []\n",
    "for ngram, analyzer, use_idf in itertools.product(ngrams, analyzers, use_idfs):\n",
    "    st_time = datetime.datetime.now()\n",
    "    train = extract_candidate_tfidf_sim2(train_origin, \"text\", ngram, analyzer, use_idf)\n",
    "    ed_time = datetime.datetime.now()\n",
    "    t = (ed_time - st_time).seconds\n",
    "    train = train.merge(train_origin[[\"id\", \"point_of_interest\"]].add_prefix(\"match_\"), on=\"match_id\", how=\"left\")\n",
    "    score = calc_max_score(train, train_origin)\n",
    "    print(ngram, analyzer, use_idf, score, t)\n",
    "    results.append([ngram, analyzer, use_idf, score, t])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>ngram_range</th>\n",
       "      <th>analyzer</th>\n",
       "      <th>use_idf</th>\n",
       "      <th>1st_max_score</th>\n",
       "      <th>time</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>(1, 1)</td>\n",
       "      <td>word</td>\n",
       "      <td>True</td>\n",
       "      <td>0.875655</td>\n",
       "      <td>32</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>(1, 1)</td>\n",
       "      <td>word</td>\n",
       "      <td>False</td>\n",
       "      <td>0.860140</td>\n",
       "      <td>31</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>(1, 1)</td>\n",
       "      <td>char</td>\n",
       "      <td>True</td>\n",
       "      <td>0.812408</td>\n",
       "      <td>35</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>(1, 1)</td>\n",
       "      <td>char</td>\n",
       "      <td>False</td>\n",
       "      <td>0.806555</td>\n",
       "      <td>36</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>(1, 1)</td>\n",
       "      <td>char_wb</td>\n",
       "      <td>True</td>\n",
       "      <td>0.809724</td>\n",
       "      <td>41</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>5</th>\n",
       "      <td>(1, 1)</td>\n",
       "      <td>char_wb</td>\n",
       "      <td>False</td>\n",
       "      <td>0.801595</td>\n",
       "      <td>40</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>6</th>\n",
       "      <td>(1, 2)</td>\n",
       "      <td>word</td>\n",
       "      <td>True</td>\n",
       "      <td>0.844075</td>\n",
       "      <td>39</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>7</th>\n",
       "      <td>(1, 2)</td>\n",
       "      <td>word</td>\n",
       "      <td>False</td>\n",
       "      <td>0.840552</td>\n",
       "      <td>38</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>8</th>\n",
       "      <td>(1, 2)</td>\n",
       "      <td>char</td>\n",
       "      <td>True</td>\n",
       "      <td>0.889381</td>\n",
       "      <td>57</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>9</th>\n",
       "      <td>(1, 2)</td>\n",
       "      <td>char</td>\n",
       "      <td>False</td>\n",
       "      <td>0.859524</td>\n",
       "      <td>56</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>10</th>\n",
       "      <td>(1, 2)</td>\n",
       "      <td>char_wb</td>\n",
       "      <td>True</td>\n",
       "      <td>0.887774</td>\n",
       "      <td>64</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>11</th>\n",
       "      <td>(1, 2)</td>\n",
       "      <td>char_wb</td>\n",
       "      <td>False</td>\n",
       "      <td>0.854294</td>\n",
       "      <td>63</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>12</th>\n",
       "      <td>(1, 3)</td>\n",
       "      <td>word</td>\n",
       "      <td>True</td>\n",
       "      <td>0.830667</td>\n",
       "      <td>46</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>13</th>\n",
       "      <td>(1, 3)</td>\n",
       "      <td>word</td>\n",
       "      <td>False</td>\n",
       "      <td>0.827171</td>\n",
       "      <td>45</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>14</th>\n",
       "      <td>(1, 3)</td>\n",
       "      <td>char</td>\n",
       "      <td>True</td>\n",
       "      <td>0.905537</td>\n",
       "      <td>95</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>15</th>\n",
       "      <td>(1, 3)</td>\n",
       "      <td>char</td>\n",
       "      <td>False</td>\n",
       "      <td>0.875148</td>\n",
       "      <td>93</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>16</th>\n",
       "      <td>(1, 3)</td>\n",
       "      <td>char_wb</td>\n",
       "      <td>True</td>\n",
       "      <td>0.906534</td>\n",
       "      <td>100</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>17</th>\n",
       "      <td>(1, 3)</td>\n",
       "      <td>char_wb</td>\n",
       "      <td>False</td>\n",
       "      <td>0.869421</td>\n",
       "      <td>99</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>18</th>\n",
       "      <td>(2, 2)</td>\n",
       "      <td>word</td>\n",
       "      <td>True</td>\n",
       "      <td>0.746984</td>\n",
       "      <td>34</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>19</th>\n",
       "      <td>(2, 2)</td>\n",
       "      <td>word</td>\n",
       "      <td>False</td>\n",
       "      <td>0.751055</td>\n",
       "      <td>33</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>20</th>\n",
       "      <td>(2, 2)</td>\n",
       "      <td>char</td>\n",
       "      <td>True</td>\n",
       "      <td>0.897703</td>\n",
       "      <td>46</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>21</th>\n",
       "      <td>(2, 2)</td>\n",
       "      <td>char</td>\n",
       "      <td>False</td>\n",
       "      <td>0.882241</td>\n",
       "      <td>45</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>22</th>\n",
       "      <td>(2, 2)</td>\n",
       "      <td>char_wb</td>\n",
       "      <td>True</td>\n",
       "      <td>0.899972</td>\n",
       "      <td>49</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>23</th>\n",
       "      <td>(2, 2)</td>\n",
       "      <td>char_wb</td>\n",
       "      <td>False</td>\n",
       "      <td>0.885383</td>\n",
       "      <td>49</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>24</th>\n",
       "      <td>(2, 3)</td>\n",
       "      <td>word</td>\n",
       "      <td>True</td>\n",
       "      <td>0.730444</td>\n",
       "      <td>41</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>25</th>\n",
       "      <td>(2, 3)</td>\n",
       "      <td>word</td>\n",
       "      <td>False</td>\n",
       "      <td>0.738010</td>\n",
       "      <td>41</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>26</th>\n",
       "      <td>(2, 3)</td>\n",
       "      <td>char</td>\n",
       "      <td>True</td>\n",
       "      <td>0.907685</td>\n",
       "      <td>79</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>27</th>\n",
       "      <td>(2, 3)</td>\n",
       "      <td>char</td>\n",
       "      <td>False</td>\n",
       "      <td>0.889579</td>\n",
       "      <td>78</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>28</th>\n",
       "      <td>(2, 3)</td>\n",
       "      <td>char_wb</td>\n",
       "      <td>True</td>\n",
       "      <td>0.910318</td>\n",
       "      <td>81</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>29</th>\n",
       "      <td>(2, 3)</td>\n",
       "      <td>char_wb</td>\n",
       "      <td>False</td>\n",
       "      <td>0.892736</td>\n",
       "      <td>80</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "   ngram_range analyzer  use_idf  1st_max_score  time\n",
       "0       (1, 1)     word     True       0.875655    32\n",
       "1       (1, 1)     word    False       0.860140    31\n",
       "2       (1, 1)     char     True       0.812408    35\n",
       "3       (1, 1)     char    False       0.806555    36\n",
       "4       (1, 1)  char_wb     True       0.809724    41\n",
       "5       (1, 1)  char_wb    False       0.801595    40\n",
       "6       (1, 2)     word     True       0.844075    39\n",
       "7       (1, 2)     word    False       0.840552    38\n",
       "8       (1, 2)     char     True       0.889381    57\n",
       "9       (1, 2)     char    False       0.859524    56\n",
       "10      (1, 2)  char_wb     True       0.887774    64\n",
       "11      (1, 2)  char_wb    False       0.854294    63\n",
       "12      (1, 3)     word     True       0.830667    46\n",
       "13      (1, 3)     word    False       0.827171    45\n",
       "14      (1, 3)     char     True       0.905537    95\n",
       "15      (1, 3)     char    False       0.875148    93\n",
       "16      (1, 3)  char_wb     True       0.906534   100\n",
       "17      (1, 3)  char_wb    False       0.869421    99\n",
       "18      (2, 2)     word     True       0.746984    34\n",
       "19      (2, 2)     word    False       0.751055    33\n",
       "20      (2, 2)     char     True       0.897703    46\n",
       "21      (2, 2)     char    False       0.882241    45\n",
       "22      (2, 2)  char_wb     True       0.899972    49\n",
       "23      (2, 2)  char_wb    False       0.885383    49\n",
       "24      (2, 3)     word     True       0.730444    41\n",
       "25      (2, 3)     word    False       0.738010    41\n",
       "26      (2, 3)     char     True       0.907685    79\n",
       "27      (2, 3)     char    False       0.889579    78\n",
       "28      (2, 3)  char_wb     True       0.910318    81\n",
       "29      (2, 3)  char_wb    False       0.892736    80"
      ]
     },
     "execution_count": 40,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "result = pd.DataFrame(results, columns=[\"ngram_range\", \"analyzer\", \"use_idf\", \"1st_max_score\", \"time\"])\n",
    "result"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(3, 3) word True 0.8927355327239233 80\n"
     ]
    }
   ],
   "source": [
    "print(ngram, analyzer, use_idf, score, t)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "interpreter": {
   "hash": "d4d1e4263499bec80672ea0156c357c1ee493ec2b1c70f0acce89fc37c4a6abe"
  },
  "kernelspec": {
   "display_name": "Python 3.8.12 ('base')",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.12"
  },
  "orig_nbformat": 4
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
