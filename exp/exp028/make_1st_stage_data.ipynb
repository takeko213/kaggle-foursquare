{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# exp027"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "MODE = 'local_train'\n",
    "#MODE = 'kaggle_inference'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "exp_name = 'exp028'\n",
    "memo = ''"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import sys\n",
    "import gc\n",
    "\n",
    "if MODE == 'local_train':\n",
    "    sys.path.append('/home/kaggler/.local/lib/python3.8/site-packages')\n",
    "    from dotenv import load_dotenv\n",
    "    load_dotenv\n",
    "    sys.path.append(os.getenv('UTILS_PATH'))\n",
    "    import line_notify\n",
    "    import slack_notify\n",
    "    \n",
    "if MODE == \"kaggle_inference\":\n",
    "    from cuml import ForestInference\n",
    "    import treelite\n",
    "    \n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt; plt.style.use(\"ggplot\")\n",
    "import seaborn as sns\n",
    "from sklearn.metrics.pairwise import haversine_distances\n",
    "from sklearn.model_selection import GroupKFold, KFold, StratifiedKFold\n",
    "from sklearn.metrics import accuracy_score\n",
    "from sklearn.decomposition import TruncatedSVD\n",
    "import lightgbm as lgb\n",
    "import itertools\n",
    "from scipy.spatial.distance import canberra\n",
    "from sklearn.neighbors import KNeighborsRegressor\n",
    "import functools\n",
    "import multiprocessing\n",
    "import Levenshtein\n",
    "import difflib\n",
    "import pickle\n",
    "from tqdm import tqdm\n",
    "%load_ext Cython\n",
    "\n",
    "from transformers import DistilBertModel, DistilBertTokenizer\n",
    "from torch.utils.data import DataLoader, Dataset\n",
    "import torch.nn.functional as F\n",
    "import torch.nn as nn\n",
    "import torch\n",
    "\n",
    "from sklearn.metrics.pairwise import cosine_similarity\n",
    "from sklearn.feature_extraction.text import TfidfVectorizer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "# directry_setting\n",
    "if MODE == 'local_train':\n",
    "    INPUT_DIR = os.getenv('INPUT_DIR')\n",
    "    OUTPUT_DIR = os.getenv('OUTPUT_DIR')\n",
    "    MODEL_DIR = os.getenv('OUTPUT_DIR')\n",
    "    BERT_MODEL = \"distilbert-base-uncased\"\n",
    "    os.makedirs(OUTPUT_DIR + exp_name, exist_ok=True)\n",
    "\n",
    "elif MODE == 'kaggle_inference':\n",
    "    INPUT_DIR = '/kaggle/input/foursquare-location-matching/'\n",
    "    OUTPUT_DIR = './'\n",
    "    MODEL_DIR = f'../input/fs{exp_name}/'\n",
    "    BERT_MODEL = \"../input/distilbertbaseuncased\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "# CONFIG\n",
    "SEED = 42\n",
    "N_NEIGHBORS = 10\n",
    "N_SPLITS = 5\n",
    "PROB_TH = 0.5\n",
    "MAX_LEN = 32\n",
    "BS = 512\n",
    "NW = 2\n",
    "SVD_N_COMP = 50"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "class Cat2VecModel(nn.Module):\n",
    "    def __init__(self):\n",
    "        super(Cat2VecModel, self).__init__()\n",
    "        self.distill_bert = DistilBertModel.from_pretrained(BERT_MODEL)\n",
    "        \n",
    "    def forward(self, ids, mask):\n",
    "        x = self.distill_bert(ids, mask)[0]\n",
    "        x = F.normalize((x[:, 1:, :]*mask[:, 1:, None]).mean(axis=1))\n",
    "        return x\n",
    "\n",
    "class InferenceDataset(Dataset):\n",
    "    \n",
    "    def __init__(self, df, max_len, col):\n",
    "        super().__init__()\n",
    "        self.df = df.reset_index(drop=True)\n",
    "        self.max_len = max_len\n",
    "        self.tokenizer = DistilBertTokenizer.from_pretrained(BERT_MODEL, do_lower_case=True)\n",
    "        self.col = col\n",
    "\n",
    "    def __getitem__(self, index):\n",
    "        row = self.df.iloc[index]\n",
    "        \n",
    "        inputs = self.tokenizer.encode_plus(\n",
    "            row[self.col],\n",
    "            None,\n",
    "            add_special_tokens=True,\n",
    "            max_length=MAX_LEN,\n",
    "            padding=\"max_length\",\n",
    "            return_token_type_ids=True,\n",
    "            truncation=True\n",
    "        )\n",
    "        ids = torch.LongTensor(inputs['input_ids'])\n",
    "        mask = torch.LongTensor(inputs['attention_mask'])\n",
    "\n",
    "        return ids, mask\n",
    "\n",
    "    def __len__(self):\n",
    "        return self.df.shape[0]\n",
    "\n",
    "def inference(ds):\n",
    "    cat2vec_model = Cat2VecModel()\n",
    "    cat2vec_model = cat2vec_model.cuda()\n",
    "    \n",
    "    loader = DataLoader(ds, batch_size=BS, shuffle=False, num_workers=NW,\n",
    "                        pin_memory=False, drop_last=False)\n",
    "    \n",
    "    vs = []\n",
    "    with torch.no_grad():\n",
    "        for idx, (ids, masks) in enumerate(loader):\n",
    "            v = cat2vec_model(ids.cuda(), masks.cuda()).detach().cpu().numpy()\n",
    "            vs.append(v)\n",
    "    return np.concatenate(vs)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "def make_bert_vec(df, col):\n",
    "    cat_df = df[[col]].drop_duplicates()\n",
    "    cat_df[col] = cat_df[col].fillna(\"null\")\n",
    "\n",
    "    cat_ds = InferenceDataset(cat_df, max_len=MAX_LEN, col=col)\n",
    "    V = inference(cat_ds)\n",
    "    svd = TruncatedSVD(n_components=SVD_N_COMP, random_state=SEED)\n",
    "    V = svd.fit_transform(V)\n",
    "    V = V.astype(\"float16\")\n",
    "    bert_vec = {k:v for k,v in zip(cat_df[col].values, V)}\n",
    "    return bert_vec"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "def preprocess(df):\n",
    "    columns = ['id', 'name', 'address', 'city', 'state',\n",
    "        'zip', 'country', 'url', 'phone', 'categories']\n",
    "    for c in columns:\n",
    "        if c != \"id\":\n",
    "            df[c] = df[c].astype(str).str.lower()\n",
    "\n",
    "    df[[\"latitude\", \"longitude\"]] = np.deg2rad(df[[\"latitude\", \"longitude\"]])\n",
    "    \n",
    "    return df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "def extract_candidate(df):\n",
    "    dfs = []\n",
    "    for country, country_df in tqdm(df.groupby(\"country\")):\n",
    "        country_df = country_df.reset_index(drop=True)\n",
    "        \n",
    "        knn = KNeighborsRegressor(n_neighbors=min(len(country_df), N_NEIGHBORS), \n",
    "                                    metric='haversine', n_jobs=-1)\n",
    "        knn.fit(country_df[['latitude','longitude']], country_df.index)\n",
    "        dists, nears = knn.kneighbors(country_df[['latitude','longitude']], return_distance=True)\n",
    "        \n",
    "        k = min(len(country_df), N_NEIGHBORS)\n",
    "        country_df['match_id'] = country_df['id'].values[nears[:, :k]].tolist()\n",
    "        country_df['d_near'] = dists[:, :k].tolist()\n",
    "        country_df = country_df.explode(['match_id','d_near'])\n",
    "        country_df = country_df.loc[country_df['id'] != country_df['match_id']].copy()\n",
    "        dfs.append(country_df)\n",
    "    df = pd.concat(dfs).reset_index(drop=True)\n",
    "    return df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "def add_orgin_data(df, org_df):\n",
    "    df = df.merge(org_df.add_prefix('match_'), on='match_id')\n",
    "    df = df.reset_index(drop=True)\n",
    "    return df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "# https://www.kaggle.com/code/columbia2131/foursquare-iou-metrics\n",
    "def get_id2poi(input_df: pd.DataFrame) -> dict:\n",
    "    return dict(zip(input_df['id'], input_df['point_of_interest']))\n",
    "\n",
    "def get_poi2ids(input_df: pd.DataFrame) -> dict:\n",
    "    return input_df.groupby('point_of_interest')['id'].apply(set).to_dict()\n",
    "\n",
    "def get_score(input_df: pd.DataFrame, org_data):\n",
    "    scores = []\n",
    "    id2poi = get_id2poi(org_data)\n",
    "    poi2ids = get_poi2ids(org_data)\n",
    "    for id_str, matches in zip(input_df['id'].to_numpy(), input_df['matches'].to_numpy()):\n",
    "        targets = poi2ids[id2poi[id_str]]\n",
    "        preds = set(matches.split())\n",
    "        score = len((targets & preds)) / len((targets | preds))\n",
    "        scores.append(score)\n",
    "    scores = np.array(scores)\n",
    "    return scores.mean()\n",
    "\n",
    "def calc_max_score(tr_data, org_data):\n",
    "    train_candidate = pd.DataFrame()\n",
    "    train_candidate['id'] = org_data['id'].unique()\n",
    "    train_candidate['matches'] = org_data['id'].unique()\n",
    "    idx = tr_data['point_of_interest']==tr_data['match_point_of_interest']\n",
    "    train_match = tr_data.loc[idx].groupby('id')['match_id'].apply(list).map(\" \".join).reset_index()\n",
    "    train_match.columns = ['id','candidates']\n",
    "    train_candidate = train_candidate.merge(train_match, on = 'id', how = 'left')\n",
    "    idx = ~train_candidate['candidates'].isna()\n",
    "    train_candidate.loc[idx, \"matches\"] += \" \" + train_candidate.loc[idx, \"candidates\"]\n",
    "    score = get_score(train_candidate, org_data)\n",
    "    print('1st_stage_max_score : ' + '{:.5f}'.format(score))\n",
    "    return score"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "def categorical_similarity(A, B):\n",
    "    if not A or not B:\n",
    "        return -1\n",
    "\n",
    "    A = set(str(A).split(\", \"))\n",
    "    B = set(str(B).split(\", \"))\n",
    "\n",
    "    # Find intersection of two sets\n",
    "    nominator = A.intersection(B)\n",
    "\n",
    "    similarity_1 = len(nominator) / len(A)\n",
    "    similarity_2 = len(nominator) / len(B)\n",
    "\n",
    "    return max(similarity_1, similarity_2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "%%cython\n",
    "def LCS(str S, str T):\n",
    "    cdef int i, j\n",
    "    cdef list dp = [[0] * (len(T) + 1) for _ in range(len(S) + 1)]\n",
    "    for i in range(len(S)):\n",
    "        for j in range(len(T)):\n",
    "            dp[i + 1][j + 1] = max(dp[i][j] + (S[i] == T[j]), dp[i + 1][j], dp[i][j + 1], dp[i + 1][j + 1])\n",
    "    return dp[len(S)][len(T)]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "def _add_distance_features(args):\n",
    "    _, df = args\n",
    "\n",
    "    columns = ['name', 'address', 'city', 'state',\n",
    "               'zip', 'url', 'phone', 'categories']\n",
    "\n",
    "    for c in columns:\n",
    "        geshs = []\n",
    "        levens = []\n",
    "        jaros = []\n",
    "        lcss = []\n",
    "\n",
    "        for str1, str2 in df[[f\"{c}\", f\"match_{c}\"]].values.astype(str):\n",
    "            if str1 != \"nan\" and str2 != \"nan\":\n",
    "                geshs.append(difflib.SequenceMatcher(None, str1, str2).ratio())\n",
    "                levens.append(Levenshtein.distance(str1, str2))\n",
    "                jaros.append(Levenshtein.jaro_winkler(str1, str2))\n",
    "                lcss.append(LCS(str(str1), str(str2)))\n",
    "            else:\n",
    "                geshs.append(-1)\n",
    "                levens.append(-1)\n",
    "                jaros.append(-1)\n",
    "                lcss.append(-1)\n",
    "        df[f\"match_{c}_gesh\"] = geshs\n",
    "        df[f\"match_{c}_gesh\"] = df[f\"match_{c}_gesh\"].astype(np.float16)\n",
    "        df[f\"match_{c}_leven\"] = levens\n",
    "        df[f\"match_{c}_leven\"] = df[f\"match_{c}_leven\"].astype(np.float16)\n",
    "        df[f\"match_{c}_jaro\"] = jaros\n",
    "        df[f\"match_{c}_jaro\"] = df[f\"match_{c}_jaro\"].astype(np.float16)\n",
    "        df[f\"match_{c}_lcs\"] = lcss\n",
    "        df[f\"match_{c}_lcs\"] = df[f\"match_{c}_lcs\"].astype(np.float16)\n",
    "            \n",
    "        if not c in ['country', \"phone\", \"zip\"]:\n",
    "            df[f\"match_{c}_len\"] = df[f\"match_{c}\"].astype(str).map(len)\n",
    "            df[f\"{c}_len\"] = df[f\"{c}\"].astype(str).map(len)\n",
    "            df[f\"match_{c}_nleven\"] = df[f\"match_{c}_leven\"] / df[[f\"match_{c}_len\", f\"{c}_len\"]].max(axis=1)\n",
    "            df[f\"match_{c}_nleven\"] = df[f\"match_{c}_nleven\"].astype(np.float16)\n",
    "            df[f\"match_{c}_nlcsi\"] = df[f\"match_{c}_lcs\"] / df[f\"match_{c}_len\"]\n",
    "            df[f\"match_{c}_nlcs0\"] = df[f\"match_{c}_lcs\"] / df[f\"{c}_len\"]\n",
    "            df[f\"match_{c}_nlcsi\"] = df[f\"match_{c}_nlcsi\"].astype(np.float16)\n",
    "            df[f\"match_{c}_nlcs0\"] = df[f\"match_{c}_nlcs0\"].astype(np.float16)\n",
    "            df.drop(f'{c}_len',axis=1, inplace = True)\n",
    "            df.drop(f\"match_{c}_len\",axis=1, inplace = True)\n",
    "\n",
    "    return df\n",
    "\n",
    "\n",
    "def add_distance_features(df):\n",
    "    processes = multiprocessing.cpu_count()\n",
    "    with multiprocessing.Pool(processes=processes) as pool:\n",
    "        dfs = pool.imap_unordered(_add_distance_features, df.groupby('country'))\n",
    "        dfs = tqdm(dfs)\n",
    "        dfs = list(dfs)\n",
    "    df = pd.concat(dfs)\n",
    "    return df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [],
   "source": [
    "def add_vec_sim_features(df, vec, vec_name, col):\n",
    "    sim_list = []\n",
    "    for str1, str2 in tqdm(df[[f\"{col}\", f\"match_{col}\"]].values.astype(str)):\n",
    "        if str1!=\"nan\" and str2!=\"nan\":\n",
    "            sim = cosine_similarity(vec[str1].reshape(1, -1), vec[str2].reshape(1, -1))[0][0]\n",
    "        else:\n",
    "            sim = -1\n",
    "        sim_list.append(sim)\n",
    "    df[f\"{vec_name}_sim_{col}\"] = sim_list\n",
    "    df[f\"{vec_name}_sim_{col}\"] = df[f\"{vec_name}_sim_{col}\"].astype(np.float16)\n",
    "    return df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [],
   "source": [
    "def reduce_data_size(df, features):\n",
    "    if MODE == 'local_train':\n",
    "        df = df[features + ['target', 'id', 'match_id']].copy()\n",
    "    elif MODE == 'kaggle_inference':\n",
    "        df = df[features + ['id', 'match_id']].copy()\n",
    "\n",
    "\n",
    "    df[features] = df[features].astype(np.float16)\n",
    "    for _ in range(5):\n",
    "        gc.collect()\n",
    "        \n",
    "    return df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [],
   "source": [
    "def train_model(df, features):\n",
    "    params = {'objective': 'binary', \n",
    "              'boosting': 'gbdt',\n",
    "              'learning_rate': 0.1, \n",
    "              'metric': 'binary_logloss', \n",
    "              'seed': SEED, \n",
    "              'feature_pre_filter': False, \n",
    "              'lambda_l1': 0.5745709668124809, \n",
    "              'lambda_l2': 0.5123383865042099, \n",
    "              'num_leaves': 239, \n",
    "              'feature_fraction': 0.784, \n",
    "              'bagging_fraction': 1.0, \n",
    "              'bagging_freq': 0, \n",
    "              'min_child_samples': 5\n",
    "              }\n",
    "\n",
    "    # split folds\n",
    "    kf = StratifiedKFold(n_splits=N_SPLITS, shuffle=True, random_state=SEED)\n",
    "    for i, (trn_idx, val_idx) in enumerate(kf.split(df, df[\"target\"], df[\"target\"])):\n",
    "        df.loc[val_idx, \"fold\"] = i\n",
    "    \n",
    "    fi = pd.DataFrame()\n",
    "    oof = df[['id', 'match_id', 'target']].copy()\n",
    "    oof['prob'] = 0.0\n",
    "    scores = []\n",
    "\n",
    "    for i in range(N_SPLITS):\n",
    "        print('fold : ' + str(i))\n",
    "        tr_idx = df[df['fold'] != i].index\n",
    "        vl_idx = df[df['fold'] == i].index\n",
    "        tr_x, tr_y = df.loc[tr_idx, features], df.loc[tr_idx, 'target']\n",
    "        vl_x, vl_y = df.loc[vl_idx, features], df.loc[vl_idx, 'target']\n",
    "        tr_data = lgb.Dataset(tr_x, label=tr_y)\n",
    "        vl_data = lgb.Dataset(vl_x, label=vl_y)\n",
    "\n",
    "        model = lgb.train(params, tr_data, valid_sets=[tr_data, vl_data],\n",
    "                        num_boost_round=20000, early_stopping_rounds=100, verbose_eval=1000)\n",
    "\n",
    "        # 特徴量重要度\n",
    "        fi_tmp = pd.DataFrame()\n",
    "        fi_tmp['feature'] = model.feature_name()\n",
    "        fi_tmp['importance'] = model.feature_importance(importance_type='gain')\n",
    "        fi_tmp['iter'] = i\n",
    "        fi = fi.append(fi_tmp)\n",
    "\n",
    "        pred = model.predict(vl_x, num_iteration=model.best_iteration)\n",
    "        oof.loc[vl_idx, 'prob'] = pred\n",
    "\n",
    "        score = accuracy_score((pred > PROB_TH).astype(int), vl_y)\n",
    "        scores.append(score)\n",
    "        print(f'fold{i} | accuracy = ' + '{:.5f}'.format(score))\n",
    "\n",
    "        with open(OUTPUT_DIR + f'{exp_name}/model{i}.pickle', 'wb') as f:\n",
    "            pickle.dump(model, f)\n",
    "        \n",
    "    oof.to_csv(OUTPUT_DIR + f'{exp_name}/{exp_name}_oof.csv', index=False)\n",
    "\n",
    "    print('accuracy(mean) : ' + '{:.5f}'.format(np.mean(scores)))\n",
    "    print(scores)\n",
    "\n",
    "    fi_n = fi['feature'].nunique()\n",
    "    order = list(fi.groupby(\"feature\").mean().sort_values(\"importance\", ascending=False).index)\n",
    "    plt.figure(figsize=(10, fi_n*0.2))\n",
    "    sns.barplot(x=\"importance\", y=\"feature\", data=fi, order=order)\n",
    "    plt.title(f\"LGBM importance\")\n",
    "    plt.tight_layout()\n",
    "    plt.savefig(OUTPUT_DIR + f'{exp_name}/lgbm_importance.png')\n",
    "\n",
    "    return oof, np.mean(scores)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [],
   "source": [
    "def model_inference(df, features):\n",
    "    pred = np.zeros(len(df))\n",
    "    for i in range(N_SPLITS):\n",
    "        with open(MODEL_DIR + f'model{i}.pickle', 'rb') as f:\n",
    "            model = pickle.load(f)\n",
    "        model.save_model(\"test.lgb\")\n",
    "        fi = ForestInference()\n",
    "        fi = ForestInference.load(\"test.lgb\", output_class=True, model_type=\"lightgbm\")\n",
    "        pred += fi.predict(df[features]) / N_SPLITS\n",
    "    return pred"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [],
   "source": [
    "def transform_data(df, org_data):\n",
    "    train_candidate = pd.DataFrame()\n",
    "    train_candidate['id'] = org_data['id'].unique()\n",
    "    train_candidate['matches'] = org_data['id'].unique()\n",
    "\n",
    "    train_match = df[df['prob'] >= PROB_TH].copy()\n",
    "    train_match = train_match.groupby('id')['match_id'].apply(list).map(\" \".join).reset_index()\n",
    "    train_match.columns = ['id','candidates']\n",
    "    train_candidate = train_candidate.merge(train_match, on = 'id', how = 'left')\n",
    "    idx = ~train_candidate['candidates'].isna()\n",
    "    train_candidate.loc[idx, \"matches\"] += \" \" + train_candidate.loc[idx, \"candidates\"]\n",
    "    return train_candidate[['id', 'matches']]\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [],
   "source": [
    "def postprocess(df):\n",
    "    id2match = dict(zip(df[\"id\"].values, df[\"matches\"].str.split()))\n",
    "\n",
    "    for match in tqdm(df[\"matches\"]):\n",
    "        match = match.split()\n",
    "        if len(match) == 1:        \n",
    "            continue\n",
    "\n",
    "        base = match[0]\n",
    "        for m in match[1:]:\n",
    "            if not base in id2match[m]:\n",
    "                id2match[m].append(base)\n",
    "    df[\"matches\"] = df[\"id\"].map(id2match).map(\" \".join)\n",
    "    return df "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_origin = pd.read_csv(INPUT_DIR + \"train.csv\")\n",
    "train_origin = preprocess(train_origin)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 210/210 [00:38<00:00,  5.47it/s]\n",
      "100%|██████████| 211/211 [00:43<00:00,  4.86it/s]\n"
     ]
    }
   ],
   "source": [
    "\n",
    "train_origin = pd.read_csv(INPUT_DIR + \"train.csv\")\n",
    "train_origin = preprocess(train_origin)\n",
    "\n",
    "# trainデータの分割\n",
    "kf = GroupKFold(n_splits=2)\n",
    "for i, (trn_idx, val_idx) in enumerate(kf.split(train_origin, train_origin['point_of_interest'], train_origin['point_of_interest'])):\n",
    "    train_origin.loc[val_idx, \"set\"] = i\n",
    "\n",
    "# 1st stage\n",
    "train = pd.concat([\n",
    "    extract_candidate(train_origin[train_origin[\"set\"]==0]), \n",
    "    extract_candidate(train_origin[train_origin[\"set\"]==1]), \n",
    "])\n",
    "train = add_orgin_data(train, train_origin)\n",
    "train.to_csv(OUTPUT_DIR + f'{exp_name}/first_stage_data.csv', index=False)"
   ]
  }
 ],
 "metadata": {
  "interpreter": {
   "hash": "d4d1e4263499bec80672ea0156c357c1ee493ec2b1c70f0acce89fc37c4a6abe"
  },
  "kernelspec": {
   "display_name": "Python 3.8.12 ('base')",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.12"
  },
  "orig_nbformat": 4
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
