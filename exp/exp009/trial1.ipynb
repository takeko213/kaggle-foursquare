{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# exp009_trial1\n",
    "https://www.kaggle.com/code/aerdem4/foursquare-cat2vec"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "MODE = 'local_train'\n",
    "#MODE = 'kaggle_inference'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "exp_name = 'exp009'\n",
    "#memo = 'exp001のhypopt'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "The Cython extension is already loaded. To reload it, use:\n",
      "  %reload_ext Cython\n"
     ]
    }
   ],
   "source": [
    "import os\n",
    "import sys\n",
    "import gc\n",
    "\n",
    "if MODE == 'local_train':\n",
    "    sys.path.append('/home/kaggler/.local/lib/python3.8/site-packages')\n",
    "    from dotenv import load_dotenv\n",
    "    load_dotenv\n",
    "    sys.path.append(os.getenv('UTILS_PATH'))\n",
    "    import line_notify\n",
    "    import slack_notify\n",
    "    \n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt; plt.style.use(\"ggplot\")\n",
    "import seaborn as sns\n",
    "from sklearn.metrics.pairwise import haversine_distances\n",
    "from sklearn.model_selection import GroupKFold, KFold, StratifiedKFold\n",
    "from sklearn.metrics import accuracy_score\n",
    "import lightgbm as lgb\n",
    "import itertools\n",
    "from scipy.spatial.distance import canberra\n",
    "from sklearn.neighbors import KNeighborsRegressor\n",
    "import multiprocessing\n",
    "import Levenshtein\n",
    "import difflib\n",
    "import pickle\n",
    "%load_ext Cython\n",
    "\n",
    "from transformers import DistilBertModel, DistilBertTokenizer\n",
    "import torch.nn.functional as F\n",
    "import torch.nn as nn\n",
    "import torch\n",
    "\n",
    "from sklearn.metrics.pairwise import cosine_similarity"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "# directry_setting\n",
    "if MODE == 'local_train':\n",
    "    INPUT_DIR = os.getenv('INPUT_DIR')\n",
    "    OUTPUT_DIR = os.getenv('OUTPUT_DIR')\n",
    "    MODEL_DIR = os.getenv('OUTPUT_DIR')\n",
    "    #os.makedirs(OUTPUT_DIR + exp_name, exist_ok=True)\n",
    "\n",
    "elif MODE == 'kaggle_inference':\n",
    "    INPUT_DIR = '/kaggle/input/foursquare-location-matching/'\n",
    "    OUTPUT_DIR = './'\n",
    "    MODEL_DIR = f'../input/fs{exp_name}/'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {},
   "outputs": [],
   "source": [
    "# CONFIG\n",
    "SEED = 42\n",
    "N_NEIGHBORS = 10\n",
    "N_SPLITS = 5\n",
    "PROB_TH = 0.5\n",
    "MAX_LEN = 32\n",
    "BS = 256\n",
    "NW = 2    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "def preprocess(df):\n",
    "    columns = ['id', 'name', 'address', 'city', 'state',\n",
    "        'zip', 'country', 'url', 'phone', 'categories']\n",
    "    for c in columns:\n",
    "        if c != \"id\":\n",
    "            df[c] = df[c].astype(str).str.lower()\n",
    "    return df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "def extract_candidate(df):\n",
    "    dfs = []\n",
    "    candidates = pd.DataFrame()\n",
    "    for country, country_df in tqdm(df.groupby(\"country\")):\n",
    "        country_df = country_df.reset_index(drop=True)\n",
    "        \n",
    "        knn = KNeighborsRegressor(n_neighbors=min(len(country_df), N_NEIGHBORS), \n",
    "                                  metric='haversine', n_jobs=-1)\n",
    "        knn.fit(country_df[['latitude','longitude']], country_df.index)\n",
    "        dists, nears = knn.kneighbors(country_df[['latitude','longitude']], return_distance=True)\n",
    "        \n",
    "        k = min(len(country_df), N_NEIGHBORS)\n",
    "        country_df['match_id'] = country_df['id'].values[nears[:, :k]].tolist()\n",
    "        country_df['d_near'] = dists[:, :k].tolist()\n",
    "        dfs.append(country_df[['id','match_id','d_near']])\n",
    "    df = pd.concat(dfs).reset_index(drop=True)\n",
    "    return df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "def add_orgin_data(df, org_df):\n",
    "    df = df.explode(['match_id','d_near'])\n",
    "    df = df.loc[df['id'] != df['match_id']].copy()\n",
    "    df = df.merge(org_df, on='id')\n",
    "    df = df.merge(org_df.add_prefix('match_'), on='match_id')\n",
    "    df = df.reset_index(drop=True)\n",
    "    return df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Some weights of the model checkpoint at distilbert-base-uncased were not used when initializing DistilBertModel: ['vocab_layer_norm.weight', 'vocab_transform.bias', 'vocab_projector.weight', 'vocab_projector.bias', 'vocab_layer_norm.bias', 'vocab_transform.weight']\n",
      "- This IS expected if you are initializing DistilBertModel from the checkpoint of a model trained on another task or with another architecture (e.g. initializing a BertForSequenceClassification model from a BertForPreTraining model).\n",
      "- This IS NOT expected if you are initializing DistilBertModel from the checkpoint of a model that you expect to be exactly identical (initializing a BertForSequenceClassification model from a BertForSequenceClassification model).\n"
     ]
    }
   ],
   "source": [
    "from torch.utils.data import DataLoader, Dataset\n",
    "\n",
    "class Cat2VecModel(nn.Module):\n",
    "    def __init__(self):\n",
    "        super(Cat2VecModel, self).__init__()\n",
    "        self.distill_bert = DistilBertModel.from_pretrained(\"distilbert-base-uncased\")\n",
    "        \n",
    "    def forward(self, ids, mask):\n",
    "        x = self.distill_bert(ids, mask)[0]\n",
    "        x = F.normalize((x[:, 1:, :]*mask[:, 1:, None]).mean(axis=1))\n",
    "        return x\n",
    "\n",
    "class InferenceDataset(Dataset):\n",
    "    \n",
    "    def __init__(self, df, max_len):\n",
    "        super().__init__()\n",
    "        self.df = df.reset_index(drop=True)\n",
    "        self.max_len = max_len\n",
    "        self.tokenizer = DistilBertTokenizer.from_pretrained(\"distilbert-base-uncased\", do_lower_case=True)\n",
    "\n",
    "    def __getitem__(self, index):\n",
    "        row = self.df.iloc[index]\n",
    "        \n",
    "        inputs = self.tokenizer.encode_plus(\n",
    "            row.categories,\n",
    "            None,\n",
    "            add_special_tokens=True,\n",
    "            max_length=MAX_LEN,\n",
    "            padding=\"max_length\",\n",
    "            return_token_type_ids=True,\n",
    "            truncation=True\n",
    "        )\n",
    "        ids = torch.LongTensor(inputs['input_ids'])\n",
    "        mask = torch.LongTensor(inputs['attention_mask'])\n",
    "\n",
    "        return ids, mask\n",
    "\n",
    "    def __len__(self):\n",
    "        return self.df.shape[0]\n",
    "\n",
    "def inference(ds):\n",
    "    loader = DataLoader(ds, batch_size=BS, shuffle=False, num_workers=NW,\n",
    "                        pin_memory=False, drop_last=False)\n",
    "    \n",
    "    vs = []\n",
    "    with torch.no_grad():\n",
    "        for idx, (ids, masks) in enumerate(loader):\n",
    "            v = cat2vec_model(ids.cuda(), masks.cuda()).detach().cpu().numpy()\n",
    "            vs.append(v)\n",
    "    return np.concatenate(vs)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>categories</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>Bars</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>Brazilian Restaurants</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>Salons / Barbershops</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>Mobile Phone Shops</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>Spanish Restaurants</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>...</th>\n",
       "      <td>...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1138716</th>\n",
       "      <td>Event Spaces, Automotive Shops</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1138725</th>\n",
       "      <td>College Auditoriums, Concert Halls, Theaters</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1138744</th>\n",
       "      <td>Advertising Agencies, Offices, Miscellaneous S...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1138771</th>\n",
       "      <td>Dive Bars, Outdoors &amp; Recreation</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1138779</th>\n",
       "      <td>Scenic Lookouts, Lakes, Other Great Outdoors</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>52579 rows × 1 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "                                                categories\n",
       "0                                                     Bars\n",
       "1                                    Brazilian Restaurants\n",
       "2                                     Salons / Barbershops\n",
       "3                                       Mobile Phone Shops\n",
       "4                                      Spanish Restaurants\n",
       "...                                                    ...\n",
       "1138716                     Event Spaces, Automotive Shops\n",
       "1138725       College Auditoriums, Concert Halls, Theaters\n",
       "1138744  Advertising Agencies, Offices, Miscellaneous S...\n",
       "1138771                   Dive Bars, Outdoors & Recreation\n",
       "1138779       Scenic Lookouts, Lakes, Other Great Outdoors\n",
       "\n",
       "[52579 rows x 1 columns]"
      ]
     },
     "execution_count": 26,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "cat2vec_model = Cat2VecModel()\n",
    "cat2vec_model = cat2vec_model.cuda()\n",
    "\n",
    "cat_df = pd.read_csv(INPUT_DIR + \"train.csv\")[[\"categories\"]].drop_duplicates()\n",
    "cat_df[\"categories\"] = cat_df[\"categories\"].fillna(\"null\")\n",
    "\n",
    "cat_ds = InferenceDataset(cat_df, max_len=MAX_LEN)\n",
    "print(len(cat_ds))\n",
    "cat_ds[0]\n",
    "\n",
    "V = inference(cat_ds)\n",
    "bertvec_categories = {k:v for k,v in zip(cat_df[\"categories\"].values, V)}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "metadata": {},
   "outputs": [],
   "source": [
    "cat2vec_model = Cat2VecModel()\n",
    "cat2vec_model = cat2vec_model.cuda()\n",
    "\n",
    "cat_df = pd.read_csv(INPUT_DIR + \"train.csv\")[[\"categories\"]].drop_duplicates()\n",
    "cat_df[\"categories\"] = cat_df[\"categories\"].fillna(\"null\")\n",
    "\n",
    "cat_ds = InferenceDataset(cat_df, max_len=MAX_LEN)\n",
    "print(len(cat_ds))\n",
    "cat_ds[0]\n",
    "\n",
    "V = inference(cat_ds)\n",
    "bertvec_categories = {k:v for k,v in zip(cat_df[\"categories\"].values, V)}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 46,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0.78520715"
      ]
     },
     "execution_count": 46,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "cosine_similarity(bertvec_categories['Brazilian Restaurants'].reshape(1, -1), bertvec_categories['Bars'].reshape(1, -1))[0][0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "# https://www.kaggle.com/code/columbia2131/foursquare-iou-metrics\n",
    "def get_id2poi(input_df: pd.DataFrame) -> dict:\n",
    "    return dict(zip(input_df['id'], input_df['point_of_interest']))\n",
    "\n",
    "def get_poi2ids(input_df: pd.DataFrame) -> dict:\n",
    "    return input_df.groupby('point_of_interest')['id'].apply(set).to_dict()\n",
    "\n",
    "def get_score(input_df: pd.DataFrame, org_data):\n",
    "    scores = []\n",
    "    id2poi = get_id2poi(org_data)\n",
    "    poi2ids = get_poi2ids(org_data)\n",
    "    for id_str, matches in zip(input_df['id'].to_numpy(), input_df['matches'].to_numpy()):\n",
    "        targets = poi2ids[id2poi[id_str]]\n",
    "        preds = set(matches.split())\n",
    "        score = len((targets & preds)) / len((targets | preds))\n",
    "        scores.append(score)\n",
    "    scores = np.array(scores)\n",
    "    return scores.mean()\n",
    "\n",
    "def calc_max_score(tr_data, org_data):\n",
    "    train_candidate = pd.DataFrame()\n",
    "    train_candidate['id'] = org_data['id'].unique()\n",
    "    train_candidate['matches'] = org_data['id'].unique()\n",
    "    idx = tr_data['point_of_interest']==tr_data['match_point_of_interest']\n",
    "    train_match = tr_data.loc[idx].groupby('id')['match_id'].apply(list).map(\" \".join).reset_index()\n",
    "    train_match.columns = ['id','candidates']\n",
    "    train_candidate = train_candidate.merge(train_match, on = 'id', how = 'left')\n",
    "    idx = ~train_candidate['candidates'].isna()\n",
    "    train_candidate.loc[idx, \"matches\"] += \" \" + train_candidate.loc[idx, \"candidates\"]\n",
    "    score = get_score(train_candidate, org_data)\n",
    "    print('1st_stage_max_score : ' + '{:.5f}'.format(score))\n",
    "    return score"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "%%cython\n",
    "def LCS(str S, str T):\n",
    "    cdef int i, j\n",
    "    cdef list dp = [[0] * (len(T) + 1) for _ in range(len(S) + 1)]\n",
    "    for i in range(len(S)):\n",
    "        for j in range(len(T)):\n",
    "            dp[i + 1][j + 1] = max(dp[i][j] + (S[i] == T[j]), dp[i + 1][j], dp[i][j + 1], dp[i + 1][j + 1])\n",
    "    return dp[len(S)][len(T)]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "def _add_distance_features(args):\n",
    "    _, df = args\n",
    "\n",
    "    columns = ['name', 'address', 'city', 'state',\n",
    "               'zip', 'country', 'url', 'phone', 'categories']\n",
    "\n",
    "    for c in columns:\n",
    "        geshs = []\n",
    "        levens = []\n",
    "        jaros = []\n",
    "        lcss = []\n",
    "        for str1, str2 in df[[f\"{c}\", f\"match_{c}\"]].values.astype(str):\n",
    "            if str1==str1 and str2==str2:\n",
    "                geshs.append(difflib.SequenceMatcher(None, str1, str2).ratio())\n",
    "                levens.append(Levenshtein.distance(str1, str2))\n",
    "                jaros.append(Levenshtein.jaro_winkler(str1, str2))\n",
    "                lcss.append(LCS(str(str1), str(str2)))\n",
    "            else:\n",
    "                geshs.append(-1)\n",
    "                levens.append(-1)\n",
    "                jaros.append(-1)\n",
    "        df[f\"match_{c}_gesh\"] = geshs\n",
    "        df[f\"match_{c}_leven\"] = levens\n",
    "        df[f\"match_{c}_jaro\"] = jaros\n",
    "        df[f\"match_{c}_lcs\"] = lcss\n",
    "            \n",
    "        if not c in ['country', \"phone\", \"zip\"]:\n",
    "            df[f\"match_{c}_len\"] = df[f\"match_{c}\"].astype(str).map(len)\n",
    "            df[f\"match_{c}_nleven\"] = df[f\"match_{c}_leven\"] / df[f\"match_{c}_len\"]\n",
    "            df[f\"match_{c}_nlcsi\"] = df[f\"match_{c}_lcs\"] / df[f\"match_{c}_len\"]\n",
    "    return df\n",
    "\n",
    "\n",
    "def add_distance_features(df):\n",
    "    processes = multiprocessing.cpu_count()\n",
    "    with multiprocessing.Pool(processes=processes) as pool:\n",
    "        dfs = pool.imap_unordered(_add_distance_features, df.groupby('country'))\n",
    "        dfs = tqdm(dfs)\n",
    "        dfs = list(dfs)\n",
    "    df = pd.concat(dfs)\n",
    "    return df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "def reduce_data_size(df, features):\n",
    "    if MODE == 'local_train':\n",
    "        df = df[features + ['target', 'id', 'match_id']].copy()\n",
    "    elif MODE == 'kaggle_inference':\n",
    "        df = df[features + ['id', 'match_id']].copy()\n",
    "\n",
    "\n",
    "    df[features] = df[features].astype(np.float16)\n",
    "    for _ in range(5):\n",
    "        gc.collect()\n",
    "        \n",
    "    return df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "def train_model(df, features):\n",
    "    params = {'objective': 'binary', \n",
    "              'boosting': 'gbdt',\n",
    "              'learning_rate': 0.1, \n",
    "              'metric': 'binary_logloss', \n",
    "              'seed': SEED, \n",
    "              'feature_pre_filter': False, \n",
    "              'lambda_l1': 0.5745709668124809, \n",
    "              'lambda_l2': 0.5123383865042099, \n",
    "              'num_leaves': 239, \n",
    "              'feature_fraction': 0.784, \n",
    "              'bagging_fraction': 1.0, \n",
    "              'bagging_freq': 0, \n",
    "              'min_child_samples': 5\n",
    "              }\n",
    "\n",
    "    # split folds\n",
    "    kf = StratifiedKFold(n_splits=N_SPLITS, shuffle=True, random_state=SEED)\n",
    "    for i, (trn_idx, val_idx) in enumerate(kf.split(df, df[\"target\"], df[\"target\"])):\n",
    "        df.loc[val_idx, \"fold\"] = i\n",
    "    \n",
    "    fi = pd.DataFrame()\n",
    "    oof = df[['id', 'match_id', 'target']].copy()\n",
    "    oof['prob'] = 0.0\n",
    "    scores = []\n",
    "\n",
    "    for i in range(N_SPLITS):\n",
    "        print('fold : ' + str(i))\n",
    "        tr_idx = df[df['fold'] != i].index\n",
    "        vl_idx = df[df['fold'] == i].index\n",
    "        tr_x, tr_y = df.loc[tr_idx, features], df.loc[tr_idx, 'target']\n",
    "        vl_x, vl_y = df.loc[vl_idx, features], df.loc[vl_idx, 'target']\n",
    "        tr_data = lgb.Dataset(tr_x, label=tr_y)\n",
    "        vl_data = lgb.Dataset(vl_x, label=vl_y)\n",
    "\n",
    "        model = lgb.train(params, tr_data, valid_sets=[tr_data, vl_data],\n",
    "                        num_boost_round=20000, early_stopping_rounds=100, verbose_eval=1000)\n",
    "\n",
    "        # 特徴量重要度\n",
    "        fi_tmp = pd.DataFrame()\n",
    "        fi_tmp['feature'] = model.feature_name()\n",
    "        fi_tmp['importance'] = model.feature_importance(importance_type='gain')\n",
    "        fi_tmp['iter'] = i\n",
    "        fi = fi.append(fi_tmp)\n",
    "\n",
    "        pred = model.predict(vl_x, num_iteration=model.best_iteration)\n",
    "        oof.loc[vl_idx, 'prob'] = pred\n",
    "\n",
    "        score = accuracy_score((pred > PROB_TH).astype(int), vl_y)\n",
    "        scores.append(score)\n",
    "        print(f'fold{i} | accuracy = ' + '{:.5f}'.format(score))\n",
    "\n",
    "        with open(OUTPUT_DIR + f'{exp_name}/model{i}.pickle', 'wb') as f:\n",
    "            pickle.dump(model, f)\n",
    "        \n",
    "    oof.to_csv(OUTPUT_DIR + f'{exp_name}/{exp_name}_oof.csv', index=False)\n",
    "\n",
    "    print('accuracy(mean) : ' + '{:.5f}'.format(np.mean(scores)))\n",
    "    print(scores)\n",
    "\n",
    "    fi_n = fi['feature'].nunique()\n",
    "    order = list(fi.groupby(\"feature\").mean().sort_values(\"importance\", ascending=False).index)\n",
    "    plt.figure(figsize=(10, fi_n*0.2))\n",
    "    sns.barplot(x=\"importance\", y=\"feature\", data=fi, order=order)\n",
    "    plt.title(f\"LGBM importance\")\n",
    "    plt.tight_layout()\n",
    "    plt.savefig(OUTPUT_DIR + f'{exp_name}/lgbm_importance.png')\n",
    "\n",
    "    return oof, np.mean(scores)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "def model_inference(df, features):\n",
    "    pred = np.zeros(len(df))\n",
    "    for i in range(N_SPLITS):\n",
    "        with open(MODEL_DIR + f'model{i}.pickle', 'rb') as f:\n",
    "            model = pickle.load(f)\n",
    "        pred += model.predict(df[features], num_iteration=model.best_iteration) / N_SPLITS\n",
    "    return pred"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [],
   "source": [
    "def transform_data(df, org_data):\n",
    "    train_candidate = pd.DataFrame()\n",
    "    train_candidate['id'] = org_data['id'].unique()\n",
    "    train_candidate['matches'] = org_data['id'].unique()\n",
    "\n",
    "    train_match = df[df['prob'] >= PROB_TH].copy()\n",
    "    train_match = train_match.groupby('id')['match_id'].apply(list).map(\" \".join).reset_index()\n",
    "    train_match.columns = ['id','candidates']\n",
    "    train_candidate = train_candidate.merge(train_match, on = 'id', how = 'left')\n",
    "    idx = ~train_candidate['candidates'].isna()\n",
    "    train_candidate.loc[idx, \"matches\"] += \" \" + train_candidate.loc[idx, \"candidates\"]\n",
    "    return train_candidate[['id', 'matches']]\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [],
   "source": [
    "def postprocess(df):\n",
    "    id2match = dict(zip(df[\"id\"].values, df[\"matches\"].str.split()))\n",
    "\n",
    "    for match in tqdm(df[\"matches\"]):\n",
    "        match = match.split()\n",
    "        if len(match) == 1:        \n",
    "            continue\n",
    "\n",
    "        base = match[0]\n",
    "        for m in match[1:]:\n",
    "            if not base in id2match[m]:\n",
    "                id2match[m].append(base)\n",
    "    df[\"matches\"] = df[\"id\"].map(id2match).map(\" \".join)\n",
    "    return df "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [],
   "source": [
    "def run_train():\n",
    "    train_origin = pd.read_csv(INPUT_DIR + \"train.csv\")\n",
    "    train_origin = preprocess(train_origin)\n",
    "\n",
    "    # trainデータの分割\n",
    "    kf = GroupKFold(n_splits=2)\n",
    "    for i, (trn_idx, val_idx) in enumerate(kf.split(train_origin, train_origin['point_of_interest'], train_origin['point_of_interest'])):\n",
    "        train_origin.loc[val_idx, \"set\"] = i\n",
    "\n",
    "    # 1st stage\n",
    "    train = pd.concat([\n",
    "        extract_candidate(train_origin[train_origin[\"set\"]==0]), \n",
    "        extract_candidate(train_origin[train_origin[\"set\"]==1]), \n",
    "    ])\n",
    "    train = add_orgin_data(train, train_origin)\n",
    "    stage1_max_score = calc_max_score(train, train_origin)\n",
    "\n",
    "    # 2nd stage\n",
    "    # create target\n",
    "    train['target'] = (train['point_of_interest'] == train['match_point_of_interest']).values.astype(int)\n",
    "    train[\"target\"] = train[\"target\"].fillna(0)\n",
    "\n",
    "    train = add_distance_features(train)\n",
    "\n",
    "    not_use_cols = ['match_state_leven', 'address', 'categories', 'point_of_interest', 'match_address_leven',\n",
    "                    'city', 'match_point_of_interest', 'match_name', 'match_categories_leven', 'match_longitude',\n",
    "                    'target', 'match_city_leven', 'zip', 'match_categories', 'match_city', 'match_latitude',\n",
    "                    'match_zip', 'match_url', 'id', 'match_set', 'country', 'match_state', 'match_address',\n",
    "                    'match_name_leven', 'match_id', 'latitude', 'url', 'set', 'name', 'phone', 'longitude',\n",
    "                    'match_url_leven', 'state', 'match_phone', 'match_country']\n",
    "    features = [c for c in train.columns if c not in not_use_cols]\n",
    "    with open(OUTPUT_DIR + f'{exp_name}/features.pickle', 'wb') as f:\n",
    "        pickle.dump(features, f)\n",
    "\n",
    "    train = reduce_data_size(train, features)\n",
    "\n",
    "    oof, stage2_mean_accuracy = train_model(train, features)\n",
    "    oof = transform_data(oof, train_origin)\n",
    "\n",
    "    cv_score = get_score(oof, train_origin)\n",
    "    print(f'cv_score = ' + '{:.5f}'.format(cv_score))\n",
    "\n",
    "    oof = postprocess(oof)\n",
    "    cv_score_after_pp = get_score(oof, train_origin)\n",
    "    print(f'cv_score(after_pp) = ' + '{:.5f}'.format(cv_score_after_pp))\n",
    "\n",
    "\n",
    "    report = f'{exp_name}\\n'\n",
    "    report += memo + '\\n'\n",
    "    report += 'stage1_max_score : ' + '{:.5f}'.format(stage1_max_score) + '\\n'\n",
    "    report += 'stage2_mean_accuracy : ' + '{:.5f}'.format(stage2_mean_accuracy) + '\\n'\n",
    "    report += 'cv_score : ' + '{:.5f}'.format(cv_score) + '\\n'\n",
    "    report += 'cv_score_after_pp : ' + '{:.5f}'.format(cv_score_after_pp) + '\\n'\n",
    "    print(report)\n",
    "    line_notify.send(report)\n",
    "    slack_notify.send(report)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [],
   "source": [
    "def run_inference():\n",
    "    test_origin = pd.read_csv(INPUT_DIR + \"test.csv\")\n",
    "    \n",
    "    if len(test_origin) == 5:\n",
    "        test_origin = pd.read_csv(INPUT_DIR + \"train.csv\")\n",
    "        kf = GroupKFold(n_splits=2)\n",
    "        for i, (trn_idx, val_idx) in enumerate(kf.split(test_origin, test_origin['point_of_interest'], test_origin['point_of_interest'])):\n",
    "            test_origin = test_origin.loc[trn_idx]\n",
    "            break\n",
    "\n",
    "    test_origin = preprocess(test_origin)\n",
    "\n",
    "    # 1st stage\n",
    "    test = extract_candidate(test_origin)\n",
    "\n",
    "    # 2nd stage\n",
    "    test = add_orgin_data(test, test_origin)\n",
    "    test = add_distance_features(test)\n",
    "    with open(MODEL_DIR + 'features.pickle', 'rb') as f:\n",
    "        features = pickle.load(f)\n",
    "    test = reduce_data_size(test, features)\n",
    "    \n",
    "    print('test_dataset_info')\n",
    "    print(test.info())\n",
    "    \n",
    "    test['prob'] = model_inference(test, features)\n",
    "    test = transform_data(test, test_origin)\n",
    "    test = postprocess(test)\n",
    "    test.to_csv('submission.csv', index=False)"
   ]
  }
 ],
 "metadata": {
  "interpreter": {
   "hash": "d4d1e4263499bec80672ea0156c357c1ee493ec2b1c70f0acce89fc37c4a6abe"
  },
  "kernelspec": {
   "display_name": "Python 3.8.12 ('base')",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.12"
  },
  "orig_nbformat": 4
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
