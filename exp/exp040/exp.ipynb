{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# exp040"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {},
   "outputs": [],
   "source": [
    "MODE = 'local_train'\n",
    "#MODE = 'kaggle_inference'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "metadata": {},
   "outputs": [],
   "source": [
    "exp_name = 'exp040'\n",
    "memo = 'exp038にpykakasi, unicode追加'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "The Cython extension is already loaded. To reload it, use:\n",
      "  %reload_ext Cython\n"
     ]
    }
   ],
   "source": [
    "import os\n",
    "import sys\n",
    "import gc\n",
    "\n",
    "if MODE == 'local_train':\n",
    "    sys.path.append('/home/kaggler/.local/lib/python3.8/site-packages')\n",
    "    from dotenv import load_dotenv\n",
    "    load_dotenv\n",
    "    sys.path.append(os.getenv('UTILS_PATH'))\n",
    "    import line_notify\n",
    "    import slack_notify\n",
    "    \n",
    "if MODE == \"kaggle_inference\":\n",
    "    from cuml import ForestInference\n",
    "    import treelite\n",
    "    \n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "from numpy import dot\n",
    "from numpy.linalg import norm\n",
    "import matplotlib.pyplot as plt; plt.style.use(\"ggplot\")\n",
    "import seaborn as sns\n",
    "from sklearn.metrics.pairwise import haversine_distances\n",
    "from sklearn.model_selection import GroupKFold, KFold, StratifiedKFold\n",
    "from sklearn.metrics import accuracy_score\n",
    "from sklearn.decomposition import TruncatedSVD\n",
    "import lightgbm as lgb\n",
    "import itertools\n",
    "from scipy.spatial.distance import canberra\n",
    "from cuml.neighbors import KNeighborsRegressor\n",
    "import functools\n",
    "import multiprocessing\n",
    "import Levenshtein\n",
    "import difflib\n",
    "import pickle\n",
    "from tqdm import tqdm\n",
    "%load_ext Cython\n",
    "\n",
    "from transformers import DistilBertModel, DistilBertTokenizer\n",
    "from torch.utils.data import DataLoader, Dataset\n",
    "import torch.nn.functional as F\n",
    "import torch.nn as nn\n",
    "import torch\n",
    "\n",
    "from sklearn.metrics.pairwise import cosine_similarity\n",
    "from sklearn.feature_extraction.text import TfidfVectorizer\n",
    "\n",
    "from cuml.feature_extraction.text import TfidfVectorizer as TfidfVectorizer_gpu\n",
    "import cudf, cuml, cupy\n",
    "from cuml.neighbors import NearestNeighbors as NearestNeighbors_gpu\n",
    "\n",
    "from unidecode import unidecode\n",
    "import pykakasi"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "metadata": {},
   "outputs": [],
   "source": [
    "# directry_setting\n",
    "if MODE == 'local_train':\n",
    "    INPUT_DIR = os.getenv('INPUT_DIR')\n",
    "    OUTPUT_DIR = os.getenv('OUTPUT_DIR')\n",
    "    MODEL_DIR = os.getenv('OUTPUT_DIR')\n",
    "    BERT_MODEL = \"distilbert-base-multilingual-cased\"\n",
    "    os.makedirs(OUTPUT_DIR + exp_name, exist_ok=True)\n",
    "\n",
    "elif MODE == 'kaggle_inference':\n",
    "    INPUT_DIR = '/kaggle/input/foursquare-location-matching/'\n",
    "    OUTPUT_DIR = './'\n",
    "    MODEL_DIR = f'../input/fs{exp_name}/'\n",
    "    BERT_MODEL = \"../input/distilbertbaseuncased\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "metadata": {},
   "outputs": [],
   "source": [
    "# CONFIG\n",
    "SEED = 42\n",
    "N_NEIGHBORS = 15\n",
    "N_SPLITS = 5\n",
    "PROB_TH = 0.5\n",
    "MAX_LEN = 32\n",
    "BS = 512\n",
    "NW = 2\n",
    "SVD_N_COMP = 50"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "metadata": {},
   "outputs": [],
   "source": [
    "class Cat2VecModel(nn.Module):\n",
    "    def __init__(self):\n",
    "        super(Cat2VecModel, self).__init__()\n",
    "        self.distill_bert = DistilBertModel.from_pretrained(BERT_MODEL)\n",
    "        \n",
    "    def forward(self, ids, mask):\n",
    "        x = self.distill_bert(ids, mask)[0]\n",
    "        x = F.normalize((x[:, 1:, :]*mask[:, 1:, None]).mean(axis=1))\n",
    "        return x\n",
    "\n",
    "class InferenceDataset(Dataset):\n",
    "    \n",
    "    def __init__(self, df, max_len, col):\n",
    "        super().__init__()\n",
    "        self.df = df.reset_index(drop=True)\n",
    "        self.max_len = max_len\n",
    "        self.tokenizer = DistilBertTokenizer.from_pretrained(BERT_MODEL, do_lower_case=True)\n",
    "        self.col = col\n",
    "\n",
    "    def __getitem__(self, index):\n",
    "        row = self.df.iloc[index]\n",
    "        \n",
    "        inputs = self.tokenizer.encode_plus(\n",
    "            row[self.col],\n",
    "            None,\n",
    "            add_special_tokens=True,\n",
    "            max_length=MAX_LEN,\n",
    "            padding=\"max_length\",\n",
    "            return_token_type_ids=True,\n",
    "            truncation=True\n",
    "        )\n",
    "        ids = torch.LongTensor(inputs['input_ids'])\n",
    "        mask = torch.LongTensor(inputs['attention_mask'])\n",
    "\n",
    "        return ids, mask\n",
    "\n",
    "    def __len__(self):\n",
    "        return self.df.shape[0]\n",
    "\n",
    "def inference(ds):\n",
    "    cat2vec_model = Cat2VecModel()\n",
    "    cat2vec_model = cat2vec_model.cuda()\n",
    "    \n",
    "    loader = DataLoader(ds, batch_size=BS, shuffle=False, num_workers=NW,\n",
    "                        pin_memory=False, drop_last=False)\n",
    "    \n",
    "    vs = []\n",
    "    with torch.no_grad():\n",
    "        for idx, (ids, masks) in enumerate(loader):\n",
    "            v = cat2vec_model(ids.cuda(), masks.cuda()).detach().cpu().numpy()\n",
    "            vs.append(v)\n",
    "    return np.concatenate(vs)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "metadata": {},
   "outputs": [],
   "source": [
    "def make_bert_vec(df, col):\n",
    "    cat_df = df[[col]].drop_duplicates()\n",
    "    cat_df[col] = cat_df[col].fillna(\"null\")\n",
    "\n",
    "    cat_ds = InferenceDataset(cat_df, max_len=MAX_LEN, col=col)\n",
    "    V = inference(cat_ds)\n",
    "    #svd = TruncatedSVD(n_components=SVD_N_COMP, random_state=SEED)\n",
    "    #V = svd.fit_transform(V)\n",
    "    V = V.astype(\"float16\")\n",
    "    bert_vec = {k:v for k,v in zip(cat_df[col].values, V)}\n",
    "    return bert_vec"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "metadata": {},
   "outputs": [],
   "source": [
    "def preprocess(df):\n",
    "    columns = ['id', 'name', 'address', 'city', 'state',\n",
    "        'zip', 'country', 'url', 'phone', 'categories']\n",
    "    for c in columns:\n",
    "        if c != \"id\":\n",
    "            df[c] = df[c].astype(str).str.lower()\n",
    "\n",
    "    df[[\"latitude\", \"longitude\"]] = np.deg2rad(df[[\"latitude\", \"longitude\"]])\n",
    "    \n",
    "    return df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "metadata": {},
   "outputs": [],
   "source": [
    "def convert_unicode(df, cols):\n",
    "    for c in cols:\n",
    "        df.loc[df[\"country\"]!=\"jp\", c+\"_org\"] = df.loc[df[\"country\"]!=\"jp\", c]\n",
    "        df.loc[df[\"country\"]!=\"jp\", c] = df.loc[df[\"country\"]!=\"jp\", c].apply(unidecode)\n",
    "    return df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "metadata": {},
   "outputs": [],
   "source": [
    "def convert_japanese_alphabet(df, cols):\n",
    "    kakasi = pykakasi.kakasi()\n",
    "    kakasi.setMode('H', 'a')  # Convert Hiragana into alphabet\n",
    "    kakasi.setMode('K', 'a')  # Convert Katakana into alphabet\n",
    "    kakasi.setMode('J', 'a')  # Convert Kanji into alphabet\n",
    "    conversion = kakasi.getConverter()\n",
    "\n",
    "    def convert_jp(row):\n",
    "        for column in cols:\n",
    "            try:\n",
    "                row[column] = conversion.do(row[column])\n",
    "            except:\n",
    "                pass\n",
    "        return row\n",
    "    \n",
    "\n",
    "    for c in cols:\n",
    "        df.loc[df[\"country\"]==\"jp\", c+\"_org\"] = df.loc[df[\"country\"]==\"jp\", c]\n",
    "\n",
    "    df[df[\"country\"] == \"jp\"] = df[df[\"country\"] == \"jp\"].apply(convert_jp, axis=1)\n",
    "    return df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 42,
   "metadata": {},
   "outputs": [],
   "source": [
    "def extract_candidate_dist(df):\n",
    "    dfs = []\n",
    "    for country, country_df in tqdm(df.groupby(\"country\")):\n",
    "        country_df = country_df.reset_index(drop=True)\n",
    "        \n",
    "        knn = KNeighborsRegressor(n_neighbors=min(len(country_df), N_NEIGHBORS), \n",
    "                                    metric='haversine', algorithm=\"brute\")\n",
    "        knn.fit(country_df[['latitude','longitude']], country_df.index.values)\n",
    "        nears = knn.kneighbors(country_df[['latitude','longitude']], return_distance=False)\n",
    "        \n",
    "        k = min(len(country_df), N_NEIGHBORS)\n",
    "        country_df['match_id'] = country_df['id'].values[nears[:, :k]].tolist()\n",
    "        country_df = country_df.explode(['match_id'])\n",
    "        country_df = country_df.loc[country_df['id'] != country_df['match_id']].copy()\n",
    "        dfs.append(country_df)\n",
    "    df = pd.concat(dfs).reset_index(drop=True)\n",
    "    return df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 43,
   "metadata": {},
   "outputs": [],
   "source": [
    "def extract_candidate_tfidf_sim(df, col):\n",
    "    dfs = []\n",
    "    for country, country_df in tqdm(df.groupby(\"country\")):\n",
    "        country_df = country_df[country_df[col]!=\"nan\"].copy()\n",
    "        if len(country_df) < 2:\n",
    "            continue\n",
    "\n",
    "        country_df = country_df.reset_index(drop=True)\n",
    "        \n",
    "        model = TfidfVectorizer(ngram_range=(3,3), analyzer=\"char_wb\", use_idf=True)\n",
    "        text_embeddings = model.fit_transform(country_df[\"name\"].tolist())\n",
    "\n",
    "        model = NearestNeighbors_gpu(n_neighbors=min(len(country_df), N_NEIGHBORS), algorithm=\"brute\")\n",
    "        model.fit(text_embeddings)\n",
    "        nears = model.kneighbors(text_embeddings, return_distance=False)\n",
    "        \n",
    "        k = min(len(country_df), N_NEIGHBORS)\n",
    "        country_df['match_id'] = country_df['id'].values[nears[:, :k]].tolist()\n",
    "        country_df = country_df.explode(['match_id'])\n",
    "        country_df = country_df.loc[country_df['id'] != country_df['match_id']].copy()\n",
    "        dfs.append(country_df)\n",
    "    df = pd.concat(dfs).reset_index(drop=True)\n",
    "    return df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 44,
   "metadata": {},
   "outputs": [],
   "source": [
    "def add_orgin_data(df, org_df):\n",
    "    df = df.merge(org_df.add_prefix('match_'), on='match_id')\n",
    "    df = df.reset_index(drop=True)\n",
    "    return df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 45,
   "metadata": {},
   "outputs": [],
   "source": [
    "def haversine_np(lon1, lat1, lon2, lat2):\n",
    "    \"\"\"\n",
    "    Calculate the great circle distance between two points\n",
    "    on the earth (specified in decimal degrees)\n",
    "\n",
    "    All args must be of equal length.    \n",
    "\n",
    "    \"\"\"\n",
    "    #lon1, lat1, lon2, lat2 = map(np.radians, [lon1, lat1, lon2, lat2])\n",
    "\n",
    "    dlon = lon2 - lon1\n",
    "    dlat = lat2 - lat1\n",
    "\n",
    "    a = np.sin(dlat/2.0)**2 + np.cos(lat1) * np.cos(lat2) * np.sin(dlon/2.0)**2\n",
    "\n",
    "    c = 2 * np.arcsin(np.sqrt(a))\n",
    "    return c"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 46,
   "metadata": {},
   "outputs": [],
   "source": [
    "# https://www.kaggle.com/code/columbia2131/foursquare-iou-metrics\n",
    "def get_id2poi(input_df: pd.DataFrame) -> dict:\n",
    "    return dict(zip(input_df['id'], input_df['point_of_interest']))\n",
    "\n",
    "def get_poi2ids(input_df: pd.DataFrame) -> dict:\n",
    "    return input_df.groupby('point_of_interest')['id'].apply(set).to_dict()\n",
    "\n",
    "def get_score(input_df: pd.DataFrame, org_data):\n",
    "    scores = []\n",
    "    id2poi = get_id2poi(org_data)\n",
    "    poi2ids = get_poi2ids(org_data)\n",
    "    for id_str, matches in zip(input_df['id'].to_numpy(), input_df['matches'].to_numpy()):\n",
    "        targets = poi2ids[id2poi[id_str]]\n",
    "        preds = set(matches.split())\n",
    "        score = len((targets & preds)) / len((targets | preds))\n",
    "        scores.append(score)\n",
    "    scores = np.array(scores)\n",
    "    return scores.mean()\n",
    "\n",
    "def calc_max_score(tr_data, org_data):\n",
    "    train_candidate = pd.DataFrame()\n",
    "    train_candidate['id'] = org_data['id'].unique()\n",
    "    train_candidate['matches'] = org_data['id'].unique()\n",
    "    idx = tr_data['point_of_interest']==tr_data['match_point_of_interest']\n",
    "    train_match = tr_data.loc[idx].groupby('id')['match_id'].apply(list).map(\" \".join).reset_index()\n",
    "    train_match.columns = ['id','candidates']\n",
    "    train_candidate = train_candidate.merge(train_match, on = 'id', how = 'left')\n",
    "    idx = ~train_candidate['candidates'].isna()\n",
    "    train_candidate.loc[idx, \"matches\"] += \" \" + train_candidate.loc[idx, \"candidates\"]\n",
    "    score = get_score(train_candidate, org_data)\n",
    "    print('1st_stage_max_score : ' + '{:.5f}'.format(score))\n",
    "    return score"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 47,
   "metadata": {},
   "outputs": [],
   "source": [
    "def categorical_similarity(A, B):\n",
    "    if not A or not B:\n",
    "        return -1\n",
    "\n",
    "    A = set(str(A).split(\", \"))\n",
    "    B = set(str(B).split(\", \"))\n",
    "\n",
    "    # Find intersection of two sets\n",
    "    nominator = A.intersection(B)\n",
    "\n",
    "    similarity_1 = len(nominator) / len(A)\n",
    "    similarity_2 = len(nominator) / len(B)\n",
    "\n",
    "    return max(similarity_1, similarity_2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 48,
   "metadata": {},
   "outputs": [],
   "source": [
    "%%cython\n",
    "# Optimized version\n",
    "import numpy as np\n",
    "cimport numpy as np\n",
    "\n",
    "import cython\n",
    "from libc.stdlib cimport malloc, free\n",
    "\n",
    "\n",
    "@cython.boundscheck(False) # turn off bounds-checking for entire function\n",
    "@cython.wraparound(False)  # turn off negative index wrapping for entire function\n",
    "def LCS(str S, str T):\n",
    "    if len(S) < len(T):\n",
    "        S, T = T, S\n",
    "\n",
    "    cdef int i, j\n",
    "    cdef np.uint16_t[:] dp_prev, dp_curr\n",
    "    \n",
    "    dp_prev = np.zeros(len(T) + 1, dtype=np.uint16)\n",
    "    dp_curr = np.zeros(len(T) + 1, dtype=np.uint16)\n",
    "\n",
    "    for i in range(len(S)):\n",
    "        for j in range(len(T)):\n",
    "            dp_curr[j + 1]  = max(dp_prev[j] + (1 if S[i] == T[j] else 0), dp_curr[j], dp_prev[j + 1])\n",
    "        dp_prev, dp_curr = dp_curr, dp_prev\n",
    "    return dp_prev[len(T)]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 49,
   "metadata": {},
   "outputs": [],
   "source": [
    "def make_tfidf_vec(df, col):\n",
    "    df_ = pd.concat([df[col], df[\"match_\" + col]]).drop_duplicates().to_frame()\n",
    "    df_ = df_.reset_index(drop=True)\n",
    "    df_.columns = [col]\n",
    "\n",
    "    tfidf_vectorizer = TfidfVectorizer(ngram_range=(3,3), analyzer=\"char_wb\", use_idf=True)\n",
    "    tfidf_vec = tfidf_vectorizer.fit_transform(df_[col].tolist())\n",
    "    tfidf_vec = {k:v for k,v in zip(df_[col].values, tfidf_vec)}\n",
    "    return tfidf_vec"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 50,
   "metadata": {},
   "outputs": [],
   "source": [
    "def _add_distance_features(args):\n",
    "    _, df = args\n",
    "\n",
    "    columns = ['name', 'address', 'city', 'state',\n",
    "               'zip', 'url', 'phone', 'categories']\n",
    "\n",
    "    for c in columns:\n",
    "        geshs = []\n",
    "        levens = []\n",
    "        jaros = []\n",
    "        lcss = []\n",
    "        tfidf_sims = []\n",
    "\n",
    "        if c in [\"name\", \"categories\"]:\n",
    "            tfidf_vec = make_tfidf_vec(df, c)\n",
    "\n",
    "\n",
    "        for str1, str2 in df[[f\"{c}\", f\"match_{c}\"]].values.astype(str):\n",
    "            if str1 != \"nan\" and str2 != \"nan\":\n",
    "                geshs.append(difflib.SequenceMatcher(None, str1, str2).ratio())\n",
    "                levens.append(Levenshtein.distance(str1, str2))\n",
    "                jaros.append(Levenshtein.jaro_winkler(str1, str2))\n",
    "                lcss.append(LCS(str(str1), str(str2)))\n",
    "\n",
    "                if c in [\"name\", \"categories\"]:\n",
    "                    sim = cosine_similarity(tfidf_vec[str1].reshape(1, -1), tfidf_vec[str2].reshape(1, -1))[0][0]\n",
    "                    tfidf_sims.append(sim)\n",
    "\n",
    "            else:\n",
    "                geshs.append(-1)\n",
    "                levens.append(-1)\n",
    "                jaros.append(-1)\n",
    "                lcss.append(-1)\n",
    "                \n",
    "                if c in [\"name\", \"categories\"]:\n",
    "                    tfidf_sims.append(-1)\n",
    "\n",
    "\n",
    "        df[f\"match_{c}_gesh\"] = geshs\n",
    "        df[f\"match_{c}_gesh\"] = df[f\"match_{c}_gesh\"].astype(np.float16)\n",
    "        df[f\"match_{c}_leven\"] = levens\n",
    "        df[f\"match_{c}_leven\"] = df[f\"match_{c}_leven\"].astype(np.float16)\n",
    "        df[f\"match_{c}_jaro\"] = jaros\n",
    "        df[f\"match_{c}_jaro\"] = df[f\"match_{c}_jaro\"].astype(np.float16)\n",
    "        df[f\"match_{c}_lcs\"] = lcss\n",
    "        df[f\"match_{c}_lcs\"] = df[f\"match_{c}_lcs\"].astype(np.float16)\n",
    "            \n",
    "        if not c in ['country', \"phone\", \"zip\"]:\n",
    "            df[f\"match_{c}_len\"] = df[f\"match_{c}\"].astype(str).map(len)\n",
    "            df[f\"{c}_len\"] = df[f\"{c}\"].astype(str).map(len)\n",
    "            df[f\"match_{c}_nleven\"] = df[f\"match_{c}_leven\"] / df[[f\"match_{c}_len\", f\"{c}_len\"]].max(axis=1)\n",
    "            df[f\"match_{c}_nleven\"] = df[f\"match_{c}_nleven\"].astype(np.float16)\n",
    "            df[f\"match_{c}_nlcsi\"] = df[f\"match_{c}_lcs\"] / df[f\"match_{c}_len\"]\n",
    "            df[f\"match_{c}_nlcs0\"] = df[f\"match_{c}_lcs\"] / df[f\"{c}_len\"]\n",
    "            df[f\"match_{c}_nlcsi\"] = df[f\"match_{c}_nlcsi\"].astype(np.float16)\n",
    "            df[f\"match_{c}_nlcs0\"] = df[f\"match_{c}_nlcs0\"].astype(np.float16)\n",
    "            df.drop(f'{c}_len',axis=1, inplace = True)\n",
    "            df.drop(f\"match_{c}_len\",axis=1, inplace = True)\n",
    "\n",
    "        if c in [\"name\", \"categories\"]:\n",
    "            df[f\"tfidf_sim_{c}\"] = tfidf_sims\n",
    "            df[f\"tfidf_sim_{c}\"] = df[f\"tfidf_sim_{c}\"].astype(np.float16)\n",
    "\n",
    "    return df\n",
    "\n",
    "\n",
    "def add_distance_features(df):\n",
    "    processes = multiprocessing.cpu_count()\n",
    "    with multiprocessing.Pool(processes=processes) as pool:\n",
    "        dfs = pool.imap_unordered(_add_distance_features, df.groupby('country'))\n",
    "        dfs = tqdm(dfs)\n",
    "        dfs = list(dfs)\n",
    "    df = pd.concat(dfs)\n",
    "    return df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 51,
   "metadata": {},
   "outputs": [],
   "source": [
    "def add_vec_sim_features(df, vec, vec_name, col):\n",
    "    sim_list = []\n",
    "    for str1, str2 in tqdm(df[[f\"{col}\", f\"match_{col}\"]].values.astype(str)):\n",
    "        if str1!=\"nan\" and str2!=\"nan\":\n",
    "            sim = dot(vec[str1], vec[str2]) / (norm(vec[str1])*norm(vec[str2]))\n",
    "        else:\n",
    "            sim = -1\n",
    "        sim_list.append(sim)\n",
    "    df[f\"{vec_name}_sim_{col}\"] = sim_list\n",
    "    return df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 52,
   "metadata": {},
   "outputs": [],
   "source": [
    "def reduce_data_size(df, features):\n",
    "    if MODE == 'local_train':\n",
    "        df = df[features + ['target', 'id', 'match_id']].copy()\n",
    "    elif MODE == 'kaggle_inference':\n",
    "        df = df[features + ['id', 'match_id']].copy()\n",
    "\n",
    "\n",
    "    df[features] = df[features].astype(np.float16)\n",
    "    for _ in range(5):\n",
    "        gc.collect()\n",
    "        \n",
    "    return df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 53,
   "metadata": {},
   "outputs": [],
   "source": [
    "def train_model(df, features):\n",
    "    params = {'objective': 'binary', \n",
    "              'boosting': 'gbdt',\n",
    "              'learning_rate': 0.1, \n",
    "              'metric': 'binary_logloss', \n",
    "              'seed': SEED, \n",
    "              'feature_pre_filter': False, \n",
    "              'lambda_l1': 0.5745709668124809, \n",
    "              'lambda_l2': 0.5123383865042099, \n",
    "              'num_leaves': 239, \n",
    "              'feature_fraction': 0.784, \n",
    "              'bagging_fraction': 1.0, \n",
    "              'bagging_freq': 0, \n",
    "              'min_child_samples': 5\n",
    "              }\n",
    "\n",
    "    # split folds\n",
    "    kf = StratifiedKFold(n_splits=N_SPLITS, shuffle=True, random_state=SEED)\n",
    "    for i, (trn_idx, val_idx) in enumerate(kf.split(df, df[\"target\"], df[\"target\"])):\n",
    "        df.loc[val_idx, \"fold\"] = i\n",
    "    \n",
    "    fi = pd.DataFrame()\n",
    "    oof = df[['id', 'match_id', 'target']].copy()\n",
    "    oof['prob'] = 0.0\n",
    "    scores = []\n",
    "\n",
    "    for i in range(N_SPLITS):\n",
    "        print('fold : ' + str(i))\n",
    "        tr_idx = df[df['fold'] != i].index\n",
    "        vl_idx = df[df['fold'] == i].index\n",
    "        tr_x, tr_y = df.loc[tr_idx, features], df.loc[tr_idx, 'target']\n",
    "        vl_x, vl_y = df.loc[vl_idx, features], df.loc[vl_idx, 'target']\n",
    "        tr_data = lgb.Dataset(tr_x, label=tr_y)\n",
    "        vl_data = lgb.Dataset(vl_x, label=vl_y)\n",
    "\n",
    "        model = lgb.train(params, tr_data, valid_sets=[tr_data, vl_data],\n",
    "                        num_boost_round=200000, early_stopping_rounds=100, verbose_eval=1000)\n",
    "\n",
    "        # 特徴量重要度\n",
    "        fi_tmp = pd.DataFrame()\n",
    "        fi_tmp['feature'] = model.feature_name()\n",
    "        fi_tmp['importance'] = model.feature_importance(importance_type='gain')\n",
    "        fi_tmp['iter'] = i\n",
    "        fi = fi.append(fi_tmp)\n",
    "\n",
    "        pred = model.predict(vl_x, num_iteration=model.best_iteration)\n",
    "        oof.loc[vl_idx, 'prob'] = pred\n",
    "\n",
    "        score = accuracy_score((pred > PROB_TH).astype(int), vl_y)\n",
    "        scores.append(score)\n",
    "        print(f'fold{i} | accuracy = ' + '{:.5f}'.format(score))\n",
    "\n",
    "        with open(OUTPUT_DIR + f'{exp_name}/model{i}.pickle', 'wb') as f:\n",
    "            pickle.dump(model, f)\n",
    "        \n",
    "    oof.to_csv(OUTPUT_DIR + f'{exp_name}/{exp_name}_oof.csv', index=False)\n",
    "\n",
    "    print('accuracy(mean) : ' + '{:.5f}'.format(np.mean(scores)))\n",
    "    print(scores)\n",
    "\n",
    "    fi_n = fi['feature'].nunique()\n",
    "    order = list(fi.groupby(\"feature\").mean().sort_values(\"importance\", ascending=False).index)\n",
    "    plt.figure(figsize=(10, fi_n*0.2))\n",
    "    sns.barplot(x=\"importance\", y=\"feature\", data=fi, order=order)\n",
    "    plt.title(f\"LGBM importance\")\n",
    "    plt.tight_layout()\n",
    "    plt.savefig(OUTPUT_DIR + f'{exp_name}/lgbm_importance.png')\n",
    "\n",
    "    return oof, np.mean(scores)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 54,
   "metadata": {},
   "outputs": [],
   "source": [
    "def model_inference(df, features):\n",
    "    pred = np.zeros(len(df))\n",
    "    for i in range(N_SPLITS):\n",
    "        with open(MODEL_DIR + f'model{i}.pickle', 'rb') as f:\n",
    "            model = pickle.load(f)\n",
    "        model.save_model(\"test.lgb\")\n",
    "        fi = ForestInference()\n",
    "        fi = ForestInference.load(\"test.lgb\", output_class=True, model_type=\"lightgbm\")\n",
    "        pred += fi.predict(df[features]) / N_SPLITS\n",
    "    return pred"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 55,
   "metadata": {},
   "outputs": [],
   "source": [
    "def transform_data(df, org_data):\n",
    "    train_candidate = pd.DataFrame()\n",
    "    train_candidate['id'] = org_data['id'].unique()\n",
    "    train_candidate['matches'] = org_data['id'].unique()\n",
    "\n",
    "    train_match = df[df['prob'] >= PROB_TH].copy()\n",
    "    train_match = train_match.groupby('id')['match_id'].apply(list).map(\" \".join).reset_index()\n",
    "    train_match.columns = ['id','candidates']\n",
    "    train_candidate = train_candidate.merge(train_match, on = 'id', how = 'left')\n",
    "    idx = ~train_candidate['candidates'].isna()\n",
    "    train_candidate.loc[idx, \"matches\"] += \" \" + train_candidate.loc[idx, \"candidates\"]\n",
    "    return train_candidate[['id', 'matches']]\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 56,
   "metadata": {},
   "outputs": [],
   "source": [
    "def postprocess(df):\n",
    "    id2match = dict(zip(df[\"id\"].values, df[\"matches\"].str.split()))\n",
    "\n",
    "    for match in tqdm(df[\"matches\"]):\n",
    "        match = match.split()\n",
    "        if len(match) == 1:        \n",
    "            continue\n",
    "\n",
    "        base = match[0]\n",
    "        for m in match[1:]:\n",
    "            if not base in id2match[m]:\n",
    "                id2match[m].append(base)\n",
    "    df[\"matches\"] = df[\"id\"].map(id2match).map(\" \".join)\n",
    "    return df "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 57,
   "metadata": {},
   "outputs": [],
   "source": [
    "def run_train():\n",
    "    train_origin = pd.read_csv(INPUT_DIR + \"train.csv\")\n",
    "    train_origin = preprocess(train_origin)\n",
    "    train_origin = convert_unicode(train_origin, [\"name\", \"address\", \"city\", \"state\"])\n",
    "    train_origin = convert_japanese_alphabet(train_origin, [\"name\", \"address\", \"city\", \"state\"])\n",
    "    \n",
    "    # trainデータの分割\n",
    "    kf = GroupKFold(n_splits=2)\n",
    "    for i, (trn_idx, val_idx) in enumerate(kf.split(train_origin, train_origin['point_of_interest'], train_origin['point_of_interest'])):\n",
    "        train_origin.loc[val_idx, \"set\"] = i\n",
    "\n",
    "    # 1st stage\n",
    "    train = pd.concat([\n",
    "        extract_candidate_dist(train_origin[train_origin[\"set\"]==0]), \n",
    "        extract_candidate_dist(train_origin[train_origin[\"set\"]==1]),\n",
    "        extract_candidate_tfidf_sim(train_origin[train_origin[\"set\"]==0], \"name\"), \n",
    "        extract_candidate_tfidf_sim(train_origin[train_origin[\"set\"]==1], \"name\")\n",
    "    ])\n",
    "    train = train.drop_duplicates(subset=[\"id\", \"match_id\"])\n",
    "    train = add_orgin_data(train, train_origin)\n",
    "    stage1_max_score = calc_max_score(train, train_origin)\n",
    "\n",
    "    # 2nd stage\n",
    "    train[\"habersine_dist\"] = haversine_np(train[\"longitude\"], train[\"latitude\"], train[\"match_longitude\"], train[\"match_latitude\"])\n",
    "    # create target\n",
    "    train['target'] = (train['point_of_interest'] == train['match_point_of_interest']).values.astype(int)\n",
    "    train[\"target\"] = train[\"target\"].fillna(0)\n",
    "\n",
    "    train = add_distance_features(train)\n",
    "    train[\"category_venn\"] = train[[\"categories\", \"match_categories\"]].apply(lambda row: categorical_similarity(row.categories, row.match_categories), axis=1)\n",
    "    \n",
    "    # reduce memory\n",
    "    train = train.drop(columns=['latitude', 'longitude', 'address', 'city', 'state', 'zip', 'country', 'url', 'phone',\n",
    "                                'match_latitude', 'match_longitude', 'match_address', 'match_city', 'match_state', \n",
    "                                'match_zip', 'match_country', 'match_url', 'match_phone'])\n",
    "    gc.collect()\n",
    "\n",
    "    # bert類似度\n",
    "    bert_vec_categories = make_bert_vec(train_origin[[\"categories\"]], \"categories\")\n",
    "    train = add_vec_sim_features(train, bert_vec_categories, \"bert\", \"categories\")\n",
    "    del bert_vec_categories\n",
    "    gc.collect()\n",
    "\n",
    "    print(\"add_bert_sim_name\")\n",
    "    bert_vec_name = make_bert_vec(train_origin[[\"name_org\"]], \"name_org\")\n",
    "    train = add_vec_sim_features(train, bert_vec_name, \"bert\", \"name_org\")\n",
    "    del bert_vec_name\n",
    "    gc.collect()\n",
    "\n",
    "\n",
    "    not_use_cols = ['match_state_leven', 'address', 'categories', 'point_of_interest', 'match_address_leven',\n",
    "                    'city', 'match_point_of_interest', 'match_name', 'match_categories_leven', 'match_longitude',\n",
    "                    'target', 'match_city_leven', 'zip', 'match_categories', 'match_city', 'match_latitude',\n",
    "                    'match_zip', 'match_url', 'id', 'match_set', 'country', 'match_state', 'match_address',\n",
    "                    'match_name_leven', 'match_id', 'latitude', 'url', 'set', 'name', 'phone', 'longitude',\n",
    "                    'match_url_leven', 'state', 'match_phone', 'match_country',\n",
    "                    \"name_org\", \"address_org\", \"city_org\", \"state_org\",\n",
    "                    \"match_name_org\", \"match_address_org\", \"match_city_org\", \"match_state_org\"]\n",
    "                    \n",
    "    features = [c for c in train.columns if c not in not_use_cols]\n",
    "    with open(OUTPUT_DIR + f'{exp_name}/features.pickle', 'wb') as f:\n",
    "        pickle.dump(features, f)\n",
    "\n",
    "    train = reduce_data_size(train, features)\n",
    "\n",
    "    oof, stage2_mean_accuracy = train_model(train, features)\n",
    "    oof = transform_data(oof, train_origin)\n",
    "\n",
    "    cv_score = get_score(oof, train_origin)\n",
    "    print(f'cv_score = ' + '{:.5f}'.format(cv_score))\n",
    "\n",
    "    oof = postprocess(oof)\n",
    "    cv_score_after_pp = get_score(oof, train_origin)\n",
    "    print(f'cv_score(after_pp) = ' + '{:.5f}'.format(cv_score_after_pp))\n",
    "\n",
    "\n",
    "    report = f'{exp_name}\\n'\n",
    "    report += memo + '\\n'\n",
    "    report += 'stage1_max_score : ' + '{:.5f}'.format(stage1_max_score) + '\\n'\n",
    "    report += 'stage2_mean_accuracy : ' + '{:.5f}'.format(stage2_mean_accuracy) + '\\n'\n",
    "    report += 'cv_score : ' + '{:.5f}'.format(cv_score) + '\\n'\n",
    "    report += 'cv_score_after_pp : ' + '{:.5f}'.format(cv_score_after_pp) + '\\n'\n",
    "    print(report)\n",
    "    line_notify.send(report)\n",
    "    slack_notify.send(report)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 58,
   "metadata": {},
   "outputs": [],
   "source": [
    "def run_inference():\n",
    "    test_origin = pd.read_csv(INPUT_DIR + \"test.csv\")\n",
    "    sub = pd.read_csv(INPUT_DIR + \"sample_submission.csv\", usecols=[\"id\"])\n",
    "\n",
    "    if len(test_origin) == 5:\n",
    "        test_origin = pd.read_csv(INPUT_DIR + \"train.csv\")\n",
    "        kf = GroupKFold(n_splits=100)\n",
    "        for i, (trn_idx, val_idx) in enumerate(kf.split(test_origin, test_origin['point_of_interest'], test_origin['point_of_interest'])):\n",
    "            test_origin = test_origin.loc[val_idx]\n",
    "            test_origin = test_origin.reset_index(drop=True)\n",
    "            sub = test_origin[[\"id\"]].copy()\n",
    "            break\n",
    "    \n",
    "    test_origin = preprocess(test_origin)\n",
    "    \n",
    "    kf = GroupKFold(n_splits=5)\n",
    "    for i, (trn_idx, val_idx) in enumerate(kf.split(test_origin, test_origin['country'].fillna(\"\"), test_origin['country'].fillna(\"\"))):\n",
    "        test_origin.loc[val_idx, \"c_fold\"] = int(i)\n",
    "\n",
    "    country_tests = []\n",
    "    for c_fold in test_origin[\"c_fold\"].unique():\n",
    "        test_origin_batch = test_origin[test_origin[\"c_fold\"]==c_fold].copy()\n",
    "        test_origin_batch = test_origin_batch.reset_index(drop=True)\n",
    "        test_origin_batch =test_origin_batch.drop(columns=[\"c_fold\"])\n",
    "\n",
    "        # 1st stage\n",
    "        test = pd.concat([extract_candidate_dist(test_origin_batch),\n",
    "                          extract_candidate_tfidf_sim(test_origin_batch, \"name\")\n",
    "        ])\n",
    "        test = test.drop_duplicates(subset=[\"id\", \"match_id\"])\n",
    "\n",
    "        # 2nd stage\n",
    "        print(\"add_org\")\n",
    "        test = add_orgin_data(test, test_origin_batch)\n",
    "        \n",
    "        print(\"add_dist_feat\")\n",
    "        test[\"habersine_dist\"] = haversine_np(test[\"longitude\"], test[\"latitude\"], test[\"match_longitude\"], test[\"match_latitude\"])\n",
    "        test = add_distance_features(test)\n",
    "        test[\"category_venn\"] = test[[\"categories\", \"match_categories\"]].apply(lambda row: categorical_similarity(row.categories, row.match_categories), axis=1)\n",
    "\n",
    "        # reduce memory\n",
    "        test = test.drop(columns=['latitude', 'longitude', 'address', 'city', 'state', 'zip', 'country', 'url', 'phone',\n",
    "                                    'match_latitude', 'match_longitude', 'match_address', 'match_city', 'match_state', \n",
    "                                    'match_zip', 'match_country', 'match_url', 'match_phone'])\n",
    "        gc.collect()\n",
    "\n",
    "\n",
    "        # bert類似度\n",
    "        bert_vec_categories = make_bert_vec(test_origin_batch[[\"categories\"]], \"categories\")\n",
    "        test = add_vec_sim_features(test, bert_vec_categories, \"bert\", \"categories\")\n",
    "        del bert_vec_categories\n",
    "        gc.collect()\n",
    "        bert_vec_name = make_bert_vec(test_origin_batch[[\"name\"]], \"name\")\n",
    "        test = add_vec_sim_features(test, bert_vec_name, \"bert\", \"name\")\n",
    "        del bert_vec_name\n",
    "        gc.collect()\n",
    "        \n",
    "        with open(MODEL_DIR + 'features.pickle', 'rb') as f:\n",
    "            features = pickle.load(f)\n",
    "        test = reduce_data_size(test, features)\n",
    "\n",
    "        print('test_dataset_info')\n",
    "        print(test.info())\n",
    "\n",
    "        test['prob'] = model_inference(test, features)\n",
    "        test = transform_data(test, test_origin_batch)\n",
    "        country_tests.append(test)\n",
    "\n",
    "        del test, test_origin_batch\n",
    "        gc.collect()\n",
    "\n",
    "    test = pd.concat(country_tests)\n",
    "    test = postprocess(test)\n",
    "\n",
    "    sub = sub.merge(test, on=\"id\", how=\"left\") \n",
    "    sub.to_csv('submission.csv', index=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 59,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/tmp/ipykernel_23791/375474420.py:3: DeprecationWarning: Call to deprecated method setMode. (Old API will be removed in v3.0.) -- Deprecated since version 2.1.\n",
      "  kakasi.setMode('H', 'a')  # Convert Hiragana into alphabet\n",
      "/tmp/ipykernel_23791/375474420.py:4: DeprecationWarning: Call to deprecated method setMode. (Old API will be removed in v3.0.) -- Deprecated since version 2.1.\n",
      "  kakasi.setMode('K', 'a')  # Convert Katakana into alphabet\n",
      "/tmp/ipykernel_23791/375474420.py:5: DeprecationWarning: Call to deprecated method setMode. (Old API will be removed in v3.0.) -- Deprecated since version 2.1.\n",
      "  kakasi.setMode('J', 'a')  # Convert Kanji into alphabet\n",
      "/tmp/ipykernel_23791/375474420.py:6: DeprecationWarning: Call to deprecated method getConverter. (Old API will be removed in v3.0.) -- Deprecated since version 2.1.\n",
      "  conversion = kakasi.getConverter()\n",
      "/tmp/ipykernel_23791/375474420.py:11: DeprecationWarning: Call to deprecated method do. (Old API will be removed in v3.0.) -- Deprecated since version 2.1.\n",
      "  row[column] = conversion.do(row[column])\n",
      "100%|██████████| 210/210 [00:20<00:00, 10.36it/s]\n",
      "100%|██████████| 211/211 [00:19<00:00, 10.57it/s]\n",
      "100%|██████████| 210/210 [00:39<00:00,  5.30it/s]\n",
      "100%|██████████| 211/211 [00:39<00:00,  5.33it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "1st_stage_max_score : 0.96927\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "213it [1:45:50, 29.82s/it]  \n",
      "Some weights of the model checkpoint at distilbert-base-multilingual-cased were not used when initializing DistilBertModel: ['vocab_projector.bias', 'vocab_projector.weight', 'vocab_layer_norm.bias', 'vocab_layer_norm.weight', 'vocab_transform.weight', 'vocab_transform.bias']\n",
      "- This IS expected if you are initializing DistilBertModel from the checkpoint of a model trained on another task or with another architecture (e.g. initializing a BertForSequenceClassification model from a BertForPreTraining model).\n",
      "- This IS NOT expected if you are initializing DistilBertModel from the checkpoint of a model that you expect to be exactly identical (initializing a BertForSequenceClassification model from a BertForSequenceClassification model).\n",
      "100%|██████████| 30952349/30952349 [08:41<00:00, 59323.49it/s] \n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "add_bert_sim_name\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Some weights of the model checkpoint at distilbert-base-multilingual-cased were not used when initializing DistilBertModel: ['vocab_projector.bias', 'vocab_projector.weight', 'vocab_layer_norm.bias', 'vocab_layer_norm.weight', 'vocab_transform.weight', 'vocab_transform.bias']\n",
      "- This IS expected if you are initializing DistilBertModel from the checkpoint of a model trained on another task or with another architecture (e.g. initializing a BertForSequenceClassification model from a BertForPreTraining model).\n",
      "- This IS NOT expected if you are initializing DistilBertModel from the checkpoint of a model that you expect to be exactly identical (initializing a BertForSequenceClassification model from a BertForSequenceClassification model).\n",
      "100%|██████████| 30952349/30952349 [10:14<00:00, 50395.93it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "fold : 0\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/kaggler/.local/lib/python3.8/site-packages/lightgbm/engine.py:181: UserWarning: 'early_stopping_rounds' argument is deprecated and will be removed in a future release of LightGBM. Pass 'early_stopping()' callback via 'callbacks' argument instead.\n",
      "  _log_warning(\"'early_stopping_rounds' argument is deprecated and will be removed in a future release of LightGBM. \"\n",
      "/home/kaggler/.local/lib/python3.8/site-packages/lightgbm/engine.py:239: UserWarning: 'verbose_eval' argument is deprecated and will be removed in a future release of LightGBM. Pass 'log_evaluation()' callback via 'callbacks' argument instead.\n",
      "  _log_warning(\"'verbose_eval' argument is deprecated and will be removed in a future release of LightGBM. \"\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[LightGBM] [Info] Number of positive: 933836, number of negative: 23828043\n",
      "[LightGBM] [Warning] Auto-choosing col-wise multi-threading, the overhead of testing was 1.911095 seconds.\n",
      "You can set `force_col_wise=true` to remove the overhead.\n",
      "[LightGBM] [Info] Total Bins 9308\n",
      "[LightGBM] [Info] Number of data points in the train set: 24761879, number of used features: 50\n",
      "[LightGBM] [Info] [binary:BoostFromScore]: pavg=0.037713 -> initscore=-3.239318\n",
      "[LightGBM] [Info] Start training from score -3.239318\n",
      "Training until validation scores don't improve for 100 rounds\n",
      "[1000]\ttraining's binary_logloss: 0.0352817\tvalid_1's binary_logloss: 0.0405745\n",
      "[2000]\ttraining's binary_logloss: 0.0286712\tvalid_1's binary_logloss: 0.0370868\n",
      "[3000]\ttraining's binary_logloss: 0.0239768\tvalid_1's binary_logloss: 0.0346348\n",
      "[4000]\ttraining's binary_logloss: 0.0203345\tvalid_1's binary_logloss: 0.0326725\n",
      "[5000]\ttraining's binary_logloss: 0.0174896\tvalid_1's binary_logloss: 0.0311353\n",
      "[6000]\ttraining's binary_logloss: 0.0150032\tvalid_1's binary_logloss: 0.0297089\n",
      "[7000]\ttraining's binary_logloss: 0.0129721\tvalid_1's binary_logloss: 0.0285706\n",
      "[8000]\ttraining's binary_logloss: 0.011266\tvalid_1's binary_logloss: 0.0275982\n",
      "[9000]\ttraining's binary_logloss: 0.00983963\tvalid_1's binary_logloss: 0.0267752\n",
      "[10000]\ttraining's binary_logloss: 0.00860425\tvalid_1's binary_logloss: 0.0260415\n",
      "[11000]\ttraining's binary_logloss: 0.00755947\tvalid_1's binary_logloss: 0.0254374\n",
      "[12000]\ttraining's binary_logloss: 0.00666467\tvalid_1's binary_logloss: 0.0249103\n",
      "[13000]\ttraining's binary_logloss: 0.0058863\tvalid_1's binary_logloss: 0.0244368\n",
      "[14000]\ttraining's binary_logloss: 0.00519857\tvalid_1's binary_logloss: 0.024033\n",
      "[15000]\ttraining's binary_logloss: 0.0046395\tvalid_1's binary_logloss: 0.0237182\n",
      "[16000]\ttraining's binary_logloss: 0.00414871\tvalid_1's binary_logloss: 0.0234379\n",
      "[17000]\ttraining's binary_logloss: 0.00371711\tvalid_1's binary_logloss: 0.0231855\n",
      "[18000]\ttraining's binary_logloss: 0.00333623\tvalid_1's binary_logloss: 0.0229747\n",
      "[19000]\ttraining's binary_logloss: 0.00301773\tvalid_1's binary_logloss: 0.0227995\n",
      "[20000]\ttraining's binary_logloss: 0.0027351\tvalid_1's binary_logloss: 0.0226572\n",
      "[21000]\ttraining's binary_logloss: 0.0024961\tvalid_1's binary_logloss: 0.0225343\n",
      "[22000]\ttraining's binary_logloss: 0.0022874\tvalid_1's binary_logloss: 0.0224538\n",
      "[23000]\ttraining's binary_logloss: 0.0021036\tvalid_1's binary_logloss: 0.0223852\n",
      "[24000]\ttraining's binary_logloss: 0.00194443\tvalid_1's binary_logloss: 0.0223364\n",
      "[25000]\ttraining's binary_logloss: 0.00180002\tvalid_1's binary_logloss: 0.022297\n",
      "Early stopping, best iteration is:\n",
      "[25115]\ttraining's binary_logloss: 0.00178388\tvalid_1's binary_logloss: 0.0222933\n",
      "fold0 | accuracy = 0.99290\n",
      "fold : 1\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/kaggler/.local/lib/python3.8/site-packages/lightgbm/engine.py:181: UserWarning: 'early_stopping_rounds' argument is deprecated and will be removed in a future release of LightGBM. Pass 'early_stopping()' callback via 'callbacks' argument instead.\n",
      "  _log_warning(\"'early_stopping_rounds' argument is deprecated and will be removed in a future release of LightGBM. \"\n",
      "/home/kaggler/.local/lib/python3.8/site-packages/lightgbm/engine.py:239: UserWarning: 'verbose_eval' argument is deprecated and will be removed in a future release of LightGBM. Pass 'log_evaluation()' callback via 'callbacks' argument instead.\n",
      "  _log_warning(\"'verbose_eval' argument is deprecated and will be removed in a future release of LightGBM. \"\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[LightGBM] [Info] Number of positive: 933077, number of negative: 23828802\n",
      "[LightGBM] [Warning] Auto-choosing col-wise multi-threading, the overhead of testing was 1.806673 seconds.\n",
      "You can set `force_col_wise=true` to remove the overhead.\n",
      "[LightGBM] [Info] Total Bins 9253\n",
      "[LightGBM] [Info] Number of data points in the train set: 24761879, number of used features: 50\n",
      "[LightGBM] [Info] [binary:BoostFromScore]: pavg=0.037682 -> initscore=-3.240163\n",
      "[LightGBM] [Info] Start training from score -3.240163\n",
      "Training until validation scores don't improve for 100 rounds\n",
      "[1000]\ttraining's binary_logloss: 0.0352112\tvalid_1's binary_logloss: 0.0407356\n"
     ]
    },
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
      "\u001b[0;32m/tmp/ipykernel_23791/3247393611.py\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[1;32m      1\u001b[0m \u001b[0;32mif\u001b[0m \u001b[0mMODE\u001b[0m \u001b[0;34m==\u001b[0m \u001b[0;34m'local_train'\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m----> 2\u001b[0;31m     \u001b[0mrun_train\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m      3\u001b[0m \u001b[0;32melif\u001b[0m \u001b[0mMODE\u001b[0m \u001b[0;34m==\u001b[0m \u001b[0;34m'kaggle_inference'\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      4\u001b[0m     \u001b[0mrun_inference\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/tmp/ipykernel_23791/819457231.py\u001b[0m in \u001b[0;36mrun_train\u001b[0;34m()\u001b[0m\n\u001b[1;32m     64\u001b[0m     \u001b[0mtrain\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mreduce_data_size\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mtrain\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mfeatures\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     65\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 66\u001b[0;31m     \u001b[0moof\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mstage2_mean_accuracy\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mtrain_model\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mtrain\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mfeatures\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     67\u001b[0m     \u001b[0moof\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mtransform_data\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0moof\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mtrain_origin\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     68\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/tmp/ipykernel_23791/597276122.py\u001b[0m in \u001b[0;36mtrain_model\u001b[0;34m(df, features)\u001b[0m\n\u001b[1;32m     34\u001b[0m         \u001b[0mvl_data\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mlgb\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mDataset\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mvl_x\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mlabel\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mvl_y\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     35\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 36\u001b[0;31m         model = lgb.train(params, tr_data, valid_sets=[tr_data, vl_data],\n\u001b[0m\u001b[1;32m     37\u001b[0m                         num_boost_round=200000, early_stopping_rounds=100, verbose_eval=1000)\n\u001b[1;32m     38\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/.local/lib/python3.8/site-packages/lightgbm/engine.py\u001b[0m in \u001b[0;36mtrain\u001b[0;34m(params, train_set, num_boost_round, valid_sets, valid_names, fobj, feval, init_model, feature_name, categorical_feature, early_stopping_rounds, evals_result, verbose_eval, learning_rates, keep_training_booster, callbacks)\u001b[0m\n\u001b[1;32m    290\u001b[0m                                     evaluation_result_list=None))\n\u001b[1;32m    291\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 292\u001b[0;31m         \u001b[0mbooster\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mupdate\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mfobj\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mfobj\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    293\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    294\u001b[0m         \u001b[0mevaluation_result_list\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;34m[\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/.local/lib/python3.8/site-packages/lightgbm/basic.py\u001b[0m in \u001b[0;36mupdate\u001b[0;34m(self, train_set, fobj)\u001b[0m\n\u001b[1;32m   3019\u001b[0m             \u001b[0;32mif\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m__set_objective_to_none\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   3020\u001b[0m                 \u001b[0;32mraise\u001b[0m \u001b[0mLightGBMError\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m'Cannot update due to null objective function.'\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 3021\u001b[0;31m             _safe_call(_LIB.LGBM_BoosterUpdateOneIter(\n\u001b[0m\u001b[1;32m   3022\u001b[0m                 \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mhandle\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   3023\u001b[0m                 ctypes.byref(is_finished)))\n",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m: "
     ]
    }
   ],
   "source": [
    "if MODE == 'local_train':\n",
    "    run_train()\n",
    "elif MODE == 'kaggle_inference':\n",
    "    run_inference()"
   ]
  }
 ],
 "metadata": {
  "interpreter": {
   "hash": "d4d1e4263499bec80672ea0156c357c1ee493ec2b1c70f0acce89fc37c4a6abe"
  },
  "kernelspec": {
   "display_name": "Python 3.8.12 ('base')",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.12"
  },
  "orig_nbformat": 4
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
