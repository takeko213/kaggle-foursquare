{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# opt"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "metadata": {},
   "outputs": [],
   "source": [
    "MODE = 'local_train'\n",
    "#MODE = 'kaggle_inference'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "metadata": {},
   "outputs": [],
   "source": [
    "exp_name = 'exp006'\n",
    "memo = 'exp001のhypopt'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "The Cython extension is already loaded. To reload it, use:\n",
      "  %reload_ext Cython\n"
     ]
    }
   ],
   "source": [
    "import os\n",
    "import sys\n",
    "import gc\n",
    "\n",
    "if MODE == 'local_train':\n",
    "    sys.path.append('/home/kaggler/.local/lib/python3.8/site-packages')\n",
    "    from dotenv import load_dotenv\n",
    "    load_dotenv\n",
    "    sys.path.append(os.getenv('UTILS_PATH'))\n",
    "    import line_notify\n",
    "    import slack_notify\n",
    "    \n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt; plt.style.use(\"ggplot\")\n",
    "import seaborn as sns\n",
    "from sklearn.metrics.pairwise import haversine_distances\n",
    "from sklearn.model_selection import GroupKFold, KFold, StratifiedKFold\n",
    "from sklearn.metrics import accuracy_score\n",
    "import lightgbm as lgb\n",
    "import optuna.integration.lightgbm as optuna_lgb\n",
    "import itertools\n",
    "from tqdm import tqdm\n",
    "from scipy.spatial.distance import canberra\n",
    "from sklearn.neighbors import KNeighborsRegressor\n",
    "import multiprocessing\n",
    "import Levenshtein\n",
    "import difflib\n",
    "import pickle\n",
    "%load_ext Cython"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 42,
   "metadata": {},
   "outputs": [],
   "source": [
    "# directry_setting\n",
    "if MODE == 'local_train':\n",
    "    INPUT_DIR = os.getenv('INPUT_DIR')\n",
    "    OUTPUT_DIR = os.getenv('OUTPUT_DIR')\n",
    "    MODEL_DIR = os.getenv('OUTPUT_DIR')\n",
    "    os.makedirs(OUTPUT_DIR + exp_name, exist_ok=True)\n",
    "\n",
    "elif MODE == 'kaggle_inference':\n",
    "    INPUT_DIR = '/kaggle/input/foursquare-location-matching/'\n",
    "    OUTPUT_DIR = './'\n",
    "    MODEL_DIR = f'../input/fs{exp_name}/'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 43,
   "metadata": {},
   "outputs": [],
   "source": [
    "# CONFIG\n",
    "SEED = 42\n",
    "N_NEIGHBORS = 10\n",
    "N_SPLITS = 5\n",
    "PROB_TH = 0.5"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 44,
   "metadata": {},
   "outputs": [],
   "source": [
    "def preprocess(df):\n",
    "    columns = ['id', 'name', 'address', 'city', 'state',\n",
    "        'zip', 'country', 'url', 'phone', 'categories']\n",
    "    for c in columns:\n",
    "        if c != \"id\":\n",
    "            df[c] = df[c].astype(str).str.lower()\n",
    "    return df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 45,
   "metadata": {},
   "outputs": [],
   "source": [
    "def extract_candidate(df):\n",
    "    dfs = []\n",
    "    candidates = pd.DataFrame()\n",
    "    for country, country_df in tqdm(df.groupby(\"country\")):\n",
    "        country_df = country_df.reset_index(drop=True)\n",
    "        \n",
    "        knn = KNeighborsRegressor(n_neighbors=min(len(country_df), N_NEIGHBORS), \n",
    "                                  metric='haversine', n_jobs=-1)\n",
    "        knn.fit(country_df[['latitude','longitude']], country_df.index)\n",
    "        dists, nears = knn.kneighbors(country_df[['latitude','longitude']], return_distance=True)\n",
    "        \n",
    "        k = min(len(country_df), N_NEIGHBORS)\n",
    "        country_df['match_id'] = country_df['id'].values[nears[:, :k]].tolist()\n",
    "        country_df['d_near'] = dists[:, :k].tolist()\n",
    "        dfs.append(country_df[['id','match_id','d_near']])\n",
    "    df = pd.concat(dfs).reset_index(drop=True)\n",
    "    return df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 46,
   "metadata": {},
   "outputs": [],
   "source": [
    "def add_orgin_data(df, org_df):\n",
    "    df = df.explode(['match_id','d_near'])\n",
    "    df = df.loc[df['id'] != df['match_id']].copy()\n",
    "    df = df.merge(org_df, on='id')\n",
    "    df = df.merge(org_df.add_prefix('match_'), on='match_id')\n",
    "    df = df.reset_index(drop=True)\n",
    "    return df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 47,
   "metadata": {},
   "outputs": [],
   "source": [
    "# https://www.kaggle.com/code/columbia2131/foursquare-iou-metrics\n",
    "def get_id2poi(input_df: pd.DataFrame) -> dict:\n",
    "    return dict(zip(input_df['id'], input_df['point_of_interest']))\n",
    "\n",
    "def get_poi2ids(input_df: pd.DataFrame) -> dict:\n",
    "    return input_df.groupby('point_of_interest')['id'].apply(set).to_dict()\n",
    "\n",
    "def get_score(input_df: pd.DataFrame, org_data):\n",
    "    scores = []\n",
    "    id2poi = get_id2poi(org_data)\n",
    "    poi2ids = get_poi2ids(org_data)\n",
    "    for id_str, matches in zip(input_df['id'].to_numpy(), input_df['matches'].to_numpy()):\n",
    "        targets = poi2ids[id2poi[id_str]]\n",
    "        preds = set(matches.split())\n",
    "        score = len((targets & preds)) / len((targets | preds))\n",
    "        scores.append(score)\n",
    "    scores = np.array(scores)\n",
    "    return scores.mean()\n",
    "\n",
    "def calc_max_score(tr_data, org_data):\n",
    "    train_candidate = pd.DataFrame()\n",
    "    train_candidate['id'] = org_data['id'].unique()\n",
    "    train_candidate['matches'] = org_data['id'].unique()\n",
    "    idx = tr_data['point_of_interest']==tr_data['match_point_of_interest']\n",
    "    train_match = tr_data.loc[idx].groupby('id')['match_id'].apply(list).map(\" \".join).reset_index()\n",
    "    train_match.columns = ['id','candidates']\n",
    "    train_candidate = train_candidate.merge(train_match, on = 'id', how = 'left')\n",
    "    idx = ~train_candidate['candidates'].isna()\n",
    "    train_candidate.loc[idx, \"matches\"] += \" \" + train_candidate.loc[idx, \"candidates\"]\n",
    "    score = get_score(train_candidate, org_data)\n",
    "    print('1st_stage_max_score : ' + '{:.5f}'.format(score))\n",
    "    return score"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 48,
   "metadata": {},
   "outputs": [],
   "source": [
    "%%cython\n",
    "def LCS(str S, str T):\n",
    "    cdef int i, j\n",
    "    cdef list dp = [[0] * (len(T) + 1) for _ in range(len(S) + 1)]\n",
    "    for i in range(len(S)):\n",
    "        for j in range(len(T)):\n",
    "            dp[i + 1][j + 1] = max(dp[i][j] + (S[i] == T[j]), dp[i + 1][j], dp[i][j + 1], dp[i + 1][j + 1])\n",
    "    return dp[len(S)][len(T)]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 49,
   "metadata": {},
   "outputs": [],
   "source": [
    "def _add_distance_features(args):\n",
    "    _, df = args\n",
    "\n",
    "    columns = ['name', 'address', 'city', 'state',\n",
    "               'zip', 'country', 'url', 'phone', 'categories']\n",
    "\n",
    "    for c in columns:\n",
    "        geshs = []\n",
    "        levens = []\n",
    "        jaros = []\n",
    "        lcss = []\n",
    "        for str1, str2 in df[[f\"{c}\", f\"match_{c}\"]].values.astype(str):\n",
    "            if str1==str1 and str2==str2:\n",
    "                geshs.append(difflib.SequenceMatcher(None, str1, str2).ratio())\n",
    "                levens.append(Levenshtein.distance(str1, str2))\n",
    "                jaros.append(Levenshtein.jaro_winkler(str1, str2))\n",
    "                lcss.append(LCS(str(str1), str(str2)))\n",
    "            else:\n",
    "                geshs.append(-1)\n",
    "                levens.append(-1)\n",
    "                jaros.append(-1)\n",
    "        df[f\"match_{c}_gesh\"] = geshs\n",
    "        df[f\"match_{c}_leven\"] = levens\n",
    "        df[f\"match_{c}_jaro\"] = jaros\n",
    "        df[f\"match_{c}_lcs\"] = lcss\n",
    "            \n",
    "        if not c in ['country', \"phone\", \"zip\"]:\n",
    "            df[f\"match_{c}_len\"] = df[f\"match_{c}\"].astype(str).map(len)\n",
    "            df[f\"match_{c}_nleven\"] = df[f\"match_{c}_leven\"] / df[f\"match_{c}_len\"]\n",
    "            df[f\"match_{c}_nlcsi\"] = df[f\"match_{c}_lcs\"] / df[f\"match_{c}_len\"]\n",
    "    return df\n",
    "\n",
    "\n",
    "def add_distance_features(df):\n",
    "    processes = multiprocessing.cpu_count()\n",
    "    with multiprocessing.Pool(processes=processes) as pool:\n",
    "        dfs = pool.imap_unordered(_add_distance_features, df.groupby('country'))\n",
    "        dfs = tqdm(dfs)\n",
    "        dfs = list(dfs)\n",
    "    df = pd.concat(dfs)\n",
    "    return df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 50,
   "metadata": {},
   "outputs": [],
   "source": [
    "def reduce_data_size(df, features):\n",
    "    if MODE == 'local_train':\n",
    "        df = df[features + ['target', 'id', 'match_id']].copy()\n",
    "    elif MODE == 'kaggle_inference':\n",
    "        df = df[features + ['id', 'match_id']].copy()\n",
    "\n",
    "\n",
    "    df[features] = df[features].astype(np.float16)\n",
    "    for _ in range(5):\n",
    "        gc.collect()\n",
    "        \n",
    "    return df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 51,
   "metadata": {},
   "outputs": [],
   "source": [
    "def train_model(df, features):\n",
    "    params = {\n",
    "     'objective': 'binary',\n",
    "     'boosting': 'gbdt',\n",
    "     'learning_rate': 0.1,\n",
    "     'metric': 'binary_logloss',\n",
    "     'seed': SEED\n",
    "    }\n",
    "\n",
    "    # split folds\n",
    "    kf = StratifiedKFold(n_splits=N_SPLITS, shuffle=True, random_state=SEED)\n",
    "    for i, (trn_idx, val_idx) in enumerate(kf.split(df, df[\"target\"], df[\"target\"])):\n",
    "        df.loc[val_idx, \"fold\"] = i\n",
    "    \n",
    "    fi = pd.DataFrame()\n",
    "    oof = df[['id', 'match_id', 'target']].copy()\n",
    "    oof['prob'] = 0.0\n",
    "    scores = []\n",
    "\n",
    "    for i in range(N_SPLITS):\n",
    "        print('fold : ' + str(i))\n",
    "        tr_idx = df[df['fold'] != i].index\n",
    "        vl_idx = df[df['fold'] == i].index\n",
    "        tr_x, tr_y = df.loc[tr_idx, features], df.loc[tr_idx, 'target']\n",
    "        vl_x, vl_y = df.loc[vl_idx, features], df.loc[vl_idx, 'target']\n",
    "        tr_data = lgb.Dataset(tr_x, label=tr_y)\n",
    "        vl_data = lgb.Dataset(vl_x, label=vl_y)\n",
    "\n",
    "        model = lgb.train(params, tr_data, valid_sets=[tr_data, vl_data],\n",
    "                        num_boost_round=20000, early_stopping_rounds=100, verbose_eval=1000)\n",
    "\n",
    "        # 特徴量重要度\n",
    "        fi_tmp = pd.DataFrame()\n",
    "        fi_tmp['feature'] = model.feature_name()\n",
    "        fi_tmp['importance'] = model.feature_importance(importance_type='gain')\n",
    "        fi_tmp['iter'] = i\n",
    "        fi = fi.append(fi_tmp)\n",
    "\n",
    "        pred = model.predict(vl_x, num_iteration=model.best_iteration)\n",
    "        oof.loc[vl_idx, 'prob'] = pred\n",
    "\n",
    "        score = accuracy_score((pred > PROB_TH).astype(int), vl_y)\n",
    "        scores.append(score)\n",
    "        print(f'fold{i} | accuracy = ' + '{:.5f}'.format(score))\n",
    "\n",
    "        with open(OUTPUT_DIR + f'{exp_name}/model{i}.pickle', 'wb') as f:\n",
    "            pickle.dump(model, f)\n",
    "        \n",
    "    oof.to_csv(OUTPUT_DIR + f'{exp_name}/{exp_name}_oof.csv', index=False)\n",
    "\n",
    "    print('accuracy(mean) : ' + '{:.5f}'.format(np.mean(scores)))\n",
    "    print(scores)\n",
    "\n",
    "    fi_n = fi['feature'].nunique()\n",
    "    order = list(fi.groupby(\"feature\").mean().sort_values(\"importance\", ascending=False).index)\n",
    "    plt.figure(figsize=(10, fi_n*0.2))\n",
    "    sns.barplot(x=\"importance\", y=\"feature\", data=fi, order=order)\n",
    "    plt.title(f\"LGBM importance\")\n",
    "    plt.tight_layout()\n",
    "    plt.savefig(OUTPUT_DIR + f'{exp_name}/lgbm_importance.png')\n",
    "\n",
    "    return oof, np.mean(scores)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 52,
   "metadata": {},
   "outputs": [],
   "source": [
    "def model_inference(df, features):\n",
    "    pred = np.zeros(len(df))\n",
    "    for i in range(N_SPLITS):\n",
    "        with open(MODEL_DIR + f'model{i}.pickle', 'rb') as f:\n",
    "            model = pickle.load(f)\n",
    "        pred += model.predict(df[features], num_iteration=model.best_iteration) / N_SPLITS\n",
    "    return pred"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 53,
   "metadata": {},
   "outputs": [],
   "source": [
    "def transform_data(df, org_data):\n",
    "    train_candidate = pd.DataFrame()\n",
    "    train_candidate['id'] = org_data['id'].unique()\n",
    "    train_candidate['matches'] = org_data['id'].unique()\n",
    "\n",
    "    train_match = df[df['prob'] >= PROB_TH].copy()\n",
    "    train_match = train_match.groupby('id')['match_id'].apply(list).map(\" \".join).reset_index()\n",
    "    train_match.columns = ['id','candidates']\n",
    "    train_candidate = train_candidate.merge(train_match, on = 'id', how = 'left')\n",
    "    idx = ~train_candidate['candidates'].isna()\n",
    "    train_candidate.loc[idx, \"matches\"] += \" \" + train_candidate.loc[idx, \"candidates\"]\n",
    "    return train_candidate[['id', 'matches']]\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 54,
   "metadata": {},
   "outputs": [],
   "source": [
    "def postprocess(df):\n",
    "    id2match = dict(zip(df[\"id\"].values, df[\"matches\"].str.split()))\n",
    "\n",
    "    for match in tqdm(df[\"matches\"]):\n",
    "        match = match.split()\n",
    "        if len(match) == 1:        \n",
    "            continue\n",
    "\n",
    "        base = match[0]\n",
    "        for m in match[1:]:\n",
    "            if not base in id2match[m]:\n",
    "                id2match[m].append(base)\n",
    "    df[\"matches\"] = df[\"id\"].map(id2match).map(\" \".join)\n",
    "    return df "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 55,
   "metadata": {},
   "outputs": [],
   "source": [
    "def run_train():\n",
    "    train_origin = pd.read_csv(INPUT_DIR + \"train.csv\")\n",
    "    train_origin = preprocess(train_origin)\n",
    "\n",
    "    # trainデータの分割\n",
    "    kf = GroupKFold(n_splits=2)\n",
    "    for i, (trn_idx, val_idx) in enumerate(kf.split(train_origin, train_origin['point_of_interest'], train_origin['point_of_interest'])):\n",
    "        train_origin.loc[val_idx, \"set\"] = i\n",
    "\n",
    "    # 1st stage\n",
    "    train = pd.concat([\n",
    "        extract_candidate(train_origin[train_origin[\"set\"]==0]), \n",
    "        extract_candidate(train_origin[train_origin[\"set\"]==1]), \n",
    "    ])\n",
    "    train = add_orgin_data(train, train_origin)\n",
    "    stage1_max_score = calc_max_score(train, train_origin)\n",
    "\n",
    "    # 2nd stage\n",
    "    # create target\n",
    "    train['target'] = (train['point_of_interest'] == train['match_point_of_interest']).values.astype(int)\n",
    "    train[\"target\"] = train[\"target\"].fillna(0)\n",
    "\n",
    "    train = add_distance_features(train)\n",
    "\n",
    "    not_use_cols = ['match_state_leven', 'address', 'categories', 'point_of_interest', 'match_address_leven',\n",
    "                    'city', 'match_point_of_interest', 'match_name', 'match_categories_leven', 'match_longitude',\n",
    "                    'target', 'match_city_leven', 'zip', 'match_categories', 'match_city', 'match_latitude',\n",
    "                    'match_zip', 'match_url', 'id', 'match_set', 'country', 'match_state', 'match_address',\n",
    "                    'match_name_leven', 'match_id', 'latitude', 'url', 'set', 'name', 'phone', 'longitude',\n",
    "                    'match_url_leven', 'state', 'match_phone', 'match_country']\n",
    "    features = [c for c in train.columns if c not in not_use_cols]\n",
    "    with open(OUTPUT_DIR + f'{exp_name}/features.pickle', 'wb') as f:\n",
    "        pickle.dump(features, f)\n",
    "\n",
    "    train = reduce_data_size(train, features)\n",
    "\n",
    "    oof, stage2_mean_accuracy = train_model(train, features)\n",
    "    oof = transform_data(oof, train_origin)\n",
    "\n",
    "    cv_score = get_score(oof, train_origin)\n",
    "    print(f'cv_score = ' + '{:.5f}'.format(cv_score))\n",
    "\n",
    "    oof = postprocess(oof)\n",
    "    cv_score_after_pp = get_score(oof, train_origin)\n",
    "    print(f'cv_score(after_pp) = ' + '{:.5f}'.format(cv_score_after_pp))\n",
    "\n",
    "\n",
    "    report = f'{exp_name}\\n'\n",
    "    report += memo + '\\n'\n",
    "    report += 'stage1_max_score : ' + '{:.5f}'.format(stage1_max_score) + '\\n'\n",
    "    report += 'stage2_mean_accuracy : ' + '{:.5f}'.format(stage2_mean_accuracy) + '\\n'\n",
    "    report += 'cv_score : ' + '{:.5f}'.format(cv_score) + '\\n'\n",
    "    report += 'cv_score_after_pp : ' + '{:.5f}'.format(cv_score_after_pp) + '\\n'\n",
    "    print(report)\n",
    "    line_notify.send(report)\n",
    "    slack_notify.send(report)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 56,
   "metadata": {},
   "outputs": [],
   "source": [
    "def run_inference():\n",
    "    test_origin = pd.read_csv(INPUT_DIR + \"test.csv\")\n",
    "    test_origin = preprocess(test_origin)\n",
    "\n",
    "    # 1st stage\n",
    "    test = extract_candidate(test_origin)\n",
    "\n",
    "    # 2nd stage\n",
    "    test = add_orgin_data(test, test_origin)\n",
    "    test = add_distance_features(test)\n",
    "    with open(MODEL_DIR + 'features.pickle', 'rb') as f:\n",
    "        features = pickle.load(f)\n",
    "    test = reduce_data_size(test, features)\n",
    "    test['prob'] = model_inference(test, features)\n",
    "    test = transform_data(test, test_origin)\n",
    "    test = postprocess(test)\n",
    "    test.to_csv('submission.csv', index=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 57,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 210/210 [00:55<00:00,  3.79it/s]\n",
      "100%|██████████| 211/211 [00:55<00:00,  3.80it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "1st_stage_max_score : 0.89040\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "213it [13:58,  3.93s/it] \n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "fold : 0\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\u001b[32m[I 2022-06-01 01:54:17,157]\u001b[0m A new study created in memory with name: no-name-b047d011-1d03-493b-b7cc-c6dccdaeb151\u001b[0m\n",
      "feature_fraction, val_score: inf:   0%|          | 0/7 [00:00<?, ?it/s]/home/kaggler/.local/lib/python3.8/site-packages/lightgbm/engine.py:181: UserWarning: 'early_stopping_rounds' argument is deprecated and will be removed in a future release of LightGBM. Pass 'early_stopping()' callback via 'callbacks' argument instead.\n",
      "  _log_warning(\"'early_stopping_rounds' argument is deprecated and will be removed in a future release of LightGBM. \"\n",
      "/home/kaggler/.local/lib/python3.8/site-packages/lightgbm/engine.py:239: UserWarning: 'verbose_eval' argument is deprecated and will be removed in a future release of LightGBM. Pass 'log_evaluation()' callback via 'callbacks' argument instead.\n",
      "  _log_warning(\"'verbose_eval' argument is deprecated and will be removed in a future release of LightGBM. \"\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[LightGBM] [Info] Number of positive: 572173, number of negative: 7626717\n",
      "[LightGBM] [Warning] Auto-choosing row-wise multi-threading, the overhead of testing was 0.377635 seconds.\n",
      "You can set `force_row_wise=true` to remove the overhead.\n",
      "And if memory is not enough, you can set `force_col_wise=true`.\n",
      "[LightGBM] [Info] Total Bins 7249\n",
      "[LightGBM] [Info] Number of data points in the train set: 8198890, number of used features: 48\n",
      "[LightGBM] [Info] [binary:BoostFromScore]: pavg=0.069787 -> initscore=-2.589971\n",
      "[LightGBM] [Info] Start training from score -2.589971\n",
      "Training until validation scores don't improve for 100 rounds\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "feature_fraction, val_score: 0.079990:  14%|#4        | 1/7 [02:47<16:45, 167.51s/it]\u001b[32m[I 2022-06-01 01:57:04,668]\u001b[0m Trial 0 finished with value: 0.07999015131465316 and parameters: {'feature_fraction': 0.7}. Best is trial 0 with value: 0.07999015131465316.\u001b[0m\n",
      "feature_fraction, val_score: 0.079990:  14%|#4        | 1/7 [02:47<16:45, 167.51s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Early stopping, best iteration is:\n",
      "[741]\tvalid_0's binary_logloss: 0.0770319\tvalid_1's binary_logloss: 0.0799902\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/kaggler/.local/lib/python3.8/site-packages/lightgbm/engine.py:181: UserWarning: 'early_stopping_rounds' argument is deprecated and will be removed in a future release of LightGBM. Pass 'early_stopping()' callback via 'callbacks' argument instead.\n",
      "  _log_warning(\"'early_stopping_rounds' argument is deprecated and will be removed in a future release of LightGBM. \"\n",
      "/home/kaggler/.local/lib/python3.8/site-packages/lightgbm/engine.py:239: UserWarning: 'verbose_eval' argument is deprecated and will be removed in a future release of LightGBM. Pass 'log_evaluation()' callback via 'callbacks' argument instead.\n",
      "  _log_warning(\"'verbose_eval' argument is deprecated and will be removed in a future release of LightGBM. \"\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[LightGBM] [Info] Number of positive: 572173, number of negative: 7626717\n",
      "[LightGBM] [Warning] Auto-choosing row-wise multi-threading, the overhead of testing was 0.166543 seconds.\n",
      "You can set `force_row_wise=true` to remove the overhead.\n",
      "And if memory is not enough, you can set `force_col_wise=true`.\n",
      "[LightGBM] [Info] Total Bins 7249\n",
      "[LightGBM] [Info] Number of data points in the train set: 8198890, number of used features: 48\n",
      "[LightGBM] [Info] [binary:BoostFromScore]: pavg=0.069787 -> initscore=-2.589971\n",
      "[LightGBM] [Info] Start training from score -2.589971\n",
      "Training until validation scores don't improve for 100 rounds\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "feature_fraction, val_score: 0.079990:  29%|##8       | 2/7 [05:07<12:36, 151.28s/it]\u001b[32m[I 2022-06-01 01:59:24,589]\u001b[0m Trial 1 finished with value: 0.08004495900438548 and parameters: {'feature_fraction': 0.4}. Best is trial 0 with value: 0.07999015131465316.\u001b[0m\n",
      "feature_fraction, val_score: 0.079990:  29%|##8       | 2/7 [05:07<12:36, 151.28s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Early stopping, best iteration is:\n",
      "[673]\tvalid_0's binary_logloss: 0.0779854\tvalid_1's binary_logloss: 0.080045\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/kaggler/.local/lib/python3.8/site-packages/lightgbm/engine.py:181: UserWarning: 'early_stopping_rounds' argument is deprecated and will be removed in a future release of LightGBM. Pass 'early_stopping()' callback via 'callbacks' argument instead.\n",
      "  _log_warning(\"'early_stopping_rounds' argument is deprecated and will be removed in a future release of LightGBM. \"\n",
      "/home/kaggler/.local/lib/python3.8/site-packages/lightgbm/engine.py:239: UserWarning: 'verbose_eval' argument is deprecated and will be removed in a future release of LightGBM. Pass 'log_evaluation()' callback via 'callbacks' argument instead.\n",
      "  _log_warning(\"'verbose_eval' argument is deprecated and will be removed in a future release of LightGBM. \"\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[LightGBM] [Info] Number of positive: 572173, number of negative: 7626717\n",
      "[LightGBM] [Warning] Auto-choosing row-wise multi-threading, the overhead of testing was 0.230015 seconds.\n",
      "You can set `force_row_wise=true` to remove the overhead.\n",
      "And if memory is not enough, you can set `force_col_wise=true`.\n",
      "[LightGBM] [Info] Total Bins 7249\n",
      "[LightGBM] [Info] Number of data points in the train set: 8198890, number of used features: 48\n",
      "[LightGBM] [Info] [binary:BoostFromScore]: pavg=0.069787 -> initscore=-2.589971\n",
      "[LightGBM] [Info] Start training from score -2.589971\n",
      "Training until validation scores don't improve for 100 rounds\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "feature_fraction, val_score: 0.079990:  43%|####2     | 3/7 [07:23<09:36, 144.15s/it]\u001b[32m[I 2022-06-01 02:01:40,251]\u001b[0m Trial 2 finished with value: 0.08019043153406225 and parameters: {'feature_fraction': 0.8999999999999999}. Best is trial 0 with value: 0.07999015131465316.\u001b[0m\n",
      "feature_fraction, val_score: 0.079990:  43%|####2     | 3/7 [07:23<09:36, 144.15s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Early stopping, best iteration is:\n",
      "[530]\tvalid_0's binary_logloss: 0.0785431\tvalid_1's binary_logloss: 0.0801904\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/kaggler/.local/lib/python3.8/site-packages/lightgbm/engine.py:181: UserWarning: 'early_stopping_rounds' argument is deprecated and will be removed in a future release of LightGBM. Pass 'early_stopping()' callback via 'callbacks' argument instead.\n",
      "  _log_warning(\"'early_stopping_rounds' argument is deprecated and will be removed in a future release of LightGBM. \"\n",
      "/home/kaggler/.local/lib/python3.8/site-packages/lightgbm/engine.py:239: UserWarning: 'verbose_eval' argument is deprecated and will be removed in a future release of LightGBM. Pass 'log_evaluation()' callback via 'callbacks' argument instead.\n",
      "  _log_warning(\"'verbose_eval' argument is deprecated and will be removed in a future release of LightGBM. \"\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[LightGBM] [Info] Number of positive: 572173, number of negative: 7626717\n",
      "[LightGBM] [Warning] Auto-choosing row-wise multi-threading, the overhead of testing was 0.257713 seconds.\n",
      "You can set `force_row_wise=true` to remove the overhead.\n",
      "And if memory is not enough, you can set `force_col_wise=true`.\n",
      "[LightGBM] [Info] Total Bins 7249\n",
      "[LightGBM] [Info] Number of data points in the train set: 8198890, number of used features: 48\n",
      "[LightGBM] [Info] [binary:BoostFromScore]: pavg=0.069787 -> initscore=-2.589971\n",
      "[LightGBM] [Info] Start training from score -2.589971\n",
      "Training until validation scores don't improve for 100 rounds\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "feature_fraction, val_score: 0.079855:  57%|#####7    | 4/7 [10:31<08:04, 161.56s/it]\u001b[32m[I 2022-06-01 02:04:48,509]\u001b[0m Trial 3 finished with value: 0.07985478000283382 and parameters: {'feature_fraction': 1.0}. Best is trial 3 with value: 0.07985478000283382.\u001b[0m\n",
      "feature_fraction, val_score: 0.079855:  57%|#####7    | 4/7 [10:31<08:04, 161.56s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Early stopping, best iteration is:\n",
      "[834]\tvalid_0's binary_logloss: 0.076369\tvalid_1's binary_logloss: 0.0798548\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/kaggler/.local/lib/python3.8/site-packages/lightgbm/engine.py:181: UserWarning: 'early_stopping_rounds' argument is deprecated and will be removed in a future release of LightGBM. Pass 'early_stopping()' callback via 'callbacks' argument instead.\n",
      "  _log_warning(\"'early_stopping_rounds' argument is deprecated and will be removed in a future release of LightGBM. \"\n",
      "/home/kaggler/.local/lib/python3.8/site-packages/lightgbm/engine.py:239: UserWarning: 'verbose_eval' argument is deprecated and will be removed in a future release of LightGBM. Pass 'log_evaluation()' callback via 'callbacks' argument instead.\n",
      "  _log_warning(\"'verbose_eval' argument is deprecated and will be removed in a future release of LightGBM. \"\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[LightGBM] [Info] Number of positive: 572173, number of negative: 7626717\n",
      "[LightGBM] [Warning] Auto-choosing row-wise multi-threading, the overhead of testing was 0.230283 seconds.\n",
      "You can set `force_row_wise=true` to remove the overhead.\n",
      "And if memory is not enough, you can set `force_col_wise=true`.\n",
      "[LightGBM] [Info] Total Bins 7249\n",
      "[LightGBM] [Info] Number of data points in the train set: 8198890, number of used features: 48\n",
      "[LightGBM] [Info] [binary:BoostFromScore]: pavg=0.069787 -> initscore=-2.589971\n",
      "[LightGBM] [Info] Start training from score -2.589971\n",
      "Training until validation scores don't improve for 100 rounds\n",
      "[1000]\tvalid_0's binary_logloss: 0.0752972\tvalid_1's binary_logloss: 0.0791731\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "feature_fraction, val_score: 0.079038:  71%|#######1  | 5/7 [14:06<06:01, 180.90s/it]\u001b[32m[I 2022-06-01 02:08:23,698]\u001b[0m Trial 4 finished with value: 0.07903832996647378 and parameters: {'feature_fraction': 0.8}. Best is trial 4 with value: 0.07903832996647378.\u001b[0m\n",
      "feature_fraction, val_score: 0.079038:  71%|#######1  | 5/7 [14:06<06:01, 180.90s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Early stopping, best iteration is:\n",
      "[979]\tvalid_0's binary_logloss: 0.0753946\tvalid_1's binary_logloss: 0.0790383\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/kaggler/.local/lib/python3.8/site-packages/lightgbm/engine.py:181: UserWarning: 'early_stopping_rounds' argument is deprecated and will be removed in a future release of LightGBM. Pass 'early_stopping()' callback via 'callbacks' argument instead.\n",
      "  _log_warning(\"'early_stopping_rounds' argument is deprecated and will be removed in a future release of LightGBM. \"\n",
      "/home/kaggler/.local/lib/python3.8/site-packages/lightgbm/engine.py:239: UserWarning: 'verbose_eval' argument is deprecated and will be removed in a future release of LightGBM. Pass 'log_evaluation()' callback via 'callbacks' argument instead.\n",
      "  _log_warning(\"'verbose_eval' argument is deprecated and will be removed in a future release of LightGBM. \"\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[LightGBM] [Info] Number of positive: 572173, number of negative: 7626717\n",
      "[LightGBM] [Warning] Auto-choosing row-wise multi-threading, the overhead of testing was 0.398193 seconds.\n",
      "You can set `force_row_wise=true` to remove the overhead.\n",
      "And if memory is not enough, you can set `force_col_wise=true`.\n",
      "[LightGBM] [Info] Total Bins 7249\n",
      "[LightGBM] [Info] Number of data points in the train set: 8198890, number of used features: 48\n",
      "[LightGBM] [Info] [binary:BoostFromScore]: pavg=0.069787 -> initscore=-2.589971\n",
      "[LightGBM] [Info] Start training from score -2.589971\n",
      "Training until validation scores don't improve for 100 rounds\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "feature_fraction, val_score: 0.079038:  86%|########5 | 6/7 [16:37<02:50, 170.64s/it]\u001b[32m[I 2022-06-01 02:10:54,426]\u001b[0m Trial 5 finished with value: 0.08084234561809768 and parameters: {'feature_fraction': 0.6}. Best is trial 4 with value: 0.07903832996647378.\u001b[0m\n",
      "feature_fraction, val_score: 0.079038:  86%|########5 | 6/7 [16:37<02:50, 170.64s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Early stopping, best iteration is:\n",
      "[618]\tvalid_0's binary_logloss: 0.078274\tvalid_1's binary_logloss: 0.0808423\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/kaggler/.local/lib/python3.8/site-packages/lightgbm/engine.py:181: UserWarning: 'early_stopping_rounds' argument is deprecated and will be removed in a future release of LightGBM. Pass 'early_stopping()' callback via 'callbacks' argument instead.\n",
      "  _log_warning(\"'early_stopping_rounds' argument is deprecated and will be removed in a future release of LightGBM. \"\n",
      "/home/kaggler/.local/lib/python3.8/site-packages/lightgbm/engine.py:239: UserWarning: 'verbose_eval' argument is deprecated and will be removed in a future release of LightGBM. Pass 'log_evaluation()' callback via 'callbacks' argument instead.\n",
      "  _log_warning(\"'verbose_eval' argument is deprecated and will be removed in a future release of LightGBM. \"\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[LightGBM] [Info] Number of positive: 572173, number of negative: 7626717\n",
      "[LightGBM] [Warning] Auto-choosing row-wise multi-threading, the overhead of testing was 0.340349 seconds.\n",
      "You can set `force_row_wise=true` to remove the overhead.\n",
      "And if memory is not enough, you can set `force_col_wise=true`.\n",
      "[LightGBM] [Info] Total Bins 7249\n",
      "[LightGBM] [Info] Number of data points in the train set: 8198890, number of used features: 48\n",
      "[LightGBM] [Info] [binary:BoostFromScore]: pavg=0.069787 -> initscore=-2.589971\n",
      "[LightGBM] [Info] Start training from score -2.589971\n",
      "Training until validation scores don't improve for 100 rounds\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "feature_fraction, val_score: 0.079038: 100%|##########| 7/7 [19:00<00:00, 161.74s/it]\u001b[32m[I 2022-06-01 02:13:17,851]\u001b[0m Trial 6 finished with value: 0.07988328197092064 and parameters: {'feature_fraction': 0.5}. Best is trial 4 with value: 0.07903832996647378.\u001b[0m\n",
      "feature_fraction, val_score: 0.079038: 100%|##########| 7/7 [19:00<00:00, 162.96s/it]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Early stopping, best iteration is:\n",
      "[634]\tvalid_0's binary_logloss: 0.0780421\tvalid_1's binary_logloss: 0.0798833\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "num_leaves, val_score: 0.079038:   0%|          | 0/20 [00:00<?, ?it/s]/home/kaggler/.local/lib/python3.8/site-packages/lightgbm/engine.py:181: UserWarning: 'early_stopping_rounds' argument is deprecated and will be removed in a future release of LightGBM. Pass 'early_stopping()' callback via 'callbacks' argument instead.\n",
      "  _log_warning(\"'early_stopping_rounds' argument is deprecated and will be removed in a future release of LightGBM. \"\n",
      "/home/kaggler/.local/lib/python3.8/site-packages/lightgbm/engine.py:239: UserWarning: 'verbose_eval' argument is deprecated and will be removed in a future release of LightGBM. Pass 'log_evaluation()' callback via 'callbacks' argument instead.\n",
      "  _log_warning(\"'verbose_eval' argument is deprecated and will be removed in a future release of LightGBM. \"\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[LightGBM] [Info] Number of positive: 572173, number of negative: 7626717\n",
      "[LightGBM] [Warning] Auto-choosing row-wise multi-threading, the overhead of testing was 0.237803 seconds.\n",
      "You can set `force_row_wise=true` to remove the overhead.\n",
      "And if memory is not enough, you can set `force_col_wise=true`.\n",
      "[LightGBM] [Info] Total Bins 7249\n",
      "[LightGBM] [Info] Number of data points in the train set: 8198890, number of used features: 48\n",
      "[LightGBM] [Info] [binary:BoostFromScore]: pavg=0.069787 -> initscore=-2.589971\n",
      "[LightGBM] [Info] Start training from score -2.589971\n",
      "Training until validation scores don't improve for 100 rounds\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "num_leaves, val_score: 0.078510:   5%|5         | 1/20 [01:30<28:32, 90.15s/it]\u001b[32m[I 2022-06-01 02:14:48,005]\u001b[0m Trial 7 finished with value: 0.07850970107652831 and parameters: {'num_leaves': 210}. Best is trial 7 with value: 0.07850970107652831.\u001b[0m\n",
      "num_leaves, val_score: 0.078510:   5%|5         | 1/20 [01:30<28:32, 90.15s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Early stopping, best iteration is:\n",
      "[131]\tvalid_0's binary_logloss: 0.0754863\tvalid_1's binary_logloss: 0.0785097\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/kaggler/.local/lib/python3.8/site-packages/lightgbm/engine.py:181: UserWarning: 'early_stopping_rounds' argument is deprecated and will be removed in a future release of LightGBM. Pass 'early_stopping()' callback via 'callbacks' argument instead.\n",
      "  _log_warning(\"'early_stopping_rounds' argument is deprecated and will be removed in a future release of LightGBM. \"\n",
      "/home/kaggler/.local/lib/python3.8/site-packages/lightgbm/engine.py:239: UserWarning: 'verbose_eval' argument is deprecated and will be removed in a future release of LightGBM. Pass 'log_evaluation()' callback via 'callbacks' argument instead.\n",
      "  _log_warning(\"'verbose_eval' argument is deprecated and will be removed in a future release of LightGBM. \"\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[LightGBM] [Info] Number of positive: 572173, number of negative: 7626717\n",
      "[LightGBM] [Warning] Auto-choosing row-wise multi-threading, the overhead of testing was 0.239035 seconds.\n",
      "You can set `force_row_wise=true` to remove the overhead.\n",
      "And if memory is not enough, you can set `force_col_wise=true`.\n",
      "[LightGBM] [Info] Total Bins 7249\n",
      "[LightGBM] [Info] Number of data points in the train set: 8198890, number of used features: 48\n",
      "[LightGBM] [Info] [binary:BoostFromScore]: pavg=0.069787 -> initscore=-2.589971\n",
      "[LightGBM] [Info] Start training from score -2.589971\n",
      "Training until validation scores don't improve for 100 rounds\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "num_leaves, val_score: 0.078510:  10%|#         | 2/20 [02:57<26:31, 88.43s/it]\u001b[32m[I 2022-06-01 02:16:15,232]\u001b[0m Trial 8 finished with value: 0.07982558828791084 and parameters: {'num_leaves': 112}. Best is trial 7 with value: 0.07850970107652831.\u001b[0m\n",
      "num_leaves, val_score: 0.078510:  10%|#         | 2/20 [02:57<26:31, 88.43s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Early stopping, best iteration is:\n",
      "[156]\tvalid_0's binary_logloss: 0.0780586\tvalid_1's binary_logloss: 0.0798256\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/kaggler/.local/lib/python3.8/site-packages/lightgbm/engine.py:181: UserWarning: 'early_stopping_rounds' argument is deprecated and will be removed in a future release of LightGBM. Pass 'early_stopping()' callback via 'callbacks' argument instead.\n",
      "  _log_warning(\"'early_stopping_rounds' argument is deprecated and will be removed in a future release of LightGBM. \"\n",
      "/home/kaggler/.local/lib/python3.8/site-packages/lightgbm/engine.py:239: UserWarning: 'verbose_eval' argument is deprecated and will be removed in a future release of LightGBM. Pass 'log_evaluation()' callback via 'callbacks' argument instead.\n",
      "  _log_warning(\"'verbose_eval' argument is deprecated and will be removed in a future release of LightGBM. \"\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[LightGBM] [Info] Number of positive: 572173, number of negative: 7626717\n",
      "[LightGBM] [Warning] Auto-choosing row-wise multi-threading, the overhead of testing was 0.248807 seconds.\n",
      "You can set `force_row_wise=true` to remove the overhead.\n",
      "And if memory is not enough, you can set `force_col_wise=true`.\n",
      "[LightGBM] [Info] Total Bins 7249\n",
      "[LightGBM] [Info] Number of data points in the train set: 8198890, number of used features: 48\n",
      "[LightGBM] [Info] [binary:BoostFromScore]: pavg=0.069787 -> initscore=-2.589971\n",
      "[LightGBM] [Info] Start training from score -2.589971\n",
      "Training until validation scores don't improve for 100 rounds\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "num_leaves, val_score: 0.078510:  15%|#5        | 3/20 [05:30<33:26, 118.05s/it]\u001b[32m[I 2022-06-01 02:18:48,524]\u001b[0m Trial 9 finished with value: 0.08105633921835112 and parameters: {'num_leaves': 33}. Best is trial 7 with value: 0.07850970107652831.\u001b[0m\n",
      "num_leaves, val_score: 0.078510:  15%|#5        | 3/20 [05:30<33:26, 118.05s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Early stopping, best iteration is:\n",
      "[623]\tvalid_0's binary_logloss: 0.0778791\tvalid_1's binary_logloss: 0.0810563\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/kaggler/.local/lib/python3.8/site-packages/lightgbm/engine.py:181: UserWarning: 'early_stopping_rounds' argument is deprecated and will be removed in a future release of LightGBM. Pass 'early_stopping()' callback via 'callbacks' argument instead.\n",
      "  _log_warning(\"'early_stopping_rounds' argument is deprecated and will be removed in a future release of LightGBM. \"\n",
      "/home/kaggler/.local/lib/python3.8/site-packages/lightgbm/engine.py:239: UserWarning: 'verbose_eval' argument is deprecated and will be removed in a future release of LightGBM. Pass 'log_evaluation()' callback via 'callbacks' argument instead.\n",
      "  _log_warning(\"'verbose_eval' argument is deprecated and will be removed in a future release of LightGBM. \"\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[LightGBM] [Info] Number of positive: 572173, number of negative: 7626717\n",
      "[LightGBM] [Warning] Auto-choosing row-wise multi-threading, the overhead of testing was 0.231224 seconds.\n",
      "You can set `force_row_wise=true` to remove the overhead.\n",
      "And if memory is not enough, you can set `force_col_wise=true`.\n",
      "[LightGBM] [Info] Total Bins 7249\n",
      "[LightGBM] [Info] Number of data points in the train set: 8198890, number of used features: 48\n",
      "[LightGBM] [Info] [binary:BoostFromScore]: pavg=0.069787 -> initscore=-2.589971\n",
      "[LightGBM] [Info] Start training from score -2.589971\n",
      "Training until validation scores don't improve for 100 rounds\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "num_leaves, val_score: 0.078510:  20%|##        | 4/20 [06:57<28:09, 105.57s/it]\u001b[32m[I 2022-06-01 02:20:14,963]\u001b[0m Trial 10 finished with value: 0.07967864985089254 and parameters: {'num_leaves': 132}. Best is trial 7 with value: 0.07850970107652831.\u001b[0m\n",
      "num_leaves, val_score: 0.078510:  20%|##        | 4/20 [06:57<28:09, 105.57s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Early stopping, best iteration is:\n",
      "[141]\tvalid_0's binary_logloss: 0.0777719\tvalid_1's binary_logloss: 0.0796786\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/kaggler/.local/lib/python3.8/site-packages/lightgbm/engine.py:181: UserWarning: 'early_stopping_rounds' argument is deprecated and will be removed in a future release of LightGBM. Pass 'early_stopping()' callback via 'callbacks' argument instead.\n",
      "  _log_warning(\"'early_stopping_rounds' argument is deprecated and will be removed in a future release of LightGBM. \"\n",
      "/home/kaggler/.local/lib/python3.8/site-packages/lightgbm/engine.py:239: UserWarning: 'verbose_eval' argument is deprecated and will be removed in a future release of LightGBM. Pass 'log_evaluation()' callback via 'callbacks' argument instead.\n",
      "  _log_warning(\"'verbose_eval' argument is deprecated and will be removed in a future release of LightGBM. \"\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[LightGBM] [Info] Number of positive: 572173, number of negative: 7626717\n",
      "[LightGBM] [Warning] Auto-choosing row-wise multi-threading, the overhead of testing was 0.237403 seconds.\n",
      "You can set `force_row_wise=true` to remove the overhead.\n",
      "And if memory is not enough, you can set `force_col_wise=true`.\n",
      "[LightGBM] [Info] Total Bins 7249\n",
      "[LightGBM] [Info] Number of data points in the train set: 8198890, number of used features: 48\n",
      "[LightGBM] [Info] [binary:BoostFromScore]: pavg=0.069787 -> initscore=-2.589971\n",
      "[LightGBM] [Info] Start training from score -2.589971\n",
      "Training until validation scores don't improve for 100 rounds\n",
      "[1000]\tvalid_0's binary_logloss: 0.0762252\tvalid_1's binary_logloss: 0.079419\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "num_leaves, val_score: 0.078510:  25%|##5       | 5/20 [10:16<34:50, 139.35s/it]\u001b[32m[I 2022-06-01 02:23:34,207]\u001b[0m Trial 11 finished with value: 0.07919237994751338 and parameters: {'num_leaves': 27}. Best is trial 7 with value: 0.07850970107652831.\u001b[0m\n",
      "num_leaves, val_score: 0.078510:  25%|##5       | 5/20 [10:16<34:50, 139.35s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Early stopping, best iteration is:\n",
      "[922]\tvalid_0's binary_logloss: 0.0765572\tvalid_1's binary_logloss: 0.0791924\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/kaggler/.local/lib/python3.8/site-packages/lightgbm/engine.py:181: UserWarning: 'early_stopping_rounds' argument is deprecated and will be removed in a future release of LightGBM. Pass 'early_stopping()' callback via 'callbacks' argument instead.\n",
      "  _log_warning(\"'early_stopping_rounds' argument is deprecated and will be removed in a future release of LightGBM. \"\n",
      "/home/kaggler/.local/lib/python3.8/site-packages/lightgbm/engine.py:239: UserWarning: 'verbose_eval' argument is deprecated and will be removed in a future release of LightGBM. Pass 'log_evaluation()' callback via 'callbacks' argument instead.\n",
      "  _log_warning(\"'verbose_eval' argument is deprecated and will be removed in a future release of LightGBM. \"\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[LightGBM] [Info] Number of positive: 572173, number of negative: 7626717\n",
      "[LightGBM] [Warning] Auto-choosing row-wise multi-threading, the overhead of testing was 0.256330 seconds.\n",
      "You can set `force_row_wise=true` to remove the overhead.\n",
      "And if memory is not enough, you can set `force_col_wise=true`.\n",
      "[LightGBM] [Info] Total Bins 7249\n",
      "[LightGBM] [Info] Number of data points in the train set: 8198890, number of used features: 48\n",
      "[LightGBM] [Info] [binary:BoostFromScore]: pavg=0.069787 -> initscore=-2.589971\n",
      "[LightGBM] [Info] Start training from score -2.589971\n",
      "Training until validation scores don't improve for 100 rounds\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "num_leaves, val_score: 0.078510:  30%|###       | 6/20 [11:41<28:10, 120.78s/it]\u001b[32m[I 2022-06-01 02:24:58,942]\u001b[0m Trial 12 finished with value: 0.08018735994688159 and parameters: {'num_leaves': 102}. Best is trial 7 with value: 0.07850970107652831.\u001b[0m\n",
      "num_leaves, val_score: 0.078510:  30%|###       | 6/20 [11:41<28:10, 120.78s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Early stopping, best iteration is:\n",
      "[153]\tvalid_0's binary_logloss: 0.0786505\tvalid_1's binary_logloss: 0.0801874\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/kaggler/.local/lib/python3.8/site-packages/lightgbm/engine.py:181: UserWarning: 'early_stopping_rounds' argument is deprecated and will be removed in a future release of LightGBM. Pass 'early_stopping()' callback via 'callbacks' argument instead.\n",
      "  _log_warning(\"'early_stopping_rounds' argument is deprecated and will be removed in a future release of LightGBM. \"\n",
      "/home/kaggler/.local/lib/python3.8/site-packages/lightgbm/engine.py:239: UserWarning: 'verbose_eval' argument is deprecated and will be removed in a future release of LightGBM. Pass 'log_evaluation()' callback via 'callbacks' argument instead.\n",
      "  _log_warning(\"'verbose_eval' argument is deprecated and will be removed in a future release of LightGBM. \"\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[LightGBM] [Info] Number of positive: 572173, number of negative: 7626717\n",
      "[LightGBM] [Warning] Auto-choosing row-wise multi-threading, the overhead of testing was 0.229053 seconds.\n",
      "You can set `force_row_wise=true` to remove the overhead.\n",
      "And if memory is not enough, you can set `force_col_wise=true`.\n",
      "[LightGBM] [Info] Total Bins 7249\n",
      "[LightGBM] [Info] Number of data points in the train set: 8198890, number of used features: 48\n",
      "[LightGBM] [Info] [binary:BoostFromScore]: pavg=0.069787 -> initscore=-2.589971\n",
      "[LightGBM] [Info] Start training from score -2.589971\n",
      "Training until validation scores don't improve for 100 rounds\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "num_leaves, val_score: 0.078510:  35%|###5      | 7/20 [13:17<24:26, 112.82s/it]\u001b[32m[I 2022-06-01 02:26:35,387]\u001b[0m Trial 13 finished with value: 0.0791044102515654 and parameters: {'num_leaves': 117}. Best is trial 7 with value: 0.07850970107652831.\u001b[0m\n",
      "num_leaves, val_score: 0.078510:  35%|###5      | 7/20 [13:17<24:26, 112.82s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Early stopping, best iteration is:\n",
      "[195]\tvalid_0's binary_logloss: 0.0764606\tvalid_1's binary_logloss: 0.0791044\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/kaggler/.local/lib/python3.8/site-packages/lightgbm/engine.py:181: UserWarning: 'early_stopping_rounds' argument is deprecated and will be removed in a future release of LightGBM. Pass 'early_stopping()' callback via 'callbacks' argument instead.\n",
      "  _log_warning(\"'early_stopping_rounds' argument is deprecated and will be removed in a future release of LightGBM. \"\n",
      "/home/kaggler/.local/lib/python3.8/site-packages/lightgbm/engine.py:239: UserWarning: 'verbose_eval' argument is deprecated and will be removed in a future release of LightGBM. Pass 'log_evaluation()' callback via 'callbacks' argument instead.\n",
      "  _log_warning(\"'verbose_eval' argument is deprecated and will be removed in a future release of LightGBM. \"\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[LightGBM] [Info] Number of positive: 572173, number of negative: 7626717\n",
      "[LightGBM] [Warning] Auto-choosing row-wise multi-threading, the overhead of testing was 0.226386 seconds.\n",
      "You can set `force_row_wise=true` to remove the overhead.\n",
      "And if memory is not enough, you can set `force_col_wise=true`.\n",
      "[LightGBM] [Info] Total Bins 7249\n",
      "[LightGBM] [Info] Number of data points in the train set: 8198890, number of used features: 48\n",
      "[LightGBM] [Info] [binary:BoostFromScore]: pavg=0.069787 -> initscore=-2.589971\n",
      "[LightGBM] [Info] Start training from score -2.589971\n",
      "Training until validation scores don't improve for 100 rounds\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "num_leaves, val_score: 0.078510:  40%|####      | 8/20 [15:29<23:48, 119.01s/it]\u001b[32m[I 2022-06-01 02:28:47,640]\u001b[0m Trial 14 finished with value: 0.08139369401764747 and parameters: {'num_leaves': 23}. Best is trial 7 with value: 0.07850970107652831.\u001b[0m\n",
      "num_leaves, val_score: 0.078510:  40%|####      | 8/20 [15:29<23:48, 119.01s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Early stopping, best iteration is:\n",
      "[538]\tvalid_0's binary_logloss: 0.0802562\tvalid_1's binary_logloss: 0.0813937\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/kaggler/.local/lib/python3.8/site-packages/lightgbm/engine.py:181: UserWarning: 'early_stopping_rounds' argument is deprecated and will be removed in a future release of LightGBM. Pass 'early_stopping()' callback via 'callbacks' argument instead.\n",
      "  _log_warning(\"'early_stopping_rounds' argument is deprecated and will be removed in a future release of LightGBM. \"\n",
      "/home/kaggler/.local/lib/python3.8/site-packages/lightgbm/engine.py:239: UserWarning: 'verbose_eval' argument is deprecated and will be removed in a future release of LightGBM. Pass 'log_evaluation()' callback via 'callbacks' argument instead.\n",
      "  _log_warning(\"'verbose_eval' argument is deprecated and will be removed in a future release of LightGBM. \"\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[LightGBM] [Info] Number of positive: 572173, number of negative: 7626717\n",
      "[LightGBM] [Warning] Auto-choosing row-wise multi-threading, the overhead of testing was 0.233007 seconds.\n",
      "You can set `force_row_wise=true` to remove the overhead.\n",
      "And if memory is not enough, you can set `force_col_wise=true`.\n",
      "[LightGBM] [Info] Total Bins 7249\n",
      "[LightGBM] [Info] Number of data points in the train set: 8198890, number of used features: 48\n",
      "[LightGBM] [Info] [binary:BoostFromScore]: pavg=0.069787 -> initscore=-2.589971\n",
      "[LightGBM] [Info] Start training from score -2.589971\n",
      "Training until validation scores don't improve for 100 rounds\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "num_leaves, val_score: 0.078510:  45%|####5     | 9/20 [16:56<19:58, 108.94s/it]\u001b[32m[I 2022-06-01 02:30:14,443]\u001b[0m Trial 15 finished with value: 0.07900104405940096 and parameters: {'num_leaves': 166}. Best is trial 7 with value: 0.07850970107652831.\u001b[0m\n",
      "num_leaves, val_score: 0.078510:  45%|####5     | 9/20 [16:56<19:58, 108.94s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Early stopping, best iteration is:\n",
      "[137]\tvalid_0's binary_logloss: 0.0765598\tvalid_1's binary_logloss: 0.079001\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/kaggler/.local/lib/python3.8/site-packages/lightgbm/engine.py:181: UserWarning: 'early_stopping_rounds' argument is deprecated and will be removed in a future release of LightGBM. Pass 'early_stopping()' callback via 'callbacks' argument instead.\n",
      "  _log_warning(\"'early_stopping_rounds' argument is deprecated and will be removed in a future release of LightGBM. \"\n",
      "/home/kaggler/.local/lib/python3.8/site-packages/lightgbm/engine.py:239: UserWarning: 'verbose_eval' argument is deprecated and will be removed in a future release of LightGBM. Pass 'log_evaluation()' callback via 'callbacks' argument instead.\n",
      "  _log_warning(\"'verbose_eval' argument is deprecated and will be removed in a future release of LightGBM. \"\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[LightGBM] [Info] Number of positive: 572173, number of negative: 7626717\n",
      "[LightGBM] [Warning] Auto-choosing row-wise multi-threading, the overhead of testing was 0.226082 seconds.\n",
      "You can set `force_row_wise=true` to remove the overhead.\n",
      "And if memory is not enough, you can set `force_col_wise=true`.\n",
      "[LightGBM] [Info] Total Bins 7249\n",
      "[LightGBM] [Info] Number of data points in the train set: 8198890, number of used features: 48\n",
      "[LightGBM] [Info] [binary:BoostFromScore]: pavg=0.069787 -> initscore=-2.589971\n",
      "[LightGBM] [Info] Start training from score -2.589971\n",
      "Training until validation scores don't improve for 100 rounds\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "num_leaves, val_score: 0.078510:  50%|#####     | 10/20 [18:22<16:58, 101.86s/it]\u001b[32m[I 2022-06-01 02:31:40,432]\u001b[0m Trial 16 finished with value: 0.0789121192879198 and parameters: {'num_leaves': 191}. Best is trial 7 with value: 0.07850970107652831.\u001b[0m\n",
      "num_leaves, val_score: 0.078510:  50%|#####     | 10/20 [18:22<16:58, 101.86s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Early stopping, best iteration is:\n",
      "[118]\tvalid_0's binary_logloss: 0.0766035\tvalid_1's binary_logloss: 0.0789121\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/kaggler/.local/lib/python3.8/site-packages/lightgbm/engine.py:181: UserWarning: 'early_stopping_rounds' argument is deprecated and will be removed in a future release of LightGBM. Pass 'early_stopping()' callback via 'callbacks' argument instead.\n",
      "  _log_warning(\"'early_stopping_rounds' argument is deprecated and will be removed in a future release of LightGBM. \"\n",
      "/home/kaggler/.local/lib/python3.8/site-packages/lightgbm/engine.py:239: UserWarning: 'verbose_eval' argument is deprecated and will be removed in a future release of LightGBM. Pass 'log_evaluation()' callback via 'callbacks' argument instead.\n",
      "  _log_warning(\"'verbose_eval' argument is deprecated and will be removed in a future release of LightGBM. \"\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[LightGBM] [Info] Number of positive: 572173, number of negative: 7626717\n",
      "[LightGBM] [Warning] Auto-choosing row-wise multi-threading, the overhead of testing was 0.238029 seconds.\n",
      "You can set `force_row_wise=true` to remove the overhead.\n",
      "And if memory is not enough, you can set `force_col_wise=true`.\n",
      "[LightGBM] [Info] Total Bins 7249\n",
      "[LightGBM] [Info] Number of data points in the train set: 8198890, number of used features: 48\n",
      "[LightGBM] [Info] [binary:BoostFromScore]: pavg=0.069787 -> initscore=-2.589971\n",
      "[LightGBM] [Info] Start training from score -2.589971\n",
      "Training until validation scores don't improve for 100 rounds\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "num_leaves, val_score: 0.077975:  55%|#####5    | 11/20 [19:55<14:51, 99.01s/it] \u001b[32m[I 2022-06-01 02:33:12,982]\u001b[0m Trial 17 finished with value: 0.0779753123892394 and parameters: {'num_leaves': 239}. Best is trial 17 with value: 0.0779753123892394.\u001b[0m\n",
      "num_leaves, val_score: 0.077975:  55%|#####5    | 11/20 [19:55<14:51, 99.01s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Early stopping, best iteration is:\n",
      "[130]\tvalid_0's binary_logloss: 0.0745195\tvalid_1's binary_logloss: 0.0779753\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/kaggler/.local/lib/python3.8/site-packages/lightgbm/engine.py:181: UserWarning: 'early_stopping_rounds' argument is deprecated and will be removed in a future release of LightGBM. Pass 'early_stopping()' callback via 'callbacks' argument instead.\n",
      "  _log_warning(\"'early_stopping_rounds' argument is deprecated and will be removed in a future release of LightGBM. \"\n",
      "/home/kaggler/.local/lib/python3.8/site-packages/lightgbm/engine.py:239: UserWarning: 'verbose_eval' argument is deprecated and will be removed in a future release of LightGBM. Pass 'log_evaluation()' callback via 'callbacks' argument instead.\n",
      "  _log_warning(\"'verbose_eval' argument is deprecated and will be removed in a future release of LightGBM. \"\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[LightGBM] [Info] Number of positive: 572173, number of negative: 7626717\n",
      "[LightGBM] [Warning] Auto-choosing row-wise multi-threading, the overhead of testing was 0.260749 seconds.\n",
      "You can set `force_row_wise=true` to remove the overhead.\n",
      "And if memory is not enough, you can set `force_col_wise=true`.\n",
      "[LightGBM] [Info] Total Bins 7249\n",
      "[LightGBM] [Info] Number of data points in the train set: 8198890, number of used features: 48\n",
      "[LightGBM] [Info] [binary:BoostFromScore]: pavg=0.069787 -> initscore=-2.589971\n",
      "[LightGBM] [Info] Start training from score -2.589971\n",
      "Training until validation scores don't improve for 100 rounds\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "num_leaves, val_score: 0.077975:  60%|######    | 12/20 [21:26<12:52, 96.62s/it]\u001b[32m[I 2022-06-01 02:34:44,135]\u001b[0m Trial 18 finished with value: 0.07820288032002674 and parameters: {'num_leaves': 248}. Best is trial 17 with value: 0.0779753123892394.\u001b[0m\n",
      "num_leaves, val_score: 0.077975:  60%|######    | 12/20 [21:26<12:52, 96.62s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Early stopping, best iteration is:\n",
      "[117]\tvalid_0's binary_logloss: 0.0750996\tvalid_1's binary_logloss: 0.0782029\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/kaggler/.local/lib/python3.8/site-packages/lightgbm/engine.py:181: UserWarning: 'early_stopping_rounds' argument is deprecated and will be removed in a future release of LightGBM. Pass 'early_stopping()' callback via 'callbacks' argument instead.\n",
      "  _log_warning(\"'early_stopping_rounds' argument is deprecated and will be removed in a future release of LightGBM. \"\n",
      "/home/kaggler/.local/lib/python3.8/site-packages/lightgbm/engine.py:239: UserWarning: 'verbose_eval' argument is deprecated and will be removed in a future release of LightGBM. Pass 'log_evaluation()' callback via 'callbacks' argument instead.\n",
      "  _log_warning(\"'verbose_eval' argument is deprecated and will be removed in a future release of LightGBM. \"\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[LightGBM] [Info] Number of positive: 572173, number of negative: 7626717\n",
      "[LightGBM] [Warning] Auto-choosing row-wise multi-threading, the overhead of testing was 0.249337 seconds.\n",
      "You can set `force_row_wise=true` to remove the overhead.\n",
      "And if memory is not enough, you can set `force_col_wise=true`.\n",
      "[LightGBM] [Info] Total Bins 7249\n",
      "[LightGBM] [Info] Number of data points in the train set: 8198890, number of used features: 48\n",
      "[LightGBM] [Info] [binary:BoostFromScore]: pavg=0.069787 -> initscore=-2.589971\n",
      "[LightGBM] [Info] Start training from score -2.589971\n",
      "Training until validation scores don't improve for 100 rounds\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "num_leaves, val_score: 0.077975:  65%|######5   | 13/20 [22:57<11:03, 94.85s/it]\u001b[32m[I 2022-06-01 02:36:14,924]\u001b[0m Trial 19 finished with value: 0.0783939945839433 and parameters: {'num_leaves': 246}. Best is trial 17 with value: 0.0779753123892394.\u001b[0m\n",
      "num_leaves, val_score: 0.077975:  65%|######5   | 13/20 [22:57<11:03, 94.85s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Early stopping, best iteration is:\n",
      "[117]\tvalid_0's binary_logloss: 0.0751625\tvalid_1's binary_logloss: 0.078394\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/kaggler/.local/lib/python3.8/site-packages/lightgbm/engine.py:181: UserWarning: 'early_stopping_rounds' argument is deprecated and will be removed in a future release of LightGBM. Pass 'early_stopping()' callback via 'callbacks' argument instead.\n",
      "  _log_warning(\"'early_stopping_rounds' argument is deprecated and will be removed in a future release of LightGBM. \"\n",
      "/home/kaggler/.local/lib/python3.8/site-packages/lightgbm/engine.py:239: UserWarning: 'verbose_eval' argument is deprecated and will be removed in a future release of LightGBM. Pass 'log_evaluation()' callback via 'callbacks' argument instead.\n",
      "  _log_warning(\"'verbose_eval' argument is deprecated and will be removed in a future release of LightGBM. \"\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[LightGBM] [Info] Number of positive: 572173, number of negative: 7626717\n",
      "[LightGBM] [Warning] Auto-choosing row-wise multi-threading, the overhead of testing was 0.231967 seconds.\n",
      "You can set `force_row_wise=true` to remove the overhead.\n",
      "And if memory is not enough, you can set `force_col_wise=true`.\n",
      "[LightGBM] [Info] Total Bins 7249\n",
      "[LightGBM] [Info] Number of data points in the train set: 8198890, number of used features: 48\n",
      "[LightGBM] [Info] [binary:BoostFromScore]: pavg=0.069787 -> initscore=-2.589971\n",
      "[LightGBM] [Info] Start training from score -2.589971\n",
      "Training until validation scores don't improve for 100 rounds\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "num_leaves, val_score: 0.077975:  70%|#######   | 14/20 [24:28<09:22, 93.74s/it]\u001b[32m[I 2022-06-01 02:37:46,084]\u001b[0m Trial 20 finished with value: 0.07824214640963827 and parameters: {'num_leaves': 256}. Best is trial 17 with value: 0.0779753123892394.\u001b[0m\n",
      "num_leaves, val_score: 0.077975:  70%|#######   | 14/20 [24:28<09:22, 93.74s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Early stopping, best iteration is:\n",
      "[112]\tvalid_0's binary_logloss: 0.0751764\tvalid_1's binary_logloss: 0.0782421\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/kaggler/.local/lib/python3.8/site-packages/lightgbm/engine.py:181: UserWarning: 'early_stopping_rounds' argument is deprecated and will be removed in a future release of LightGBM. Pass 'early_stopping()' callback via 'callbacks' argument instead.\n",
      "  _log_warning(\"'early_stopping_rounds' argument is deprecated and will be removed in a future release of LightGBM. \"\n",
      "/home/kaggler/.local/lib/python3.8/site-packages/lightgbm/engine.py:239: UserWarning: 'verbose_eval' argument is deprecated and will be removed in a future release of LightGBM. Pass 'log_evaluation()' callback via 'callbacks' argument instead.\n",
      "  _log_warning(\"'verbose_eval' argument is deprecated and will be removed in a future release of LightGBM. \"\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[LightGBM] [Info] Number of positive: 572173, number of negative: 7626717\n",
      "[LightGBM] [Warning] Auto-choosing row-wise multi-threading, the overhead of testing was 0.232515 seconds.\n",
      "You can set `force_row_wise=true` to remove the overhead.\n",
      "And if memory is not enough, you can set `force_col_wise=true`.\n",
      "[LightGBM] [Info] Total Bins 7249\n",
      "[LightGBM] [Info] Number of data points in the train set: 8198890, number of used features: 48\n",
      "[LightGBM] [Info] [binary:BoostFromScore]: pavg=0.069787 -> initscore=-2.589971\n",
      "[LightGBM] [Info] Start training from score -2.589971\n",
      "Training until validation scores don't improve for 100 rounds\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "num_leaves, val_score: 0.077975:  75%|#######5  | 15/20 [25:58<07:43, 92.69s/it]\u001b[32m[I 2022-06-01 02:39:16,345]\u001b[0m Trial 21 finished with value: 0.07851645599376714 and parameters: {'num_leaves': 221}. Best is trial 17 with value: 0.0779753123892394.\u001b[0m\n",
      "num_leaves, val_score: 0.077975:  75%|#######5  | 15/20 [25:58<07:43, 92.69s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Early stopping, best iteration is:\n",
      "[127]\tvalid_0's binary_logloss: 0.0754268\tvalid_1's binary_logloss: 0.0785165\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/kaggler/.local/lib/python3.8/site-packages/lightgbm/engine.py:181: UserWarning: 'early_stopping_rounds' argument is deprecated and will be removed in a future release of LightGBM. Pass 'early_stopping()' callback via 'callbacks' argument instead.\n",
      "  _log_warning(\"'early_stopping_rounds' argument is deprecated and will be removed in a future release of LightGBM. \"\n",
      "/home/kaggler/.local/lib/python3.8/site-packages/lightgbm/engine.py:239: UserWarning: 'verbose_eval' argument is deprecated and will be removed in a future release of LightGBM. Pass 'log_evaluation()' callback via 'callbacks' argument instead.\n",
      "  _log_warning(\"'verbose_eval' argument is deprecated and will be removed in a future release of LightGBM. \"\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[LightGBM] [Info] Number of positive: 572173, number of negative: 7626717\n",
      "[LightGBM] [Warning] Auto-choosing row-wise multi-threading, the overhead of testing was 0.230330 seconds.\n",
      "You can set `force_row_wise=true` to remove the overhead.\n",
      "And if memory is not enough, you can set `force_col_wise=true`.\n",
      "[LightGBM] [Info] Total Bins 7249\n",
      "[LightGBM] [Info] Number of data points in the train set: 8198890, number of used features: 48\n",
      "[LightGBM] [Info] [binary:BoostFromScore]: pavg=0.069787 -> initscore=-2.589971\n",
      "[LightGBM] [Info] Start training from score -2.589971\n",
      "Training until validation scores don't improve for 100 rounds\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "num_leaves, val_score: 0.077975:  80%|########  | 16/20 [27:22<06:00, 90.18s/it]\u001b[32m[I 2022-06-01 02:40:40,705]\u001b[0m Trial 22 finished with value: 0.07912494858767903 and parameters: {'num_leaves': 167}. Best is trial 17 with value: 0.0779753123892394.\u001b[0m\n",
      "num_leaves, val_score: 0.077975:  80%|########  | 16/20 [27:22<06:00, 90.18s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Early stopping, best iteration is:\n",
      "[127]\tvalid_0's binary_logloss: 0.0769304\tvalid_1's binary_logloss: 0.0791249\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/kaggler/.local/lib/python3.8/site-packages/lightgbm/engine.py:181: UserWarning: 'early_stopping_rounds' argument is deprecated and will be removed in a future release of LightGBM. Pass 'early_stopping()' callback via 'callbacks' argument instead.\n",
      "  _log_warning(\"'early_stopping_rounds' argument is deprecated and will be removed in a future release of LightGBM. \"\n",
      "/home/kaggler/.local/lib/python3.8/site-packages/lightgbm/engine.py:239: UserWarning: 'verbose_eval' argument is deprecated and will be removed in a future release of LightGBM. Pass 'log_evaluation()' callback via 'callbacks' argument instead.\n",
      "  _log_warning(\"'verbose_eval' argument is deprecated and will be removed in a future release of LightGBM. \"\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[LightGBM] [Info] Number of positive: 572173, number of negative: 7626717\n",
      "[LightGBM] [Warning] Auto-choosing row-wise multi-threading, the overhead of testing was 0.231223 seconds.\n",
      "You can set `force_row_wise=true` to remove the overhead.\n",
      "And if memory is not enough, you can set `force_col_wise=true`.\n",
      "[LightGBM] [Info] Total Bins 7249\n",
      "[LightGBM] [Info] Number of data points in the train set: 8198890, number of used features: 48\n",
      "[LightGBM] [Info] [binary:BoostFromScore]: pavg=0.069787 -> initscore=-2.589971\n",
      "[LightGBM] [Info] Start training from score -2.589971\n",
      "Training until validation scores don't improve for 100 rounds\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "num_leaves, val_score: 0.077975:  85%|########5 | 17/20 [28:53<04:30, 90.28s/it]\u001b[32m[I 2022-06-01 02:42:11,214]\u001b[0m Trial 23 finished with value: 0.07887504676371769 and parameters: {'num_leaves': 229}. Best is trial 17 with value: 0.0779753123892394.\u001b[0m\n",
      "num_leaves, val_score: 0.077975:  85%|########5 | 17/20 [28:53<04:30, 90.28s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Early stopping, best iteration is:\n",
      "[119]\tvalid_0's binary_logloss: 0.0758676\tvalid_1's binary_logloss: 0.078875\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/kaggler/.local/lib/python3.8/site-packages/lightgbm/engine.py:181: UserWarning: 'early_stopping_rounds' argument is deprecated and will be removed in a future release of LightGBM. Pass 'early_stopping()' callback via 'callbacks' argument instead.\n",
      "  _log_warning(\"'early_stopping_rounds' argument is deprecated and will be removed in a future release of LightGBM. \"\n",
      "/home/kaggler/.local/lib/python3.8/site-packages/lightgbm/engine.py:239: UserWarning: 'verbose_eval' argument is deprecated and will be removed in a future release of LightGBM. Pass 'log_evaluation()' callback via 'callbacks' argument instead.\n",
      "  _log_warning(\"'verbose_eval' argument is deprecated and will be removed in a future release of LightGBM. \"\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[LightGBM] [Info] Number of positive: 572173, number of negative: 7626717\n",
      "[LightGBM] [Warning] Auto-choosing row-wise multi-threading, the overhead of testing was 0.397692 seconds.\n",
      "You can set `force_row_wise=true` to remove the overhead.\n",
      "And if memory is not enough, you can set `force_col_wise=true`.\n",
      "[LightGBM] [Info] Total Bins 7249\n",
      "[LightGBM] [Info] Number of data points in the train set: 8198890, number of used features: 48\n",
      "[LightGBM] [Info] [binary:BoostFromScore]: pavg=0.069787 -> initscore=-2.589971\n",
      "[LightGBM] [Info] Start training from score -2.589971\n",
      "Training until validation scores don't improve for 100 rounds\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "num_leaves, val_score: 0.077975:  90%|######### | 18/20 [30:23<03:00, 90.31s/it]\u001b[32m[I 2022-06-01 02:43:41,585]\u001b[0m Trial 24 finished with value: 0.08034370171362785 and parameters: {'num_leaves': 72}. Best is trial 17 with value: 0.0779753123892394.\u001b[0m\n",
      "num_leaves, val_score: 0.077975:  90%|######### | 18/20 [30:23<03:00, 90.31s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Early stopping, best iteration is:\n",
      "[209]\tvalid_0's binary_logloss: 0.0788476\tvalid_1's binary_logloss: 0.0803437\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/kaggler/.local/lib/python3.8/site-packages/lightgbm/engine.py:181: UserWarning: 'early_stopping_rounds' argument is deprecated and will be removed in a future release of LightGBM. Pass 'early_stopping()' callback via 'callbacks' argument instead.\n",
      "  _log_warning(\"'early_stopping_rounds' argument is deprecated and will be removed in a future release of LightGBM. \"\n",
      "/home/kaggler/.local/lib/python3.8/site-packages/lightgbm/engine.py:239: UserWarning: 'verbose_eval' argument is deprecated and will be removed in a future release of LightGBM. Pass 'log_evaluation()' callback via 'callbacks' argument instead.\n",
      "  _log_warning(\"'verbose_eval' argument is deprecated and will be removed in a future release of LightGBM. \"\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[LightGBM] [Info] Number of positive: 572173, number of negative: 7626717\n",
      "[LightGBM] [Warning] Auto-choosing row-wise multi-threading, the overhead of testing was 0.227389 seconds.\n",
      "You can set `force_row_wise=true` to remove the overhead.\n",
      "And if memory is not enough, you can set `force_col_wise=true`.\n",
      "[LightGBM] [Info] Total Bins 7249\n",
      "[LightGBM] [Info] Number of data points in the train set: 8198890, number of used features: 48\n",
      "[LightGBM] [Info] [binary:BoostFromScore]: pavg=0.069787 -> initscore=-2.589971\n",
      "[LightGBM] [Info] Start training from score -2.589971\n",
      "Training until validation scores don't improve for 100 rounds\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "num_leaves, val_score: 0.077975:  95%|#########5| 19/20 [31:49<01:28, 88.86s/it]\u001b[32m[I 2022-06-01 02:45:07,064]\u001b[0m Trial 25 finished with value: 0.07902509622514188 and parameters: {'num_leaves': 183}. Best is trial 17 with value: 0.0779753123892394.\u001b[0m\n",
      "num_leaves, val_score: 0.077975:  95%|#########5| 19/20 [31:49<01:28, 88.86s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Early stopping, best iteration is:\n",
      "[122]\tvalid_0's binary_logloss: 0.0767318\tvalid_1's binary_logloss: 0.0790251\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/kaggler/.local/lib/python3.8/site-packages/lightgbm/engine.py:181: UserWarning: 'early_stopping_rounds' argument is deprecated and will be removed in a future release of LightGBM. Pass 'early_stopping()' callback via 'callbacks' argument instead.\n",
      "  _log_warning(\"'early_stopping_rounds' argument is deprecated and will be removed in a future release of LightGBM. \"\n",
      "/home/kaggler/.local/lib/python3.8/site-packages/lightgbm/engine.py:239: UserWarning: 'verbose_eval' argument is deprecated and will be removed in a future release of LightGBM. Pass 'log_evaluation()' callback via 'callbacks' argument instead.\n",
      "  _log_warning(\"'verbose_eval' argument is deprecated and will be removed in a future release of LightGBM. \"\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[LightGBM] [Info] Number of positive: 572173, number of negative: 7626717\n",
      "[LightGBM] [Warning] Auto-choosing row-wise multi-threading, the overhead of testing was 0.414664 seconds.\n",
      "You can set `force_row_wise=true` to remove the overhead.\n",
      "And if memory is not enough, you can set `force_col_wise=true`.\n",
      "[LightGBM] [Info] Total Bins 7249\n",
      "[LightGBM] [Info] Number of data points in the train set: 8198890, number of used features: 48\n",
      "[LightGBM] [Info] [binary:BoostFromScore]: pavg=0.069787 -> initscore=-2.589971\n",
      "[LightGBM] [Info] Start training from score -2.589971\n",
      "Training until validation scores don't improve for 100 rounds\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "num_leaves, val_score: 0.077975: 100%|##########| 20/20 [33:20<00:00, 89.62s/it]\u001b[32m[I 2022-06-01 02:46:38,450]\u001b[0m Trial 26 finished with value: 0.07892506548542931 and parameters: {'num_leaves': 199}. Best is trial 17 with value: 0.0779753123892394.\u001b[0m\n",
      "num_leaves, val_score: 0.077975: 100%|##########| 20/20 [33:20<00:00, 100.03s/it]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Early stopping, best iteration is:\n",
      "[129]\tvalid_0's binary_logloss: 0.075951\tvalid_1's binary_logloss: 0.0789251\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "bagging, val_score: 0.077975:   0%|          | 0/10 [00:00<?, ?it/s]/home/kaggler/.local/lib/python3.8/site-packages/lightgbm/engine.py:181: UserWarning: 'early_stopping_rounds' argument is deprecated and will be removed in a future release of LightGBM. Pass 'early_stopping()' callback via 'callbacks' argument instead.\n",
      "  _log_warning(\"'early_stopping_rounds' argument is deprecated and will be removed in a future release of LightGBM. \"\n",
      "/home/kaggler/.local/lib/python3.8/site-packages/lightgbm/engine.py:239: UserWarning: 'verbose_eval' argument is deprecated and will be removed in a future release of LightGBM. Pass 'log_evaluation()' callback via 'callbacks' argument instead.\n",
      "  _log_warning(\"'verbose_eval' argument is deprecated and will be removed in a future release of LightGBM. \"\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[LightGBM] [Info] Number of positive: 572173, number of negative: 7626717\n",
      "[LightGBM] [Warning] Auto-choosing row-wise multi-threading, the overhead of testing was 0.233721 seconds.\n",
      "You can set `force_row_wise=true` to remove the overhead.\n",
      "And if memory is not enough, you can set `force_col_wise=true`.\n",
      "[LightGBM] [Info] Total Bins 7249\n",
      "[LightGBM] [Info] Number of data points in the train set: 8198890, number of used features: 48\n",
      "[LightGBM] [Info] [binary:BoostFromScore]: pavg=0.069787 -> initscore=-2.589971\n",
      "[LightGBM] [Info] Start training from score -2.589971\n",
      "Training until validation scores don't improve for 100 rounds\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "bagging, val_score: 0.077975:  10%|#         | 1/10 [01:17<11:36, 77.42s/it]\u001b[32m[I 2022-06-01 02:47:55,877]\u001b[0m Trial 27 finished with value: 0.07895145201761751 and parameters: {'bagging_fraction': 0.6357963697569669, 'bagging_freq': 3}. Best is trial 27 with value: 0.07895145201761751.\u001b[0m\n",
      "bagging, val_score: 0.077975:  10%|#         | 1/10 [01:17<11:36, 77.42s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Early stopping, best iteration is:\n",
      "[111]\tvalid_0's binary_logloss: 0.0761033\tvalid_1's binary_logloss: 0.0789515\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/kaggler/.local/lib/python3.8/site-packages/lightgbm/engine.py:181: UserWarning: 'early_stopping_rounds' argument is deprecated and will be removed in a future release of LightGBM. Pass 'early_stopping()' callback via 'callbacks' argument instead.\n",
      "  _log_warning(\"'early_stopping_rounds' argument is deprecated and will be removed in a future release of LightGBM. \"\n",
      "/home/kaggler/.local/lib/python3.8/site-packages/lightgbm/engine.py:239: UserWarning: 'verbose_eval' argument is deprecated and will be removed in a future release of LightGBM. Pass 'log_evaluation()' callback via 'callbacks' argument instead.\n",
      "  _log_warning(\"'verbose_eval' argument is deprecated and will be removed in a future release of LightGBM. \"\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[LightGBM] [Info] Number of positive: 572173, number of negative: 7626717\n",
      "[LightGBM] [Warning] Auto-choosing row-wise multi-threading, the overhead of testing was 0.247609 seconds.\n",
      "You can set `force_row_wise=true` to remove the overhead.\n",
      "And if memory is not enough, you can set `force_col_wise=true`.\n",
      "[LightGBM] [Info] Total Bins 7249\n",
      "[LightGBM] [Info] Number of data points in the train set: 8198890, number of used features: 48\n",
      "[LightGBM] [Info] [binary:BoostFromScore]: pavg=0.069787 -> initscore=-2.589971\n",
      "[LightGBM] [Info] Start training from score -2.589971\n",
      "Training until validation scores don't improve for 100 rounds\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "bagging, val_score: 0.077975:  20%|##        | 2/10 [02:54<11:51, 88.88s/it]\u001b[32m[I 2022-06-01 02:49:32,774]\u001b[0m Trial 28 finished with value: 0.07832439726618703 and parameters: {'bagging_fraction': 0.9717233513586166, 'bagging_freq': 7}. Best is trial 28 with value: 0.07832439726618703.\u001b[0m\n",
      "bagging, val_score: 0.077975:  20%|##        | 2/10 [02:54<11:51, 88.88s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Early stopping, best iteration is:\n",
      "[124]\tvalid_0's binary_logloss: 0.0750729\tvalid_1's binary_logloss: 0.0783244\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/kaggler/.local/lib/python3.8/site-packages/lightgbm/engine.py:181: UserWarning: 'early_stopping_rounds' argument is deprecated and will be removed in a future release of LightGBM. Pass 'early_stopping()' callback via 'callbacks' argument instead.\n",
      "  _log_warning(\"'early_stopping_rounds' argument is deprecated and will be removed in a future release of LightGBM. \"\n",
      "/home/kaggler/.local/lib/python3.8/site-packages/lightgbm/engine.py:239: UserWarning: 'verbose_eval' argument is deprecated and will be removed in a future release of LightGBM. Pass 'log_evaluation()' callback via 'callbacks' argument instead.\n",
      "  _log_warning(\"'verbose_eval' argument is deprecated and will be removed in a future release of LightGBM. \"\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[LightGBM] [Info] Number of positive: 572173, number of negative: 7626717\n",
      "[LightGBM] [Warning] Auto-choosing row-wise multi-threading, the overhead of testing was 0.226876 seconds.\n",
      "You can set `force_row_wise=true` to remove the overhead.\n",
      "And if memory is not enough, you can set `force_col_wise=true`.\n",
      "[LightGBM] [Info] Total Bins 7249\n",
      "[LightGBM] [Info] Number of data points in the train set: 8198890, number of used features: 48\n",
      "[LightGBM] [Info] [binary:BoostFromScore]: pavg=0.069787 -> initscore=-2.589971\n",
      "[LightGBM] [Info] Start training from score -2.589971\n",
      "Training until validation scores don't improve for 100 rounds\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "bagging, val_score: 0.077975:  30%|###       | 3/10 [04:01<09:12, 78.89s/it]\u001b[32m[I 2022-06-01 02:50:39,772]\u001b[0m Trial 29 finished with value: 0.07982084156198846 and parameters: {'bagging_fraction': 0.4631904653715, 'bagging_freq': 4}. Best is trial 28 with value: 0.07832439726618703.\u001b[0m\n",
      "bagging, val_score: 0.077975:  30%|###       | 3/10 [04:01<09:12, 78.89s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Early stopping, best iteration is:\n",
      "[112]\tvalid_0's binary_logloss: 0.0765229\tvalid_1's binary_logloss: 0.0798208\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/kaggler/.local/lib/python3.8/site-packages/lightgbm/engine.py:181: UserWarning: 'early_stopping_rounds' argument is deprecated and will be removed in a future release of LightGBM. Pass 'early_stopping()' callback via 'callbacks' argument instead.\n",
      "  _log_warning(\"'early_stopping_rounds' argument is deprecated and will be removed in a future release of LightGBM. \"\n",
      "/home/kaggler/.local/lib/python3.8/site-packages/lightgbm/engine.py:239: UserWarning: 'verbose_eval' argument is deprecated and will be removed in a future release of LightGBM. Pass 'log_evaluation()' callback via 'callbacks' argument instead.\n",
      "  _log_warning(\"'verbose_eval' argument is deprecated and will be removed in a future release of LightGBM. \"\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[LightGBM] [Info] Number of positive: 572173, number of negative: 7626717\n",
      "[LightGBM] [Warning] Auto-choosing row-wise multi-threading, the overhead of testing was 0.239280 seconds.\n",
      "You can set `force_row_wise=true` to remove the overhead.\n",
      "And if memory is not enough, you can set `force_col_wise=true`.\n",
      "[LightGBM] [Info] Total Bins 7249\n",
      "[LightGBM] [Info] Number of data points in the train set: 8198890, number of used features: 48\n",
      "[LightGBM] [Info] [binary:BoostFromScore]: pavg=0.069787 -> initscore=-2.589971\n",
      "[LightGBM] [Info] Start training from score -2.589971\n",
      "Training until validation scores don't improve for 100 rounds\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "bagging, val_score: 0.077975:  40%|####      | 4/10 [05:17<07:46, 77.78s/it]\u001b[32m[I 2022-06-01 02:51:55,865]\u001b[0m Trial 30 finished with value: 0.07895758401730679 and parameters: {'bagging_fraction': 0.6483634452743462, 'bagging_freq': 1}. Best is trial 28 with value: 0.07832439726618703.\u001b[0m\n",
      "bagging, val_score: 0.077975:  40%|####      | 4/10 [05:17<07:46, 77.78s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Early stopping, best iteration is:\n",
      "[113]\tvalid_0's binary_logloss: 0.0757597\tvalid_1's binary_logloss: 0.0789576\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/kaggler/.local/lib/python3.8/site-packages/lightgbm/engine.py:181: UserWarning: 'early_stopping_rounds' argument is deprecated and will be removed in a future release of LightGBM. Pass 'early_stopping()' callback via 'callbacks' argument instead.\n",
      "  _log_warning(\"'early_stopping_rounds' argument is deprecated and will be removed in a future release of LightGBM. \"\n",
      "/home/kaggler/.local/lib/python3.8/site-packages/lightgbm/engine.py:239: UserWarning: 'verbose_eval' argument is deprecated and will be removed in a future release of LightGBM. Pass 'log_evaluation()' callback via 'callbacks' argument instead.\n",
      "  _log_warning(\"'verbose_eval' argument is deprecated and will be removed in a future release of LightGBM. \"\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[LightGBM] [Info] Number of positive: 572173, number of negative: 7626717\n",
      "[LightGBM] [Warning] Auto-choosing row-wise multi-threading, the overhead of testing was 0.237034 seconds.\n",
      "You can set `force_row_wise=true` to remove the overhead.\n",
      "And if memory is not enough, you can set `force_col_wise=true`.\n",
      "[LightGBM] [Info] Total Bins 7249\n",
      "[LightGBM] [Info] Number of data points in the train set: 8198890, number of used features: 48\n",
      "[LightGBM] [Info] [binary:BoostFromScore]: pavg=0.069787 -> initscore=-2.589971\n",
      "[LightGBM] [Info] Start training from score -2.589971\n",
      "Training until validation scores don't improve for 100 rounds\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "bagging, val_score: 0.077975:  50%|#####     | 5/10 [06:25<06:11, 74.35s/it]\u001b[32m[I 2022-06-01 02:53:04,123]\u001b[0m Trial 31 finished with value: 0.0796488574243093 and parameters: {'bagging_fraction': 0.440994204451166, 'bagging_freq': 3}. Best is trial 28 with value: 0.07832439726618703.\u001b[0m\n",
      "bagging, val_score: 0.077975:  50%|#####     | 5/10 [06:25<06:11, 74.35s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Early stopping, best iteration is:\n",
      "[114]\tvalid_0's binary_logloss: 0.0762932\tvalid_1's binary_logloss: 0.0796489\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/kaggler/.local/lib/python3.8/site-packages/lightgbm/engine.py:181: UserWarning: 'early_stopping_rounds' argument is deprecated and will be removed in a future release of LightGBM. Pass 'early_stopping()' callback via 'callbacks' argument instead.\n",
      "  _log_warning(\"'early_stopping_rounds' argument is deprecated and will be removed in a future release of LightGBM. \"\n",
      "/home/kaggler/.local/lib/python3.8/site-packages/lightgbm/engine.py:239: UserWarning: 'verbose_eval' argument is deprecated and will be removed in a future release of LightGBM. Pass 'log_evaluation()' callback via 'callbacks' argument instead.\n",
      "  _log_warning(\"'verbose_eval' argument is deprecated and will be removed in a future release of LightGBM. \"\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[LightGBM] [Info] Number of positive: 572173, number of negative: 7626717\n",
      "[LightGBM] [Warning] Auto-choosing row-wise multi-threading, the overhead of testing was 0.259755 seconds.\n",
      "You can set `force_row_wise=true` to remove the overhead.\n",
      "And if memory is not enough, you can set `force_col_wise=true`.\n",
      "[LightGBM] [Info] Total Bins 7249\n",
      "[LightGBM] [Info] Number of data points in the train set: 8198890, number of used features: 48\n",
      "[LightGBM] [Info] [binary:BoostFromScore]: pavg=0.069787 -> initscore=-2.589971\n",
      "[LightGBM] [Info] Start training from score -2.589971\n",
      "Training until validation scores don't improve for 100 rounds\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "bagging, val_score: 0.077975:  60%|######    | 6/10 [07:41<04:59, 74.79s/it]\u001b[32m[I 2022-06-01 02:54:19,771]\u001b[0m Trial 32 finished with value: 0.07909127435992014 and parameters: {'bagging_fraction': 0.582815923369737, 'bagging_freq': 3}. Best is trial 28 with value: 0.07832439726618703.\u001b[0m\n",
      "bagging, val_score: 0.077975:  60%|######    | 6/10 [07:41<04:59, 74.79s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Early stopping, best iteration is:\n",
      "[112]\tvalid_0's binary_logloss: 0.0760439\tvalid_1's binary_logloss: 0.0790913\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/kaggler/.local/lib/python3.8/site-packages/lightgbm/engine.py:181: UserWarning: 'early_stopping_rounds' argument is deprecated and will be removed in a future release of LightGBM. Pass 'early_stopping()' callback via 'callbacks' argument instead.\n",
      "  _log_warning(\"'early_stopping_rounds' argument is deprecated and will be removed in a future release of LightGBM. \"\n",
      "/home/kaggler/.local/lib/python3.8/site-packages/lightgbm/engine.py:239: UserWarning: 'verbose_eval' argument is deprecated and will be removed in a future release of LightGBM. Pass 'log_evaluation()' callback via 'callbacks' argument instead.\n",
      "  _log_warning(\"'verbose_eval' argument is deprecated and will be removed in a future release of LightGBM. \"\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[LightGBM] [Info] Number of positive: 572173, number of negative: 7626717\n",
      "[LightGBM] [Warning] Auto-choosing row-wise multi-threading, the overhead of testing was 0.225319 seconds.\n",
      "You can set `force_row_wise=true` to remove the overhead.\n",
      "And if memory is not enough, you can set `force_col_wise=true`.\n",
      "[LightGBM] [Info] Total Bins 7249\n",
      "[LightGBM] [Info] Number of data points in the train set: 8198890, number of used features: 48\n",
      "[LightGBM] [Info] [binary:BoostFromScore]: pavg=0.069787 -> initscore=-2.589971\n",
      "[LightGBM] [Info] Start training from score -2.589971\n",
      "Training until validation scores don't improve for 100 rounds\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "bagging, val_score: 0.077975:  70%|#######   | 7/10 [09:13<04:01, 80.57s/it]\u001b[32m[I 2022-06-01 02:55:52,257]\u001b[0m Trial 33 finished with value: 0.07823939664340222 and parameters: {'bagging_fraction': 0.9194013876838113, 'bagging_freq': 1}. Best is trial 33 with value: 0.07823939664340222.\u001b[0m\n",
      "bagging, val_score: 0.077975:  70%|#######   | 7/10 [09:13<04:01, 80.57s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Early stopping, best iteration is:\n",
      "[122]\tvalid_0's binary_logloss: 0.0749493\tvalid_1's binary_logloss: 0.0782394\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/kaggler/.local/lib/python3.8/site-packages/lightgbm/engine.py:181: UserWarning: 'early_stopping_rounds' argument is deprecated and will be removed in a future release of LightGBM. Pass 'early_stopping()' callback via 'callbacks' argument instead.\n",
      "  _log_warning(\"'early_stopping_rounds' argument is deprecated and will be removed in a future release of LightGBM. \"\n",
      "/home/kaggler/.local/lib/python3.8/site-packages/lightgbm/engine.py:239: UserWarning: 'verbose_eval' argument is deprecated and will be removed in a future release of LightGBM. Pass 'log_evaluation()' callback via 'callbacks' argument instead.\n",
      "  _log_warning(\"'verbose_eval' argument is deprecated and will be removed in a future release of LightGBM. \"\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[LightGBM] [Info] Number of positive: 572173, number of negative: 7626717\n",
      "[LightGBM] [Warning] Auto-choosing row-wise multi-threading, the overhead of testing was 0.248873 seconds.\n",
      "You can set `force_row_wise=true` to remove the overhead.\n",
      "And if memory is not enough, you can set `force_col_wise=true`.\n",
      "[LightGBM] [Info] Total Bins 7249\n",
      "[LightGBM] [Info] Number of data points in the train set: 8198890, number of used features: 48\n",
      "[LightGBM] [Info] [binary:BoostFromScore]: pavg=0.069787 -> initscore=-2.589971\n",
      "[LightGBM] [Info] Start training from score -2.589971\n",
      "Training until validation scores don't improve for 100 rounds\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "bagging, val_score: 0.077975:  80%|########  | 8/10 [10:34<02:41, 80.67s/it]\u001b[32m[I 2022-06-01 02:57:13,135]\u001b[0m Trial 34 finished with value: 0.07905557119254447 and parameters: {'bagging_fraction': 0.7696013669256677, 'bagging_freq': 7}. Best is trial 33 with value: 0.07823939664340222.\u001b[0m\n",
      "bagging, val_score: 0.077975:  80%|########  | 8/10 [10:34<02:41, 80.67s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Early stopping, best iteration is:\n",
      "[106]\tvalid_0's binary_logloss: 0.0763648\tvalid_1's binary_logloss: 0.0790556\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/kaggler/.local/lib/python3.8/site-packages/lightgbm/engine.py:181: UserWarning: 'early_stopping_rounds' argument is deprecated and will be removed in a future release of LightGBM. Pass 'early_stopping()' callback via 'callbacks' argument instead.\n",
      "  _log_warning(\"'early_stopping_rounds' argument is deprecated and will be removed in a future release of LightGBM. \"\n",
      "/home/kaggler/.local/lib/python3.8/site-packages/lightgbm/engine.py:239: UserWarning: 'verbose_eval' argument is deprecated and will be removed in a future release of LightGBM. Pass 'log_evaluation()' callback via 'callbacks' argument instead.\n",
      "  _log_warning(\"'verbose_eval' argument is deprecated and will be removed in a future release of LightGBM. \"\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[LightGBM] [Info] Number of positive: 572173, number of negative: 7626717\n",
      "[LightGBM] [Warning] Auto-choosing row-wise multi-threading, the overhead of testing was 0.225306 seconds.\n",
      "You can set `force_row_wise=true` to remove the overhead.\n",
      "And if memory is not enough, you can set `force_col_wise=true`.\n",
      "[LightGBM] [Info] Total Bins 7249\n",
      "[LightGBM] [Info] Number of data points in the train set: 8198890, number of used features: 48\n",
      "[LightGBM] [Info] [binary:BoostFromScore]: pavg=0.069787 -> initscore=-2.589971\n",
      "[LightGBM] [Info] Start training from score -2.589971\n",
      "Training until validation scores don't improve for 100 rounds\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "bagging, val_score: 0.077975:  90%|######### | 9/10 [11:40<01:15, 75.88s/it]\u001b[32m[I 2022-06-01 02:58:18,486]\u001b[0m Trial 35 finished with value: 0.07965004260081333 and parameters: {'bagging_fraction': 0.46695303248534026, 'bagging_freq': 6}. Best is trial 33 with value: 0.07823939664340222.\u001b[0m\n",
      "bagging, val_score: 0.077975:  90%|######### | 9/10 [11:40<01:15, 75.88s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Early stopping, best iteration is:\n",
      "[105]\tvalid_0's binary_logloss: 0.076734\tvalid_1's binary_logloss: 0.07965\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/kaggler/.local/lib/python3.8/site-packages/lightgbm/engine.py:181: UserWarning: 'early_stopping_rounds' argument is deprecated and will be removed in a future release of LightGBM. Pass 'early_stopping()' callback via 'callbacks' argument instead.\n",
      "  _log_warning(\"'early_stopping_rounds' argument is deprecated and will be removed in a future release of LightGBM. \"\n",
      "/home/kaggler/.local/lib/python3.8/site-packages/lightgbm/engine.py:239: UserWarning: 'verbose_eval' argument is deprecated and will be removed in a future release of LightGBM. Pass 'log_evaluation()' callback via 'callbacks' argument instead.\n",
      "  _log_warning(\"'verbose_eval' argument is deprecated and will be removed in a future release of LightGBM. \"\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[LightGBM] [Info] Number of positive: 572173, number of negative: 7626717\n",
      "[LightGBM] [Warning] Auto-choosing row-wise multi-threading, the overhead of testing was 0.252350 seconds.\n",
      "You can set `force_row_wise=true` to remove the overhead.\n",
      "And if memory is not enough, you can set `force_col_wise=true`.\n",
      "[LightGBM] [Info] Total Bins 7249\n",
      "[LightGBM] [Info] Number of data points in the train set: 8198890, number of used features: 48\n",
      "[LightGBM] [Info] [binary:BoostFromScore]: pavg=0.069787 -> initscore=-2.589971\n",
      "[LightGBM] [Info] Start training from score -2.589971\n",
      "Training until validation scores don't improve for 100 rounds\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "bagging, val_score: 0.077975: 100%|##########| 10/10 [12:43<00:00, 72.00s/it]\u001b[32m[I 2022-06-01 02:59:21,789]\u001b[0m Trial 36 finished with value: 0.0802383756597087 and parameters: {'bagging_fraction': 0.42778392493300404, 'bagging_freq': 2}. Best is trial 33 with value: 0.07823939664340222.\u001b[0m\n",
      "bagging, val_score: 0.077975: 100%|##########| 10/10 [12:43<00:00, 76.33s/it]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Early stopping, best iteration is:\n",
      "[94]\tvalid_0's binary_logloss: 0.0776388\tvalid_1's binary_logloss: 0.0802384\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "feature_fraction_stage2, val_score: 0.077975:   0%|          | 0/6 [00:00<?, ?it/s]/home/kaggler/.local/lib/python3.8/site-packages/lightgbm/engine.py:181: UserWarning: 'early_stopping_rounds' argument is deprecated and will be removed in a future release of LightGBM. Pass 'early_stopping()' callback via 'callbacks' argument instead.\n",
      "  _log_warning(\"'early_stopping_rounds' argument is deprecated and will be removed in a future release of LightGBM. \"\n",
      "/home/kaggler/.local/lib/python3.8/site-packages/lightgbm/engine.py:239: UserWarning: 'verbose_eval' argument is deprecated and will be removed in a future release of LightGBM. Pass 'log_evaluation()' callback via 'callbacks' argument instead.\n",
      "  _log_warning(\"'verbose_eval' argument is deprecated and will be removed in a future release of LightGBM. \"\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[LightGBM] [Info] Number of positive: 572173, number of negative: 7626717\n",
      "[LightGBM] [Warning] Auto-choosing row-wise multi-threading, the overhead of testing was 0.240219 seconds.\n",
      "You can set `force_row_wise=true` to remove the overhead.\n",
      "And if memory is not enough, you can set `force_col_wise=true`.\n",
      "[LightGBM] [Info] Total Bins 7249\n",
      "[LightGBM] [Info] Number of data points in the train set: 8198890, number of used features: 48\n",
      "[LightGBM] [Info] [binary:BoostFromScore]: pavg=0.069787 -> initscore=-2.589971\n",
      "[LightGBM] [Info] Start training from score -2.589971\n",
      "Training until validation scores don't improve for 100 rounds\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "feature_fraction_stage2, val_score: 0.077975:  17%|#6        | 1/6 [01:31<07:36, 91.31s/it]\u001b[32m[I 2022-06-01 03:00:53,102]\u001b[0m Trial 37 finished with value: 0.07797531238923937 and parameters: {'feature_fraction': 0.784}. Best is trial 37 with value: 0.07797531238923937.\u001b[0m\n",
      "feature_fraction_stage2, val_score: 0.077975:  17%|#6        | 1/6 [01:31<07:36, 91.31s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Early stopping, best iteration is:\n",
      "[130]\tvalid_0's binary_logloss: 0.0745195\tvalid_1's binary_logloss: 0.0779753\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/kaggler/.local/lib/python3.8/site-packages/lightgbm/engine.py:181: UserWarning: 'early_stopping_rounds' argument is deprecated and will be removed in a future release of LightGBM. Pass 'early_stopping()' callback via 'callbacks' argument instead.\n",
      "  _log_warning(\"'early_stopping_rounds' argument is deprecated and will be removed in a future release of LightGBM. \"\n",
      "/home/kaggler/.local/lib/python3.8/site-packages/lightgbm/engine.py:239: UserWarning: 'verbose_eval' argument is deprecated and will be removed in a future release of LightGBM. Pass 'log_evaluation()' callback via 'callbacks' argument instead.\n",
      "  _log_warning(\"'verbose_eval' argument is deprecated and will be removed in a future release of LightGBM. \"\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[LightGBM] [Info] Number of positive: 572173, number of negative: 7626717\n",
      "[LightGBM] [Warning] Auto-choosing row-wise multi-threading, the overhead of testing was 0.214264 seconds.\n",
      "You can set `force_row_wise=true` to remove the overhead.\n",
      "And if memory is not enough, you can set `force_col_wise=true`.\n",
      "[LightGBM] [Info] Total Bins 7249\n",
      "[LightGBM] [Info] Number of data points in the train set: 8198890, number of used features: 48\n",
      "[LightGBM] [Info] [binary:BoostFromScore]: pavg=0.069787 -> initscore=-2.589971\n",
      "[LightGBM] [Info] Start training from score -2.589971\n",
      "Training until validation scores don't improve for 100 rounds\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "feature_fraction_stage2, val_score: 0.077975:  33%|###3      | 2/6 [02:59<05:56, 89.23s/it]\u001b[32m[I 2022-06-01 03:02:20,875]\u001b[0m Trial 38 finished with value: 0.0784904221791279 and parameters: {'feature_fraction': 0.7200000000000001}. Best is trial 37 with value: 0.07797531238923937.\u001b[0m\n",
      "feature_fraction_stage2, val_score: 0.077975:  33%|###3      | 2/6 [02:59<05:56, 89.23s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Early stopping, best iteration is:\n",
      "[114]\tvalid_0's binary_logloss: 0.075685\tvalid_1's binary_logloss: 0.0784904\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/kaggler/.local/lib/python3.8/site-packages/lightgbm/engine.py:181: UserWarning: 'early_stopping_rounds' argument is deprecated and will be removed in a future release of LightGBM. Pass 'early_stopping()' callback via 'callbacks' argument instead.\n",
      "  _log_warning(\"'early_stopping_rounds' argument is deprecated and will be removed in a future release of LightGBM. \"\n",
      "/home/kaggler/.local/lib/python3.8/site-packages/lightgbm/engine.py:239: UserWarning: 'verbose_eval' argument is deprecated and will be removed in a future release of LightGBM. Pass 'log_evaluation()' callback via 'callbacks' argument instead.\n",
      "  _log_warning(\"'verbose_eval' argument is deprecated and will be removed in a future release of LightGBM. \"\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[LightGBM] [Info] Number of positive: 572173, number of negative: 7626717\n",
      "[LightGBM] [Warning] Auto-choosing row-wise multi-threading, the overhead of testing was 0.268932 seconds.\n",
      "You can set `force_row_wise=true` to remove the overhead.\n",
      "And if memory is not enough, you can set `force_col_wise=true`.\n",
      "[LightGBM] [Info] Total Bins 7249\n",
      "[LightGBM] [Info] Number of data points in the train set: 8198890, number of used features: 48\n",
      "[LightGBM] [Info] [binary:BoostFromScore]: pavg=0.069787 -> initscore=-2.589971\n",
      "[LightGBM] [Info] Start training from score -2.589971\n",
      "Training until validation scores don't improve for 100 rounds\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "feature_fraction_stage2, val_score: 0.077975:  50%|#####     | 3/6 [04:28<04:28, 89.36s/it]\u001b[32m[I 2022-06-01 03:03:50,391]\u001b[0m Trial 39 finished with value: 0.0784389561958361 and parameters: {'feature_fraction': 0.8160000000000001}. Best is trial 37 with value: 0.07797531238923937.\u001b[0m\n",
      "feature_fraction_stage2, val_score: 0.077975:  50%|#####     | 3/6 [04:28<04:28, 89.36s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Early stopping, best iteration is:\n",
      "[115]\tvalid_0's binary_logloss: 0.0755292\tvalid_1's binary_logloss: 0.078439\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/kaggler/.local/lib/python3.8/site-packages/lightgbm/engine.py:181: UserWarning: 'early_stopping_rounds' argument is deprecated and will be removed in a future release of LightGBM. Pass 'early_stopping()' callback via 'callbacks' argument instead.\n",
      "  _log_warning(\"'early_stopping_rounds' argument is deprecated and will be removed in a future release of LightGBM. \"\n",
      "/home/kaggler/.local/lib/python3.8/site-packages/lightgbm/engine.py:239: UserWarning: 'verbose_eval' argument is deprecated and will be removed in a future release of LightGBM. Pass 'log_evaluation()' callback via 'callbacks' argument instead.\n",
      "  _log_warning(\"'verbose_eval' argument is deprecated and will be removed in a future release of LightGBM. \"\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[LightGBM] [Info] Number of positive: 572173, number of negative: 7626717\n",
      "[LightGBM] [Warning] Auto-choosing row-wise multi-threading, the overhead of testing was 0.412082 seconds.\n",
      "You can set `force_row_wise=true` to remove the overhead.\n",
      "And if memory is not enough, you can set `force_col_wise=true`.\n",
      "[LightGBM] [Info] Total Bins 7249\n",
      "[LightGBM] [Info] Number of data points in the train set: 8198890, number of used features: 48\n",
      "[LightGBM] [Info] [binary:BoostFromScore]: pavg=0.069787 -> initscore=-2.589971\n",
      "[LightGBM] [Info] Start training from score -2.589971\n",
      "Training until validation scores don't improve for 100 rounds\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "feature_fraction_stage2, val_score: 0.077975:  67%|######6   | 4/6 [05:59<02:59, 89.90s/it]\u001b[32m[I 2022-06-01 03:05:21,115]\u001b[0m Trial 40 finished with value: 0.07838391025090949 and parameters: {'feature_fraction': 0.7520000000000001}. Best is trial 37 with value: 0.07797531238923937.\u001b[0m\n",
      "feature_fraction_stage2, val_score: 0.077975:  67%|######6   | 4/6 [05:59<02:59, 89.90s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Early stopping, best iteration is:\n",
      "[119]\tvalid_0's binary_logloss: 0.0753449\tvalid_1's binary_logloss: 0.0783839\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/kaggler/.local/lib/python3.8/site-packages/lightgbm/engine.py:181: UserWarning: 'early_stopping_rounds' argument is deprecated and will be removed in a future release of LightGBM. Pass 'early_stopping()' callback via 'callbacks' argument instead.\n",
      "  _log_warning(\"'early_stopping_rounds' argument is deprecated and will be removed in a future release of LightGBM. \"\n",
      "/home/kaggler/.local/lib/python3.8/site-packages/lightgbm/engine.py:239: UserWarning: 'verbose_eval' argument is deprecated and will be removed in a future release of LightGBM. Pass 'log_evaluation()' callback via 'callbacks' argument instead.\n",
      "  _log_warning(\"'verbose_eval' argument is deprecated and will be removed in a future release of LightGBM. \"\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[LightGBM] [Info] Number of positive: 572173, number of negative: 7626717\n",
      "[LightGBM] [Warning] Auto-choosing row-wise multi-threading, the overhead of testing was 0.231605 seconds.\n",
      "You can set `force_row_wise=true` to remove the overhead.\n",
      "And if memory is not enough, you can set `force_col_wise=true`.\n",
      "[LightGBM] [Info] Total Bins 7249\n",
      "[LightGBM] [Info] Number of data points in the train set: 8198890, number of used features: 48\n",
      "[LightGBM] [Info] [binary:BoostFromScore]: pavg=0.069787 -> initscore=-2.589971\n",
      "[LightGBM] [Info] Start training from score -2.589971\n",
      "Training until validation scores don't improve for 100 rounds\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "feature_fraction_stage2, val_score: 0.077975:  83%|########3 | 5/6 [07:26<01:28, 88.95s/it]\u001b[32m[I 2022-06-01 03:06:48,378]\u001b[0m Trial 41 finished with value: 0.07873739117920203 and parameters: {'feature_fraction': 0.8480000000000001}. Best is trial 37 with value: 0.07797531238923937.\u001b[0m\n",
      "feature_fraction_stage2, val_score: 0.077975:  83%|########3 | 5/6 [07:26<01:28, 88.95s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Early stopping, best iteration is:\n",
      "[108]\tvalid_0's binary_logloss: 0.0759032\tvalid_1's binary_logloss: 0.0787374\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/kaggler/.local/lib/python3.8/site-packages/lightgbm/engine.py:181: UserWarning: 'early_stopping_rounds' argument is deprecated and will be removed in a future release of LightGBM. Pass 'early_stopping()' callback via 'callbacks' argument instead.\n",
      "  _log_warning(\"'early_stopping_rounds' argument is deprecated and will be removed in a future release of LightGBM. \"\n",
      "/home/kaggler/.local/lib/python3.8/site-packages/lightgbm/engine.py:239: UserWarning: 'verbose_eval' argument is deprecated and will be removed in a future release of LightGBM. Pass 'log_evaluation()' callback via 'callbacks' argument instead.\n",
      "  _log_warning(\"'verbose_eval' argument is deprecated and will be removed in a future release of LightGBM. \"\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[LightGBM] [Info] Number of positive: 572173, number of negative: 7626717\n",
      "[LightGBM] [Warning] Auto-choosing row-wise multi-threading, the overhead of testing was 0.239534 seconds.\n",
      "You can set `force_row_wise=true` to remove the overhead.\n",
      "And if memory is not enough, you can set `force_col_wise=true`.\n",
      "[LightGBM] [Info] Total Bins 7249\n",
      "[LightGBM] [Info] Number of data points in the train set: 8198890, number of used features: 48\n",
      "[LightGBM] [Info] [binary:BoostFromScore]: pavg=0.069787 -> initscore=-2.589971\n",
      "[LightGBM] [Info] Start training from score -2.589971\n",
      "Training until validation scores don't improve for 100 rounds\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "feature_fraction_stage2, val_score: 0.077975: 100%|##########| 6/6 [08:51<00:00, 87.65s/it]\u001b[32m[I 2022-06-01 03:08:13,517]\u001b[0m Trial 42 finished with value: 0.07853562067069128 and parameters: {'feature_fraction': 0.88}. Best is trial 37 with value: 0.07797531238923937.\u001b[0m\n",
      "feature_fraction_stage2, val_score: 0.077975: 100%|##########| 6/6 [08:51<00:00, 88.62s/it]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Early stopping, best iteration is:\n",
      "[108]\tvalid_0's binary_logloss: 0.075757\tvalid_1's binary_logloss: 0.0785356\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "regularization_factors, val_score: 0.077975:   0%|          | 0/20 [00:00<?, ?it/s]/home/kaggler/.local/lib/python3.8/site-packages/lightgbm/engine.py:181: UserWarning: 'early_stopping_rounds' argument is deprecated and will be removed in a future release of LightGBM. Pass 'early_stopping()' callback via 'callbacks' argument instead.\n",
      "  _log_warning(\"'early_stopping_rounds' argument is deprecated and will be removed in a future release of LightGBM. \"\n",
      "/home/kaggler/.local/lib/python3.8/site-packages/lightgbm/engine.py:239: UserWarning: 'verbose_eval' argument is deprecated and will be removed in a future release of LightGBM. Pass 'log_evaluation()' callback via 'callbacks' argument instead.\n",
      "  _log_warning(\"'verbose_eval' argument is deprecated and will be removed in a future release of LightGBM. \"\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[LightGBM] [Info] Number of positive: 572173, number of negative: 7626717\n",
      "[LightGBM] [Warning] Auto-choosing row-wise multi-threading, the overhead of testing was 0.234614 seconds.\n",
      "You can set `force_row_wise=true` to remove the overhead.\n",
      "And if memory is not enough, you can set `force_col_wise=true`.\n",
      "[LightGBM] [Info] Total Bins 7249\n",
      "[LightGBM] [Info] Number of data points in the train set: 8198890, number of used features: 48\n",
      "[LightGBM] [Info] [binary:BoostFromScore]: pavg=0.069787 -> initscore=-2.589971\n",
      "[LightGBM] [Info] Start training from score -2.589971\n",
      "Training until validation scores don't improve for 100 rounds\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "regularization_factors, val_score: 0.077975:   5%|5         | 1/20 [01:37<30:54, 97.61s/it]\u001b[32m[I 2022-06-01 03:09:51,132]\u001b[0m Trial 43 finished with value: 0.07827041777762707 and parameters: {'lambda_l1': 1.9824807235681123e-05, 'lambda_l2': 3.3017804879301865e-05}. Best is trial 43 with value: 0.07827041777762707.\u001b[0m\n",
      "regularization_factors, val_score: 0.077975:   5%|5         | 1/20 [01:37<30:54, 97.61s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Early stopping, best iteration is:\n",
      "[132]\tvalid_0's binary_logloss: 0.0745359\tvalid_1's binary_logloss: 0.0782704\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/kaggler/.local/lib/python3.8/site-packages/lightgbm/engine.py:181: UserWarning: 'early_stopping_rounds' argument is deprecated and will be removed in a future release of LightGBM. Pass 'early_stopping()' callback via 'callbacks' argument instead.\n",
      "  _log_warning(\"'early_stopping_rounds' argument is deprecated and will be removed in a future release of LightGBM. \"\n",
      "/home/kaggler/.local/lib/python3.8/site-packages/lightgbm/engine.py:239: UserWarning: 'verbose_eval' argument is deprecated and will be removed in a future release of LightGBM. Pass 'log_evaluation()' callback via 'callbacks' argument instead.\n",
      "  _log_warning(\"'verbose_eval' argument is deprecated and will be removed in a future release of LightGBM. \"\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[LightGBM] [Info] Number of positive: 572173, number of negative: 7626717\n",
      "[LightGBM] [Warning] Auto-choosing row-wise multi-threading, the overhead of testing was 0.253972 seconds.\n",
      "You can set `force_row_wise=true` to remove the overhead.\n",
      "And if memory is not enough, you can set `force_col_wise=true`.\n",
      "[LightGBM] [Info] Total Bins 7249\n",
      "[LightGBM] [Info] Number of data points in the train set: 8198890, number of used features: 48\n",
      "[LightGBM] [Info] [binary:BoostFromScore]: pavg=0.069787 -> initscore=-2.589971\n",
      "[LightGBM] [Info] Start training from score -2.589971\n",
      "Training until validation scores don't improve for 100 rounds\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "regularization_factors, val_score: 0.077975:  10%|#         | 2/20 [03:12<28:53, 96.29s/it]\u001b[32m[I 2022-06-01 03:11:26,502]\u001b[0m Trial 44 finished with value: 0.07797530752835226 and parameters: {'lambda_l1': 2.388491269892158e-08, 'lambda_l2': 2.4762348793355323e-08}. Best is trial 44 with value: 0.07797530752835226.\u001b[0m\n",
      "regularization_factors, val_score: 0.077975:  10%|#         | 2/20 [03:12<28:53, 96.29s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Early stopping, best iteration is:\n",
      "[130]\tvalid_0's binary_logloss: 0.0745195\tvalid_1's binary_logloss: 0.0779753\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/kaggler/.local/lib/python3.8/site-packages/lightgbm/engine.py:181: UserWarning: 'early_stopping_rounds' argument is deprecated and will be removed in a future release of LightGBM. Pass 'early_stopping()' callback via 'callbacks' argument instead.\n",
      "  _log_warning(\"'early_stopping_rounds' argument is deprecated and will be removed in a future release of LightGBM. \"\n",
      "/home/kaggler/.local/lib/python3.8/site-packages/lightgbm/engine.py:239: UserWarning: 'verbose_eval' argument is deprecated and will be removed in a future release of LightGBM. Pass 'log_evaluation()' callback via 'callbacks' argument instead.\n",
      "  _log_warning(\"'verbose_eval' argument is deprecated and will be removed in a future release of LightGBM. \"\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[LightGBM] [Info] Number of positive: 572173, number of negative: 7626717\n",
      "[LightGBM] [Warning] Auto-choosing row-wise multi-threading, the overhead of testing was 0.215527 seconds.\n",
      "You can set `force_row_wise=true` to remove the overhead.\n",
      "And if memory is not enough, you can set `force_col_wise=true`.\n",
      "[LightGBM] [Info] Total Bins 7249\n",
      "[LightGBM] [Info] Number of data points in the train set: 8198890, number of used features: 48\n",
      "[LightGBM] [Info] [binary:BoostFromScore]: pavg=0.069787 -> initscore=-2.589971\n",
      "[LightGBM] [Info] Start training from score -2.589971\n",
      "Training until validation scores don't improve for 100 rounds\n",
      "[1000]\tvalid_0's binary_logloss: 0.0564255\tvalid_1's binary_logloss: 0.0697415\n",
      "[2000]\tvalid_0's binary_logloss: 0.0447125\tvalid_1's binary_logloss: 0.0664188\n",
      "[3000]\tvalid_0's binary_logloss: 0.0367121\tvalid_1's binary_logloss: 0.0641601\n",
      "[4000]\tvalid_0's binary_logloss: 0.0305425\tvalid_1's binary_logloss: 0.0626004\n",
      "[5000]\tvalid_0's binary_logloss: 0.0254455\tvalid_1's binary_logloss: 0.0615005\n",
      "[6000]\tvalid_0's binary_logloss: 0.0214419\tvalid_1's binary_logloss: 0.0606714\n",
      "[7000]\tvalid_0's binary_logloss: 0.0180652\tvalid_1's binary_logloss: 0.0600789\n",
      "[8000]\tvalid_0's binary_logloss: 0.0153203\tvalid_1's binary_logloss: 0.0597457\n",
      "Early stopping, best iteration is:\n",
      "[8784]\tvalid_0's binary_logloss: 0.0134337\tvalid_1's binary_logloss: 0.0596062\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "regularization_factors, val_score: 0.059606:  15%|#5        | 3/20 [36:01<4:29:29, 951.17s/it]\u001b[32m[I 2022-06-01 03:44:14,974]\u001b[0m Trial 45 finished with value: 0.05960618568218905 and parameters: {'lambda_l1': 0.0022231007503277617, 'lambda_l2': 3.36729733056907}. Best is trial 45 with value: 0.05960618568218905.\u001b[0m\n",
      "regularization_factors, val_score: 0.059606:  15%|#5        | 3/20 [36:01<4:29:29, 951.17s/it]/home/kaggler/.local/lib/python3.8/site-packages/lightgbm/engine.py:181: UserWarning: 'early_stopping_rounds' argument is deprecated and will be removed in a future release of LightGBM. Pass 'early_stopping()' callback via 'callbacks' argument instead.\n",
      "  _log_warning(\"'early_stopping_rounds' argument is deprecated and will be removed in a future release of LightGBM. \"\n",
      "/home/kaggler/.local/lib/python3.8/site-packages/lightgbm/engine.py:239: UserWarning: 'verbose_eval' argument is deprecated and will be removed in a future release of LightGBM. Pass 'log_evaluation()' callback via 'callbacks' argument instead.\n",
      "  _log_warning(\"'verbose_eval' argument is deprecated and will be removed in a future release of LightGBM. \"\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[LightGBM] [Info] Number of positive: 572173, number of negative: 7626717\n",
      "[LightGBM] [Warning] Auto-choosing row-wise multi-threading, the overhead of testing was 0.229379 seconds.\n",
      "You can set `force_row_wise=true` to remove the overhead.\n",
      "And if memory is not enough, you can set `force_col_wise=true`.\n",
      "[LightGBM] [Info] Total Bins 7249\n",
      "[LightGBM] [Info] Number of data points in the train set: 8198890, number of used features: 48\n",
      "[LightGBM] [Info] [binary:BoostFromScore]: pavg=0.069787 -> initscore=-2.589971\n",
      "[LightGBM] [Info] Start training from score -2.589971\n",
      "Training until validation scores don't improve for 100 rounds\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "regularization_factors, val_score: 0.059606:  20%|##        | 4/20 [37:30<2:42:54, 610.90s/it]\u001b[32m[I 2022-06-01 03:45:44,254]\u001b[0m Trial 46 finished with value: 0.0782985791337737 and parameters: {'lambda_l1': 0.11298582108374314, 'lambda_l2': 1.7276621824418402e-06}. Best is trial 45 with value: 0.05960618568218905.\u001b[0m\n",
      "regularization_factors, val_score: 0.059606:  20%|##        | 4/20 [37:30<2:42:54, 610.90s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Early stopping, best iteration is:\n",
      "[114]\tvalid_0's binary_logloss: 0.0754304\tvalid_1's binary_logloss: 0.0782986\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/kaggler/.local/lib/python3.8/site-packages/lightgbm/engine.py:181: UserWarning: 'early_stopping_rounds' argument is deprecated and will be removed in a future release of LightGBM. Pass 'early_stopping()' callback via 'callbacks' argument instead.\n",
      "  _log_warning(\"'early_stopping_rounds' argument is deprecated and will be removed in a future release of LightGBM. \"\n",
      "/home/kaggler/.local/lib/python3.8/site-packages/lightgbm/engine.py:239: UserWarning: 'verbose_eval' argument is deprecated and will be removed in a future release of LightGBM. Pass 'log_evaluation()' callback via 'callbacks' argument instead.\n",
      "  _log_warning(\"'verbose_eval' argument is deprecated and will be removed in a future release of LightGBM. \"\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[LightGBM] [Info] Number of positive: 572173, number of negative: 7626717\n",
      "[LightGBM] [Warning] Auto-choosing row-wise multi-threading, the overhead of testing was 0.248376 seconds.\n",
      "You can set `force_row_wise=true` to remove the overhead.\n",
      "And if memory is not enough, you can set `force_col_wise=true`.\n",
      "[LightGBM] [Info] Total Bins 7249\n",
      "[LightGBM] [Info] Number of data points in the train set: 8198890, number of used features: 48\n",
      "[LightGBM] [Info] [binary:BoostFromScore]: pavg=0.069787 -> initscore=-2.589971\n",
      "[LightGBM] [Info] Start training from score -2.589971\n",
      "Training until validation scores don't improve for 100 rounds\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "regularization_factors, val_score: 0.059606:  25%|##5       | 5/20 [39:05<1:46:09, 424.66s/it]\u001b[32m[I 2022-06-01 03:47:18,672]\u001b[0m Trial 47 finished with value: 0.07793789832219052 and parameters: {'lambda_l1': 0.00015434886582638653, 'lambda_l2': 2.5140122783361567e-06}. Best is trial 45 with value: 0.05960618568218905.\u001b[0m\n",
      "regularization_factors, val_score: 0.059606:  25%|##5       | 5/20 [39:05<1:46:09, 424.66s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Early stopping, best iteration is:\n",
      "[131]\tvalid_0's binary_logloss: 0.0744585\tvalid_1's binary_logloss: 0.0779379\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/kaggler/.local/lib/python3.8/site-packages/lightgbm/engine.py:181: UserWarning: 'early_stopping_rounds' argument is deprecated and will be removed in a future release of LightGBM. Pass 'early_stopping()' callback via 'callbacks' argument instead.\n",
      "  _log_warning(\"'early_stopping_rounds' argument is deprecated and will be removed in a future release of LightGBM. \"\n",
      "/home/kaggler/.local/lib/python3.8/site-packages/lightgbm/engine.py:239: UserWarning: 'verbose_eval' argument is deprecated and will be removed in a future release of LightGBM. Pass 'log_evaluation()' callback via 'callbacks' argument instead.\n",
      "  _log_warning(\"'verbose_eval' argument is deprecated and will be removed in a future release of LightGBM. \"\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[LightGBM] [Info] Number of positive: 572173, number of negative: 7626717\n",
      "[LightGBM] [Warning] Auto-choosing row-wise multi-threading, the overhead of testing was 0.233661 seconds.\n",
      "You can set `force_row_wise=true` to remove the overhead.\n",
      "And if memory is not enough, you can set `force_col_wise=true`.\n",
      "[LightGBM] [Info] Total Bins 7249\n",
      "[LightGBM] [Info] Number of data points in the train set: 8198890, number of used features: 48\n",
      "[LightGBM] [Info] [binary:BoostFromScore]: pavg=0.069787 -> initscore=-2.589971\n",
      "[LightGBM] [Info] Start training from score -2.589971\n",
      "Training until validation scores don't improve for 100 rounds\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "regularization_factors, val_score: 0.059606:  30%|###       | 6/20 [40:43<1:13:13, 313.82s/it]\u001b[32m[I 2022-06-01 03:48:57,328]\u001b[0m Trial 48 finished with value: 0.07795578146917535 and parameters: {'lambda_l1': 5.794756044219735e-05, 'lambda_l2': 0.002868571568003392}. Best is trial 45 with value: 0.05960618568218905.\u001b[0m\n",
      "regularization_factors, val_score: 0.059606:  30%|###       | 6/20 [40:43<1:13:13, 313.82s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Early stopping, best iteration is:\n",
      "[135]\tvalid_0's binary_logloss: 0.0742525\tvalid_1's binary_logloss: 0.0779558\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/kaggler/.local/lib/python3.8/site-packages/lightgbm/engine.py:181: UserWarning: 'early_stopping_rounds' argument is deprecated and will be removed in a future release of LightGBM. Pass 'early_stopping()' callback via 'callbacks' argument instead.\n",
      "  _log_warning(\"'early_stopping_rounds' argument is deprecated and will be removed in a future release of LightGBM. \"\n",
      "/home/kaggler/.local/lib/python3.8/site-packages/lightgbm/engine.py:239: UserWarning: 'verbose_eval' argument is deprecated and will be removed in a future release of LightGBM. Pass 'log_evaluation()' callback via 'callbacks' argument instead.\n",
      "  _log_warning(\"'verbose_eval' argument is deprecated and will be removed in a future release of LightGBM. \"\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[LightGBM] [Info] Number of positive: 572173, number of negative: 7626717\n",
      "[LightGBM] [Warning] Auto-choosing row-wise multi-threading, the overhead of testing was 0.265998 seconds.\n",
      "You can set `force_row_wise=true` to remove the overhead.\n",
      "And if memory is not enough, you can set `force_col_wise=true`.\n",
      "[LightGBM] [Info] Total Bins 7249\n",
      "[LightGBM] [Info] Number of data points in the train set: 8198890, number of used features: 48\n",
      "[LightGBM] [Info] [binary:BoostFromScore]: pavg=0.069787 -> initscore=-2.589971\n",
      "[LightGBM] [Info] Start training from score -2.589971\n",
      "Training until validation scores don't improve for 100 rounds\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "regularization_factors, val_score: 0.059606:  35%|###5      | 7/20 [42:11<52:00, 240.03s/it]  \u001b[32m[I 2022-06-01 03:50:25,450]\u001b[0m Trial 49 finished with value: 0.07842508382222355 and parameters: {'lambda_l1': 0.11349664640729587, 'lambda_l2': 2.275332160272644e-05}. Best is trial 45 with value: 0.05960618568218905.\u001b[0m\n",
      "regularization_factors, val_score: 0.059606:  35%|###5      | 7/20 [42:11<52:00, 240.03s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Early stopping, best iteration is:\n",
      "[119]\tvalid_0's binary_logloss: 0.0752273\tvalid_1's binary_logloss: 0.0784251\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/kaggler/.local/lib/python3.8/site-packages/lightgbm/engine.py:181: UserWarning: 'early_stopping_rounds' argument is deprecated and will be removed in a future release of LightGBM. Pass 'early_stopping()' callback via 'callbacks' argument instead.\n",
      "  _log_warning(\"'early_stopping_rounds' argument is deprecated and will be removed in a future release of LightGBM. \"\n",
      "/home/kaggler/.local/lib/python3.8/site-packages/lightgbm/engine.py:239: UserWarning: 'verbose_eval' argument is deprecated and will be removed in a future release of LightGBM. Pass 'log_evaluation()' callback via 'callbacks' argument instead.\n",
      "  _log_warning(\"'verbose_eval' argument is deprecated and will be removed in a future release of LightGBM. \"\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[LightGBM] [Info] Number of positive: 572173, number of negative: 7626717\n",
      "[LightGBM] [Warning] Auto-choosing row-wise multi-threading, the overhead of testing was 0.236960 seconds.\n",
      "You can set `force_row_wise=true` to remove the overhead.\n",
      "And if memory is not enough, you can set `force_col_wise=true`.\n",
      "[LightGBM] [Info] Total Bins 7249\n",
      "[LightGBM] [Info] Number of data points in the train set: 8198890, number of used features: 48\n",
      "[LightGBM] [Info] [binary:BoostFromScore]: pavg=0.069787 -> initscore=-2.589971\n",
      "[LightGBM] [Info] Start training from score -2.589971\n",
      "Training until validation scores don't improve for 100 rounds\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "regularization_factors, val_score: 0.059606:  40%|####      | 8/20 [43:45<38:42, 193.50s/it]\u001b[32m[I 2022-06-01 03:51:59,327]\u001b[0m Trial 50 finished with value: 0.07797528556181059 and parameters: {'lambda_l1': 1.2695647001077275e-06, 'lambda_l2': 1.131580849585716e-07}. Best is trial 45 with value: 0.05960618568218905.\u001b[0m\n",
      "regularization_factors, val_score: 0.059606:  40%|####      | 8/20 [43:45<38:42, 193.50s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Early stopping, best iteration is:\n",
      "[130]\tvalid_0's binary_logloss: 0.0745195\tvalid_1's binary_logloss: 0.0779753\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/kaggler/.local/lib/python3.8/site-packages/lightgbm/engine.py:181: UserWarning: 'early_stopping_rounds' argument is deprecated and will be removed in a future release of LightGBM. Pass 'early_stopping()' callback via 'callbacks' argument instead.\n",
      "  _log_warning(\"'early_stopping_rounds' argument is deprecated and will be removed in a future release of LightGBM. \"\n",
      "/home/kaggler/.local/lib/python3.8/site-packages/lightgbm/engine.py:239: UserWarning: 'verbose_eval' argument is deprecated and will be removed in a future release of LightGBM. Pass 'log_evaluation()' callback via 'callbacks' argument instead.\n",
      "  _log_warning(\"'verbose_eval' argument is deprecated and will be removed in a future release of LightGBM. \"\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[LightGBM] [Info] Number of positive: 572173, number of negative: 7626717\n",
      "[LightGBM] [Warning] Auto-choosing row-wise multi-threading, the overhead of testing was 0.244085 seconds.\n",
      "You can set `force_row_wise=true` to remove the overhead.\n",
      "And if memory is not enough, you can set `force_col_wise=true`.\n",
      "[LightGBM] [Info] Total Bins 7249\n",
      "[LightGBM] [Info] Number of data points in the train set: 8198890, number of used features: 48\n",
      "[LightGBM] [Info] [binary:BoostFromScore]: pavg=0.069787 -> initscore=-2.589971\n",
      "[LightGBM] [Info] Start training from score -2.589971\n",
      "Training until validation scores don't improve for 100 rounds\n",
      "[1000]\tvalid_0's binary_logloss: 0.0518346\tvalid_1's binary_logloss: 0.0691119\n",
      "[2000]\tvalid_0's binary_logloss: 0.0388218\tvalid_1's binary_logloss: 0.0654092\n",
      "[3000]\tvalid_0's binary_logloss: 0.0299026\tvalid_1's binary_logloss: 0.0630775\n",
      "[4000]\tvalid_0's binary_logloss: 0.0233198\tvalid_1's binary_logloss: 0.061474\n",
      "[5000]\tvalid_0's binary_logloss: 0.0185325\tvalid_1's binary_logloss: 0.0606108\n",
      "[6000]\tvalid_0's binary_logloss: 0.0146965\tvalid_1's binary_logloss: 0.0600239\n",
      "Early stopping, best iteration is:\n",
      "[6628]\tvalid_0's binary_logloss: 0.0128066\tvalid_1's binary_logloss: 0.0598479\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "regularization_factors, val_score: 0.059606:  45%|####5     | 9/20 [1:07:21<1:45:32, 575.69s/it]\u001b[32m[I 2022-06-01 04:15:35,379]\u001b[0m Trial 51 finished with value: 0.059847906837609564 and parameters: {'lambda_l1': 0.011099531648039222, 'lambda_l2': 0.10994171858739626}. Best is trial 45 with value: 0.05960618568218905.\u001b[0m\n",
      "regularization_factors, val_score: 0.059606:  45%|####5     | 9/20 [1:07:21<1:45:32, 575.69s/it]/home/kaggler/.local/lib/python3.8/site-packages/lightgbm/engine.py:181: UserWarning: 'early_stopping_rounds' argument is deprecated and will be removed in a future release of LightGBM. Pass 'early_stopping()' callback via 'callbacks' argument instead.\n",
      "  _log_warning(\"'early_stopping_rounds' argument is deprecated and will be removed in a future release of LightGBM. \"\n",
      "/home/kaggler/.local/lib/python3.8/site-packages/lightgbm/engine.py:239: UserWarning: 'verbose_eval' argument is deprecated and will be removed in a future release of LightGBM. Pass 'log_evaluation()' callback via 'callbacks' argument instead.\n",
      "  _log_warning(\"'verbose_eval' argument is deprecated and will be removed in a future release of LightGBM. \"\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[LightGBM] [Info] Number of positive: 572173, number of negative: 7626717\n",
      "[LightGBM] [Warning] Auto-choosing row-wise multi-threading, the overhead of testing was 0.260027 seconds.\n",
      "You can set `force_row_wise=true` to remove the overhead.\n",
      "And if memory is not enough, you can set `force_col_wise=true`.\n",
      "[LightGBM] [Info] Total Bins 7249\n",
      "[LightGBM] [Info] Number of data points in the train set: 8198890, number of used features: 48\n",
      "[LightGBM] [Info] [binary:BoostFromScore]: pavg=0.069787 -> initscore=-2.589971\n",
      "[LightGBM] [Info] Start training from score -2.589971\n",
      "Training until validation scores don't improve for 100 rounds\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "regularization_factors, val_score: 0.059606:  50%|#####     | 10/20 [1:08:51<1:10:56, 425.65s/it]\u001b[32m[I 2022-06-01 04:17:05,070]\u001b[0m Trial 52 finished with value: 0.07869033204496463 and parameters: {'lambda_l1': 1.9802129009055624e-06, 'lambda_l2': 0.0008075351491139359}. Best is trial 45 with value: 0.05960618568218905.\u001b[0m\n",
      "regularization_factors, val_score: 0.059606:  50%|#####     | 10/20 [1:08:51<1:10:56, 425.65s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Early stopping, best iteration is:\n",
      "[111]\tvalid_0's binary_logloss: 0.0758682\tvalid_1's binary_logloss: 0.0786903\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/kaggler/.local/lib/python3.8/site-packages/lightgbm/engine.py:181: UserWarning: 'early_stopping_rounds' argument is deprecated and will be removed in a future release of LightGBM. Pass 'early_stopping()' callback via 'callbacks' argument instead.\n",
      "  _log_warning(\"'early_stopping_rounds' argument is deprecated and will be removed in a future release of LightGBM. \"\n",
      "/home/kaggler/.local/lib/python3.8/site-packages/lightgbm/engine.py:239: UserWarning: 'verbose_eval' argument is deprecated and will be removed in a future release of LightGBM. Pass 'log_evaluation()' callback via 'callbacks' argument instead.\n",
      "  _log_warning(\"'verbose_eval' argument is deprecated and will be removed in a future release of LightGBM. \"\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[LightGBM] [Info] Number of positive: 572173, number of negative: 7626717\n",
      "[LightGBM] [Warning] Auto-choosing row-wise multi-threading, the overhead of testing was 0.218209 seconds.\n",
      "You can set `force_row_wise=true` to remove the overhead.\n",
      "And if memory is not enough, you can set `force_col_wise=true`.\n",
      "[LightGBM] [Info] Total Bins 7249\n",
      "[LightGBM] [Info] Number of data points in the train set: 8198890, number of used features: 48\n",
      "[LightGBM] [Info] [binary:BoostFromScore]: pavg=0.069787 -> initscore=-2.589971\n",
      "[LightGBM] [Info] Start training from score -2.589971\n",
      "Training until validation scores don't improve for 100 rounds\n",
      "[1000]\tvalid_0's binary_logloss: 0.0580484\tvalid_1's binary_logloss: 0.0697421\n",
      "[2000]\tvalid_0's binary_logloss: 0.047628\tvalid_1's binary_logloss: 0.0665176\n",
      "[3000]\tvalid_0's binary_logloss: 0.0402564\tvalid_1's binary_logloss: 0.064536\n",
      "[4000]\tvalid_0's binary_logloss: 0.0345417\tvalid_1's binary_logloss: 0.0631148\n",
      "[5000]\tvalid_0's binary_logloss: 0.0299103\tvalid_1's binary_logloss: 0.0620122\n",
      "[6000]\tvalid_0's binary_logloss: 0.0262074\tvalid_1's binary_logloss: 0.0612795\n",
      "[7000]\tvalid_0's binary_logloss: 0.0230758\tvalid_1's binary_logloss: 0.0607134\n",
      "[8000]\tvalid_0's binary_logloss: 0.02051\tvalid_1's binary_logloss: 0.0603431\n",
      "[9000]\tvalid_0's binary_logloss: 0.0182505\tvalid_1's binary_logloss: 0.060077\n",
      "[10000]\tvalid_0's binary_logloss: 0.0163635\tvalid_1's binary_logloss: 0.0599442\n",
      "Early stopping, best iteration is:\n",
      "[10267]\tvalid_0's binary_logloss: 0.0158749\tvalid_1's binary_logloss: 0.0599145\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "regularization_factors, val_score: 0.059606:  55%|#####5    | 11/20 [1:51:11<2:40:55, 1072.84s/it]\u001b[32m[I 2022-06-01 04:59:25,370]\u001b[0m Trial 53 finished with value: 0.059914475199279185 and parameters: {'lambda_l1': 3.561317566607829, 'lambda_l2': 5.409147527374577}. Best is trial 45 with value: 0.05960618568218905.\u001b[0m\n",
      "regularization_factors, val_score: 0.059606:  55%|#####5    | 11/20 [1:51:11<2:40:55, 1072.84s/it]/home/kaggler/.local/lib/python3.8/site-packages/lightgbm/engine.py:181: UserWarning: 'early_stopping_rounds' argument is deprecated and will be removed in a future release of LightGBM. Pass 'early_stopping()' callback via 'callbacks' argument instead.\n",
      "  _log_warning(\"'early_stopping_rounds' argument is deprecated and will be removed in a future release of LightGBM. \"\n",
      "/home/kaggler/.local/lib/python3.8/site-packages/lightgbm/engine.py:239: UserWarning: 'verbose_eval' argument is deprecated and will be removed in a future release of LightGBM. Pass 'log_evaluation()' callback via 'callbacks' argument instead.\n",
      "  _log_warning(\"'verbose_eval' argument is deprecated and will be removed in a future release of LightGBM. \"\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[LightGBM] [Info] Number of positive: 572173, number of negative: 7626717\n",
      "[LightGBM] [Warning] Auto-choosing row-wise multi-threading, the overhead of testing was 0.245979 seconds.\n",
      "You can set `force_row_wise=true` to remove the overhead.\n",
      "And if memory is not enough, you can set `force_col_wise=true`.\n",
      "[LightGBM] [Info] Total Bins 7249\n",
      "[LightGBM] [Info] Number of data points in the train set: 8198890, number of used features: 48\n",
      "[LightGBM] [Info] [binary:BoostFromScore]: pavg=0.069787 -> initscore=-2.589971\n",
      "[LightGBM] [Info] Start training from score -2.589971\n",
      "Training until validation scores don't improve for 100 rounds\n",
      "[1000]\tvalid_0's binary_logloss: 0.0559878\tvalid_1's binary_logloss: 0.0697855\n",
      "[2000]\tvalid_0's binary_logloss: 0.0439075\tvalid_1's binary_logloss: 0.066207\n",
      "[3000]\tvalid_0's binary_logloss: 0.0355886\tvalid_1's binary_logloss: 0.0639227\n",
      "[4000]\tvalid_0's binary_logloss: 0.0294283\tvalid_1's binary_logloss: 0.0623767\n",
      "[5000]\tvalid_0's binary_logloss: 0.0243182\tvalid_1's binary_logloss: 0.0612601\n",
      "[6000]\tvalid_0's binary_logloss: 0.0202468\tvalid_1's binary_logloss: 0.060482\n",
      "[7000]\tvalid_0's binary_logloss: 0.0169046\tvalid_1's binary_logloss: 0.059947\n",
      "Early stopping, best iteration is:\n",
      "[7835]\tvalid_0's binary_logloss: 0.0145709\tvalid_1's binary_logloss: 0.0596902\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "regularization_factors, val_score: 0.059606:  60%|######    | 12/20 [2:20:19<2:50:25, 1278.13s/it]\u001b[32m[I 2022-06-01 05:28:33,051]\u001b[0m Trial 54 finished with value: 0.05969018563683486 and parameters: {'lambda_l1': 0.007412546609161775, 'lambda_l2': 2.304419105657086}. Best is trial 45 with value: 0.05960618568218905.\u001b[0m\n",
      "regularization_factors, val_score: 0.059606:  60%|######    | 12/20 [2:20:19<2:50:25, 1278.13s/it]/home/kaggler/.local/lib/python3.8/site-packages/lightgbm/engine.py:181: UserWarning: 'early_stopping_rounds' argument is deprecated and will be removed in a future release of LightGBM. Pass 'early_stopping()' callback via 'callbacks' argument instead.\n",
      "  _log_warning(\"'early_stopping_rounds' argument is deprecated and will be removed in a future release of LightGBM. \"\n",
      "/home/kaggler/.local/lib/python3.8/site-packages/lightgbm/engine.py:239: UserWarning: 'verbose_eval' argument is deprecated and will be removed in a future release of LightGBM. Pass 'log_evaluation()' callback via 'callbacks' argument instead.\n",
      "  _log_warning(\"'verbose_eval' argument is deprecated and will be removed in a future release of LightGBM. \"\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[LightGBM] [Info] Number of positive: 572173, number of negative: 7626717\n",
      "[LightGBM] [Warning] Auto-choosing row-wise multi-threading, the overhead of testing was 0.239033 seconds.\n",
      "You can set `force_row_wise=true` to remove the overhead.\n",
      "And if memory is not enough, you can set `force_col_wise=true`.\n",
      "[LightGBM] [Info] Total Bins 7249\n",
      "[LightGBM] [Info] Number of data points in the train set: 8198890, number of used features: 48\n",
      "[LightGBM] [Info] [binary:BoostFromScore]: pavg=0.069787 -> initscore=-2.589971\n",
      "[LightGBM] [Info] Start training from score -2.589971\n",
      "Training until validation scores don't improve for 100 rounds\n",
      "[1000]\tvalid_0's binary_logloss: 0.0568315\tvalid_1's binary_logloss: 0.0697909\n",
      "[2000]\tvalid_0's binary_logloss: 0.0459634\tvalid_1's binary_logloss: 0.0665958\n",
      "[3000]\tvalid_0's binary_logloss: 0.0379871\tvalid_1's binary_logloss: 0.0643566\n",
      "[4000]\tvalid_0's binary_logloss: 0.0319171\tvalid_1's binary_logloss: 0.0628254\n",
      "[5000]\tvalid_0's binary_logloss: 0.0270772\tvalid_1's binary_logloss: 0.0617123\n",
      "[6000]\tvalid_0's binary_logloss: 0.0231146\tvalid_1's binary_logloss: 0.0608474\n",
      "[7000]\tvalid_0's binary_logloss: 0.0196403\tvalid_1's binary_logloss: 0.0602644\n",
      "[8000]\tvalid_0's binary_logloss: 0.0169182\tvalid_1's binary_logloss: 0.0598141\n",
      "Early stopping, best iteration is:\n",
      "[8806]\tvalid_0's binary_logloss: 0.0149744\tvalid_1's binary_logloss: 0.0596184\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "regularization_factors, val_score: 0.059606:  65%|######5   | 13/20 [2:53:48<2:54:55, 1499.41s/it]\u001b[32m[I 2022-06-01 06:02:01,612]\u001b[0m Trial 55 finished with value: 0.05961835818115635 and parameters: {'lambda_l1': 0.0028395625220543202, 'lambda_l2': 5.090705168507331}. Best is trial 45 with value: 0.05960618568218905.\u001b[0m\n",
      "regularization_factors, val_score: 0.059606:  65%|######5   | 13/20 [2:53:48<2:54:55, 1499.41s/it]/home/kaggler/.local/lib/python3.8/site-packages/lightgbm/engine.py:181: UserWarning: 'early_stopping_rounds' argument is deprecated and will be removed in a future release of LightGBM. Pass 'early_stopping()' callback via 'callbacks' argument instead.\n",
      "  _log_warning(\"'early_stopping_rounds' argument is deprecated and will be removed in a future release of LightGBM. \"\n",
      "/home/kaggler/.local/lib/python3.8/site-packages/lightgbm/engine.py:239: UserWarning: 'verbose_eval' argument is deprecated and will be removed in a future release of LightGBM. Pass 'log_evaluation()' callback via 'callbacks' argument instead.\n",
      "  _log_warning(\"'verbose_eval' argument is deprecated and will be removed in a future release of LightGBM. \"\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[LightGBM] [Info] Number of positive: 572173, number of negative: 7626717\n",
      "[LightGBM] [Warning] Auto-choosing row-wise multi-threading, the overhead of testing was 0.247054 seconds.\n",
      "You can set `force_row_wise=true` to remove the overhead.\n",
      "And if memory is not enough, you can set `force_col_wise=true`.\n",
      "[LightGBM] [Info] Total Bins 7249\n",
      "[LightGBM] [Info] Number of data points in the train set: 8198890, number of used features: 48\n",
      "[LightGBM] [Info] [binary:BoostFromScore]: pavg=0.069787 -> initscore=-2.589971\n",
      "[LightGBM] [Info] Start training from score -2.589971\n",
      "Training until validation scores don't improve for 100 rounds\n",
      "[1000]\tvalid_0's binary_logloss: 0.0513106\tvalid_1's binary_logloss: 0.0692353\n",
      "[2000]\tvalid_0's binary_logloss: 0.0379455\tvalid_1's binary_logloss: 0.065371\n",
      "[3000]\tvalid_0's binary_logloss: 0.029377\tvalid_1's binary_logloss: 0.0631994\n",
      "[4000]\tvalid_0's binary_logloss: 0.0229343\tvalid_1's binary_logloss: 0.0616916\n",
      "[5000]\tvalid_0's binary_logloss: 0.018047\tvalid_1's binary_logloss: 0.0607375\n",
      "[6000]\tvalid_0's binary_logloss: 0.0142035\tvalid_1's binary_logloss: 0.060174\n",
      "Early stopping, best iteration is:\n",
      "[6664]\tvalid_0's binary_logloss: 0.0122147\tvalid_1's binary_logloss: 0.0599548\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "regularization_factors, val_score: 0.059606:  70%|#######   | 14/20 [3:17:34<2:27:44, 1477.42s/it]\u001b[32m[I 2022-06-01 06:25:48,232]\u001b[0m Trial 56 finished with value: 0.059954800533521264 and parameters: {'lambda_l1': 0.0029382728965806075, 'lambda_l2': 0.04507442974174255}. Best is trial 45 with value: 0.05960618568218905.\u001b[0m\n",
      "regularization_factors, val_score: 0.059606:  70%|#######   | 14/20 [3:17:34<2:27:44, 1477.42s/it]/home/kaggler/.local/lib/python3.8/site-packages/lightgbm/engine.py:181: UserWarning: 'early_stopping_rounds' argument is deprecated and will be removed in a future release of LightGBM. Pass 'early_stopping()' callback via 'callbacks' argument instead.\n",
      "  _log_warning(\"'early_stopping_rounds' argument is deprecated and will be removed in a future release of LightGBM. \"\n",
      "/home/kaggler/.local/lib/python3.8/site-packages/lightgbm/engine.py:239: UserWarning: 'verbose_eval' argument is deprecated and will be removed in a future release of LightGBM. Pass 'log_evaluation()' callback via 'callbacks' argument instead.\n",
      "  _log_warning(\"'verbose_eval' argument is deprecated and will be removed in a future release of LightGBM. \"\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[LightGBM] [Info] Number of positive: 572173, number of negative: 7626717\n",
      "[LightGBM] [Warning] Auto-choosing row-wise multi-threading, the overhead of testing was 0.230325 seconds.\n",
      "You can set `force_row_wise=true` to remove the overhead.\n",
      "And if memory is not enough, you can set `force_col_wise=true`.\n",
      "[LightGBM] [Info] Total Bins 7249\n",
      "[LightGBM] [Info] Number of data points in the train set: 8198890, number of used features: 48\n",
      "[LightGBM] [Info] [binary:BoostFromScore]: pavg=0.069787 -> initscore=-2.589971\n",
      "[LightGBM] [Info] Start training from score -2.589971\n",
      "Training until validation scores don't improve for 100 rounds\n",
      "[1000]\tvalid_0's binary_logloss: 0.052517\tvalid_1's binary_logloss: 0.0688873\n",
      "[2000]\tvalid_0's binary_logloss: 0.039654\tvalid_1's binary_logloss: 0.065432\n",
      "[3000]\tvalid_0's binary_logloss: 0.0307727\tvalid_1's binary_logloss: 0.0630593\n",
      "[4000]\tvalid_0's binary_logloss: 0.0242847\tvalid_1's binary_logloss: 0.061536\n",
      "[5000]\tvalid_0's binary_logloss: 0.0192855\tvalid_1's binary_logloss: 0.0604883\n",
      "[6000]\tvalid_0's binary_logloss: 0.0154738\tvalid_1's binary_logloss: 0.0598916\n",
      "Early stopping, best iteration is:\n",
      "[6667]\tvalid_0's binary_logloss: 0.013397\tvalid_1's binary_logloss: 0.0597295\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "regularization_factors, val_score: 0.059606:  75%|#######5  | 15/20 [3:41:26<2:01:57, 1463.59s/it]\u001b[32m[I 2022-06-01 06:49:39,753]\u001b[0m Trial 57 finished with value: 0.059729510961397364 and parameters: {'lambda_l1': 0.0012686873748042483, 'lambda_l2': 0.24617672094725088}. Best is trial 45 with value: 0.05960618568218905.\u001b[0m\n",
      "regularization_factors, val_score: 0.059606:  75%|#######5  | 15/20 [3:41:26<2:01:57, 1463.59s/it]/home/kaggler/.local/lib/python3.8/site-packages/lightgbm/engine.py:181: UserWarning: 'early_stopping_rounds' argument is deprecated and will be removed in a future release of LightGBM. Pass 'early_stopping()' callback via 'callbacks' argument instead.\n",
      "  _log_warning(\"'early_stopping_rounds' argument is deprecated and will be removed in a future release of LightGBM. \"\n",
      "/home/kaggler/.local/lib/python3.8/site-packages/lightgbm/engine.py:239: UserWarning: 'verbose_eval' argument is deprecated and will be removed in a future release of LightGBM. Pass 'log_evaluation()' callback via 'callbacks' argument instead.\n",
      "  _log_warning(\"'verbose_eval' argument is deprecated and will be removed in a future release of LightGBM. \"\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[LightGBM] [Info] Number of positive: 572173, number of negative: 7626717\n",
      "[LightGBM] [Warning] Auto-choosing row-wise multi-threading, the overhead of testing was 0.261204 seconds.\n",
      "You can set `force_row_wise=true` to remove the overhead.\n",
      "And if memory is not enough, you can set `force_col_wise=true`.\n",
      "[LightGBM] [Info] Total Bins 7249\n",
      "[LightGBM] [Info] Number of data points in the train set: 8198890, number of used features: 48\n",
      "[LightGBM] [Info] [binary:BoostFromScore]: pavg=0.069787 -> initscore=-2.589971\n",
      "[LightGBM] [Info] Start training from score -2.589971\n",
      "Training until validation scores don't improve for 100 rounds\n",
      "[1000]\tvalid_0's binary_logloss: 0.0544714\tvalid_1's binary_logloss: 0.0688173\n",
      "[2000]\tvalid_0's binary_logloss: 0.0430561\tvalid_1's binary_logloss: 0.0654423\n",
      "[3000]\tvalid_0's binary_logloss: 0.035227\tvalid_1's binary_logloss: 0.0632828\n",
      "[4000]\tvalid_0's binary_logloss: 0.0294397\tvalid_1's binary_logloss: 0.0619192\n",
      "[5000]\tvalid_0's binary_logloss: 0.0248022\tvalid_1's binary_logloss: 0.0609659\n",
      "[6000]\tvalid_0's binary_logloss: 0.0211444\tvalid_1's binary_logloss: 0.0602331\n",
      "[7000]\tvalid_0's binary_logloss: 0.0181866\tvalid_1's binary_logloss: 0.0597581\n",
      "[8000]\tvalid_0's binary_logloss: 0.0157712\tvalid_1's binary_logloss: 0.0595132\n",
      "[9000]\tvalid_0's binary_logloss: 0.013801\tvalid_1's binary_logloss: 0.059364\n",
      "Early stopping, best iteration is:\n",
      "[9378]\tvalid_0's binary_logloss: 0.0131775\tvalid_1's binary_logloss: 0.0593119\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "regularization_factors, val_score: 0.059312:  80%|########  | 16/20 [4:18:43<1:53:05, 1696.50s/it]\u001b[32m[I 2022-06-01 07:26:57,122]\u001b[0m Trial 58 finished with value: 0.059311939332030106 and parameters: {'lambda_l1': 2.9172430365759014, 'lambda_l2': 0.01259580952467805}. Best is trial 58 with value: 0.059311939332030106.\u001b[0m\n",
      "regularization_factors, val_score: 0.059312:  80%|########  | 16/20 [4:18:43<1:53:05, 1696.50s/it]/home/kaggler/.local/lib/python3.8/site-packages/lightgbm/engine.py:181: UserWarning: 'early_stopping_rounds' argument is deprecated and will be removed in a future release of LightGBM. Pass 'early_stopping()' callback via 'callbacks' argument instead.\n",
      "  _log_warning(\"'early_stopping_rounds' argument is deprecated and will be removed in a future release of LightGBM. \"\n",
      "/home/kaggler/.local/lib/python3.8/site-packages/lightgbm/engine.py:239: UserWarning: 'verbose_eval' argument is deprecated and will be removed in a future release of LightGBM. Pass 'log_evaluation()' callback via 'callbacks' argument instead.\n",
      "  _log_warning(\"'verbose_eval' argument is deprecated and will be removed in a future release of LightGBM. \"\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[LightGBM] [Info] Number of positive: 572173, number of negative: 7626717\n",
      "[LightGBM] [Warning] Auto-choosing row-wise multi-threading, the overhead of testing was 0.243397 seconds.\n",
      "You can set `force_row_wise=true` to remove the overhead.\n",
      "And if memory is not enough, you can set `force_col_wise=true`.\n",
      "[LightGBM] [Info] Total Bins 7249\n",
      "[LightGBM] [Info] Number of data points in the train set: 8198890, number of used features: 48\n",
      "[LightGBM] [Info] [binary:BoostFromScore]: pavg=0.069787 -> initscore=-2.589971\n",
      "[LightGBM] [Info] Start training from score -2.589971\n",
      "Training until validation scores don't improve for 100 rounds\n",
      "[1000]\tvalid_0's binary_logloss: 0.0584796\tvalid_1's binary_logloss: 0.0696836\n",
      "[2000]\tvalid_0's binary_logloss: 0.0488028\tvalid_1's binary_logloss: 0.0669009\n",
      "[3000]\tvalid_0's binary_logloss: 0.0419699\tvalid_1's binary_logloss: 0.0651371\n",
      "[4000]\tvalid_0's binary_logloss: 0.0366449\tvalid_1's binary_logloss: 0.0639009\n",
      "[5000]\tvalid_0's binary_logloss: 0.0324354\tvalid_1's binary_logloss: 0.0630133\n",
      "[6000]\tvalid_0's binary_logloss: 0.0289424\tvalid_1's binary_logloss: 0.0623267\n",
      "[7000]\tvalid_0's binary_logloss: 0.0260001\tvalid_1's binary_logloss: 0.0617985\n",
      "[8000]\tvalid_0's binary_logloss: 0.0236374\tvalid_1's binary_logloss: 0.0614757\n",
      "[9000]\tvalid_0's binary_logloss: 0.0215501\tvalid_1's binary_logloss: 0.0612242\n",
      "[10000]\tvalid_0's binary_logloss: 0.0197863\tvalid_1's binary_logloss: 0.0611034\n",
      "[11000]\tvalid_0's binary_logloss: 0.0182348\tvalid_1's binary_logloss: 0.0609992\n",
      "Early stopping, best iteration is:\n",
      "[11527]\tvalid_0's binary_logloss: 0.0175239\tvalid_1's binary_logloss: 0.0609382\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "regularization_factors, val_score: 0.059312:  85%|########5 | 17/20 [5:10:56<1:46:25, 2128.53s/it]\u001b[32m[I 2022-06-01 08:19:10,399]\u001b[0m Trial 59 finished with value: 0.06093815182646755 and parameters: {'lambda_l1': 8.543345545919639, 'lambda_l2': 0.00853479380302913}. Best is trial 58 with value: 0.059311939332030106.\u001b[0m\n",
      "regularization_factors, val_score: 0.059312:  85%|########5 | 17/20 [5:10:56<1:46:25, 2128.53s/it]/home/kaggler/.local/lib/python3.8/site-packages/lightgbm/engine.py:181: UserWarning: 'early_stopping_rounds' argument is deprecated and will be removed in a future release of LightGBM. Pass 'early_stopping()' callback via 'callbacks' argument instead.\n",
      "  _log_warning(\"'early_stopping_rounds' argument is deprecated and will be removed in a future release of LightGBM. \"\n",
      "/home/kaggler/.local/lib/python3.8/site-packages/lightgbm/engine.py:239: UserWarning: 'verbose_eval' argument is deprecated and will be removed in a future release of LightGBM. Pass 'log_evaluation()' callback via 'callbacks' argument instead.\n",
      "  _log_warning(\"'verbose_eval' argument is deprecated and will be removed in a future release of LightGBM. \"\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[LightGBM] [Info] Number of positive: 572173, number of negative: 7626717\n",
      "[LightGBM] [Warning] Auto-choosing row-wise multi-threading, the overhead of testing was 0.247601 seconds.\n",
      "You can set `force_row_wise=true` to remove the overhead.\n",
      "And if memory is not enough, you can set `force_col_wise=true`.\n",
      "[LightGBM] [Info] Total Bins 7249\n",
      "[LightGBM] [Info] Number of data points in the train set: 8198890, number of used features: 48\n",
      "[LightGBM] [Info] [binary:BoostFromScore]: pavg=0.069787 -> initscore=-2.589971\n",
      "[LightGBM] [Info] Start training from score -2.589971\n",
      "Training until validation scores don't improve for 100 rounds\n",
      "[1000]\tvalid_0's binary_logloss: 0.0536112\tvalid_1's binary_logloss: 0.0690183\n",
      "[2000]\tvalid_0's binary_logloss: 0.0411177\tvalid_1's binary_logloss: 0.0652774\n",
      "[3000]\tvalid_0's binary_logloss: 0.0324553\tvalid_1's binary_logloss: 0.0628265\n",
      "[4000]\tvalid_0's binary_logloss: 0.0259451\tvalid_1's binary_logloss: 0.0612831\n",
      "[5000]\tvalid_0's binary_logloss: 0.0208566\tvalid_1's binary_logloss: 0.0601571\n",
      "[6000]\tvalid_0's binary_logloss: 0.0169481\tvalid_1's binary_logloss: 0.0595078\n",
      "[7000]\tvalid_0's binary_logloss: 0.0139567\tvalid_1's binary_logloss: 0.0591166\n",
      "[8000]\tvalid_0's binary_logloss: 0.0115942\tvalid_1's binary_logloss: 0.0589375\n",
      "Early stopping, best iteration is:\n",
      "[7937]\tvalid_0's binary_logloss: 0.011731\tvalid_1's binary_logloss: 0.0589247\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "regularization_factors, val_score: 0.058925:  90%|######### | 18/20 [5:40:33<1:07:25, 2022.72s/it]\u001b[32m[I 2022-06-01 08:48:46,778]\u001b[0m Trial 60 finished with value: 0.05892470178004879 and parameters: {'lambda_l1': 0.5745709668124809, 'lambda_l2': 0.5123383865042099}. Best is trial 60 with value: 0.05892470178004879.\u001b[0m\n",
      "regularization_factors, val_score: 0.058925:  90%|######### | 18/20 [5:40:33<1:07:25, 2022.72s/it]/home/kaggler/.local/lib/python3.8/site-packages/lightgbm/engine.py:181: UserWarning: 'early_stopping_rounds' argument is deprecated and will be removed in a future release of LightGBM. Pass 'early_stopping()' callback via 'callbacks' argument instead.\n",
      "  _log_warning(\"'early_stopping_rounds' argument is deprecated and will be removed in a future release of LightGBM. \"\n",
      "/home/kaggler/.local/lib/python3.8/site-packages/lightgbm/engine.py:239: UserWarning: 'verbose_eval' argument is deprecated and will be removed in a future release of LightGBM. Pass 'log_evaluation()' callback via 'callbacks' argument instead.\n",
      "  _log_warning(\"'verbose_eval' argument is deprecated and will be removed in a future release of LightGBM. \"\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[LightGBM] [Info] Number of positive: 572173, number of negative: 7626717\n",
      "[LightGBM] [Warning] Auto-choosing row-wise multi-threading, the overhead of testing was 0.243630 seconds.\n",
      "You can set `force_row_wise=true` to remove the overhead.\n",
      "And if memory is not enough, you can set `force_col_wise=true`.\n",
      "[LightGBM] [Info] Total Bins 7249\n",
      "[LightGBM] [Info] Number of data points in the train set: 8198890, number of used features: 48\n",
      "[LightGBM] [Info] [binary:BoostFromScore]: pavg=0.069787 -> initscore=-2.589971\n",
      "[LightGBM] [Info] Start training from score -2.589971\n",
      "Training until validation scores don't improve for 100 rounds\n",
      "[1000]\tvalid_0's binary_logloss: 0.0508041\tvalid_1's binary_logloss: 0.0692068\n",
      "[2000]\tvalid_0's binary_logloss: 0.0380154\tvalid_1's binary_logloss: 0.0654248\n",
      "[3000]\tvalid_0's binary_logloss: 0.0294242\tvalid_1's binary_logloss: 0.0631113\n",
      "[4000]\tvalid_0's binary_logloss: 0.0231982\tvalid_1's binary_logloss: 0.061532\n",
      "[5000]\tvalid_0's binary_logloss: 0.0185345\tvalid_1's binary_logloss: 0.0604956\n",
      "[6000]\tvalid_0's binary_logloss: 0.0149428\tvalid_1's binary_logloss: 0.0598725\n",
      "[7000]\tvalid_0's binary_logloss: 0.0121781\tvalid_1's binary_logloss: 0.0595658\n",
      "[8000]\tvalid_0's binary_logloss: 0.00997947\tvalid_1's binary_logloss: 0.0594393\n",
      "Early stopping, best iteration is:\n",
      "[7922]\tvalid_0's binary_logloss: 0.0101304\tvalid_1's binary_logloss: 0.0594305\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "regularization_factors, val_score: 0.058925:  95%|#########5| 19/20 [6:09:52<32:23, 1943.54s/it]  \u001b[32m[I 2022-06-01 09:18:05,886]\u001b[0m Trial 61 finished with value: 0.05943045984145651 and parameters: {'lambda_l1': 0.5892083038419742, 'lambda_l2': 0.015404901170337009}. Best is trial 60 with value: 0.05892470178004879.\u001b[0m\n",
      "regularization_factors, val_score: 0.058925:  95%|#########5| 19/20 [6:09:52<32:23, 1943.54s/it]/home/kaggler/.local/lib/python3.8/site-packages/lightgbm/engine.py:181: UserWarning: 'early_stopping_rounds' argument is deprecated and will be removed in a future release of LightGBM. Pass 'early_stopping()' callback via 'callbacks' argument instead.\n",
      "  _log_warning(\"'early_stopping_rounds' argument is deprecated and will be removed in a future release of LightGBM. \"\n",
      "/home/kaggler/.local/lib/python3.8/site-packages/lightgbm/engine.py:239: UserWarning: 'verbose_eval' argument is deprecated and will be removed in a future release of LightGBM. Pass 'log_evaluation()' callback via 'callbacks' argument instead.\n",
      "  _log_warning(\"'verbose_eval' argument is deprecated and will be removed in a future release of LightGBM. \"\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[LightGBM] [Info] Number of positive: 572173, number of negative: 7626717\n",
      "[LightGBM] [Warning] Auto-choosing row-wise multi-threading, the overhead of testing was 0.243395 seconds.\n",
      "You can set `force_row_wise=true` to remove the overhead.\n",
      "And if memory is not enough, you can set `force_col_wise=true`.\n",
      "[LightGBM] [Info] Total Bins 7249\n",
      "[LightGBM] [Info] Number of data points in the train set: 8198890, number of used features: 48\n",
      "[LightGBM] [Info] [binary:BoostFromScore]: pavg=0.069787 -> initscore=-2.589971\n",
      "[LightGBM] [Info] Start training from score -2.589971\n",
      "Training until validation scores don't improve for 100 rounds\n",
      "[1000]\tvalid_0's binary_logloss: 0.0536029\tvalid_1's binary_logloss: 0.0688069\n",
      "[2000]\tvalid_0's binary_logloss: 0.0414618\tvalid_1's binary_logloss: 0.065332\n",
      "[3000]\tvalid_0's binary_logloss: 0.0326786\tvalid_1's binary_logloss: 0.0629905\n",
      "[4000]\tvalid_0's binary_logloss: 0.026126\tvalid_1's binary_logloss: 0.0612858\n",
      "[5000]\tvalid_0's binary_logloss: 0.0211687\tvalid_1's binary_logloss: 0.060205\n",
      "[6000]\tvalid_0's binary_logloss: 0.0172543\tvalid_1's binary_logloss: 0.0594676\n",
      "[7000]\tvalid_0's binary_logloss: 0.0141848\tvalid_1's binary_logloss: 0.0590655\n",
      "Early stopping, best iteration is:\n",
      "[7648]\tvalid_0's binary_logloss: 0.0124883\tvalid_1's binary_logloss: 0.0589247\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "regularization_factors, val_score: 0.058925: 100%|##########| 20/20 [6:38:11<00:00, 1870.19s/it]\u001b[32m[I 2022-06-01 09:46:25,100]\u001b[0m Trial 62 finished with value: 0.05892472995671531 and parameters: {'lambda_l1': 0.557317670663002, 'lambda_l2': 0.6008175403293449}. Best is trial 60 with value: 0.05892470178004879.\u001b[0m\n",
      "regularization_factors, val_score: 0.058925: 100%|##########| 20/20 [6:38:11<00:00, 1194.58s/it]\n",
      "min_data_in_leaf, val_score: 0.058925:   0%|          | 0/5 [00:00<?, ?it/s]/home/kaggler/.local/lib/python3.8/site-packages/lightgbm/engine.py:181: UserWarning: 'early_stopping_rounds' argument is deprecated and will be removed in a future release of LightGBM. Pass 'early_stopping()' callback via 'callbacks' argument instead.\n",
      "  _log_warning(\"'early_stopping_rounds' argument is deprecated and will be removed in a future release of LightGBM. \"\n",
      "/home/kaggler/.local/lib/python3.8/site-packages/lightgbm/engine.py:239: UserWarning: 'verbose_eval' argument is deprecated and will be removed in a future release of LightGBM. Pass 'log_evaluation()' callback via 'callbacks' argument instead.\n",
      "  _log_warning(\"'verbose_eval' argument is deprecated and will be removed in a future release of LightGBM. \"\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[LightGBM] [Info] Number of positive: 572173, number of negative: 7626717\n",
      "[LightGBM] [Warning] Auto-choosing row-wise multi-threading, the overhead of testing was 0.231396 seconds.\n",
      "You can set `force_row_wise=true` to remove the overhead.\n",
      "And if memory is not enough, you can set `force_col_wise=true`.\n",
      "[LightGBM] [Info] Total Bins 7249\n",
      "[LightGBM] [Info] Number of data points in the train set: 8198890, number of used features: 48\n",
      "[LightGBM] [Info] [binary:BoostFromScore]: pavg=0.069787 -> initscore=-2.589971\n",
      "[LightGBM] [Info] Start training from score -2.589971\n",
      "Training until validation scores don't improve for 100 rounds\n",
      "[1000]\tvalid_0's binary_logloss: 0.0535003\tvalid_1's binary_logloss: 0.0688823\n",
      "[2000]\tvalid_0's binary_logloss: 0.0411286\tvalid_1's binary_logloss: 0.065249\n",
      "[3000]\tvalid_0's binary_logloss: 0.0325386\tvalid_1's binary_logloss: 0.0629938\n",
      "[4000]\tvalid_0's binary_logloss: 0.0260302\tvalid_1's binary_logloss: 0.0613767\n",
      "[5000]\tvalid_0's binary_logloss: 0.0210227\tvalid_1's binary_logloss: 0.0603415\n",
      "[6000]\tvalid_0's binary_logloss: 0.0170592\tvalid_1's binary_logloss: 0.0596055\n",
      "[7000]\tvalid_0's binary_logloss: 0.0140004\tvalid_1's binary_logloss: 0.0591948\n",
      "Early stopping, best iteration is:\n",
      "[7796]\tvalid_0's binary_logloss: 0.0120478\tvalid_1's binary_logloss: 0.0590133\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "min_data_in_leaf, val_score: 0.058925:  20%|##        | 1/5 [26:53<1:47:34, 1613.57s/it]\u001b[32m[I 2022-06-01 10:13:18,685]\u001b[0m Trial 63 finished with value: 0.059013319617830504 and parameters: {'min_child_samples': 25}. Best is trial 63 with value: 0.059013319617830504.\u001b[0m\n",
      "min_data_in_leaf, val_score: 0.058925:  20%|##        | 1/5 [26:53<1:47:34, 1613.57s/it]/home/kaggler/.local/lib/python3.8/site-packages/lightgbm/engine.py:181: UserWarning: 'early_stopping_rounds' argument is deprecated and will be removed in a future release of LightGBM. Pass 'early_stopping()' callback via 'callbacks' argument instead.\n",
      "  _log_warning(\"'early_stopping_rounds' argument is deprecated and will be removed in a future release of LightGBM. \"\n",
      "/home/kaggler/.local/lib/python3.8/site-packages/lightgbm/engine.py:239: UserWarning: 'verbose_eval' argument is deprecated and will be removed in a future release of LightGBM. Pass 'log_evaluation()' callback via 'callbacks' argument instead.\n",
      "  _log_warning(\"'verbose_eval' argument is deprecated and will be removed in a future release of LightGBM. \"\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[LightGBM] [Info] Number of positive: 572173, number of negative: 7626717\n",
      "[LightGBM] [Warning] Auto-choosing row-wise multi-threading, the overhead of testing was 0.414936 seconds.\n",
      "You can set `force_row_wise=true` to remove the overhead.\n",
      "And if memory is not enough, you can set `force_col_wise=true`.\n",
      "[LightGBM] [Info] Total Bins 7249\n",
      "[LightGBM] [Info] Number of data points in the train set: 8198890, number of used features: 48\n",
      "[LightGBM] [Info] [binary:BoostFromScore]: pavg=0.069787 -> initscore=-2.589971\n",
      "[LightGBM] [Info] Start training from score -2.589971\n",
      "Training until validation scores don't improve for 100 rounds\n",
      "[1000]\tvalid_0's binary_logloss: 0.053647\tvalid_1's binary_logloss: 0.0688798\n",
      "[2000]\tvalid_0's binary_logloss: 0.0413199\tvalid_1's binary_logloss: 0.0652787\n",
      "[3000]\tvalid_0's binary_logloss: 0.0325075\tvalid_1's binary_logloss: 0.0629612\n",
      "[4000]\tvalid_0's binary_logloss: 0.026031\tvalid_1's binary_logloss: 0.0612727\n",
      "[5000]\tvalid_0's binary_logloss: 0.0210014\tvalid_1's binary_logloss: 0.0602376\n",
      "[6000]\tvalid_0's binary_logloss: 0.0170702\tvalid_1's binary_logloss: 0.0595255\n",
      "[7000]\tvalid_0's binary_logloss: 0.0140295\tvalid_1's binary_logloss: 0.059073\n",
      "Early stopping, best iteration is:\n",
      "[7848]\tvalid_0's binary_logloss: 0.0119482\tvalid_1's binary_logloss: 0.0589195\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "min_data_in_leaf, val_score: 0.058919:  40%|####      | 2/5 [53:57<1:20:58, 1619.42s/it]\u001b[32m[I 2022-06-01 10:40:22,187]\u001b[0m Trial 64 finished with value: 0.05891947114051528 and parameters: {'min_child_samples': 10}. Best is trial 64 with value: 0.05891947114051528.\u001b[0m\n",
      "min_data_in_leaf, val_score: 0.058919:  40%|####      | 2/5 [53:57<1:20:58, 1619.42s/it]/home/kaggler/.local/lib/python3.8/site-packages/lightgbm/engine.py:181: UserWarning: 'early_stopping_rounds' argument is deprecated and will be removed in a future release of LightGBM. Pass 'early_stopping()' callback via 'callbacks' argument instead.\n",
      "  _log_warning(\"'early_stopping_rounds' argument is deprecated and will be removed in a future release of LightGBM. \"\n",
      "/home/kaggler/.local/lib/python3.8/site-packages/lightgbm/engine.py:239: UserWarning: 'verbose_eval' argument is deprecated and will be removed in a future release of LightGBM. Pass 'log_evaluation()' callback via 'callbacks' argument instead.\n",
      "  _log_warning(\"'verbose_eval' argument is deprecated and will be removed in a future release of LightGBM. \"\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[LightGBM] [Info] Number of positive: 572173, number of negative: 7626717\n",
      "[LightGBM] [Warning] Auto-choosing row-wise multi-threading, the overhead of testing was 0.243140 seconds.\n",
      "You can set `force_row_wise=true` to remove the overhead.\n",
      "And if memory is not enough, you can set `force_col_wise=true`.\n",
      "[LightGBM] [Info] Total Bins 7249\n",
      "[LightGBM] [Info] Number of data points in the train set: 8198890, number of used features: 48\n",
      "[LightGBM] [Info] [binary:BoostFromScore]: pavg=0.069787 -> initscore=-2.589971\n",
      "[LightGBM] [Info] Start training from score -2.589971\n",
      "Training until validation scores don't improve for 100 rounds\n",
      "[1000]\tvalid_0's binary_logloss: 0.0538167\tvalid_1's binary_logloss: 0.0689403\n",
      "[2000]\tvalid_0's binary_logloss: 0.0413371\tvalid_1's binary_logloss: 0.065295\n",
      "[3000]\tvalid_0's binary_logloss: 0.0329049\tvalid_1's binary_logloss: 0.0629714\n",
      "[4000]\tvalid_0's binary_logloss: 0.0264393\tvalid_1's binary_logloss: 0.0615015\n",
      "[5000]\tvalid_0's binary_logloss: 0.0214151\tvalid_1's binary_logloss: 0.0604053\n",
      "[6000]\tvalid_0's binary_logloss: 0.0175273\tvalid_1's binary_logloss: 0.0597913\n",
      "[7000]\tvalid_0's binary_logloss: 0.0144219\tvalid_1's binary_logloss: 0.0594473\n",
      "Early stopping, best iteration is:\n",
      "[7339]\tvalid_0's binary_logloss: 0.0135339\tvalid_1's binary_logloss: 0.0593419\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "min_data_in_leaf, val_score: 0.058919:  60%|######    | 3/5 [1:19:34<52:44, 1582.00s/it]  \u001b[32m[I 2022-06-01 11:05:59,675]\u001b[0m Trial 65 finished with value: 0.059341881889784565 and parameters: {'min_child_samples': 100}. Best is trial 64 with value: 0.05891947114051528.\u001b[0m\n",
      "min_data_in_leaf, val_score: 0.058919:  60%|######    | 3/5 [1:19:34<52:44, 1582.00s/it]/home/kaggler/.local/lib/python3.8/site-packages/lightgbm/engine.py:181: UserWarning: 'early_stopping_rounds' argument is deprecated and will be removed in a future release of LightGBM. Pass 'early_stopping()' callback via 'callbacks' argument instead.\n",
      "  _log_warning(\"'early_stopping_rounds' argument is deprecated and will be removed in a future release of LightGBM. \"\n",
      "/home/kaggler/.local/lib/python3.8/site-packages/lightgbm/engine.py:239: UserWarning: 'verbose_eval' argument is deprecated and will be removed in a future release of LightGBM. Pass 'log_evaluation()' callback via 'callbacks' argument instead.\n",
      "  _log_warning(\"'verbose_eval' argument is deprecated and will be removed in a future release of LightGBM. \"\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[LightGBM] [Info] Number of positive: 572173, number of negative: 7626717\n",
      "[LightGBM] [Warning] Auto-choosing row-wise multi-threading, the overhead of testing was 0.252944 seconds.\n",
      "You can set `force_row_wise=true` to remove the overhead.\n",
      "And if memory is not enough, you can set `force_col_wise=true`.\n",
      "[LightGBM] [Info] Total Bins 7249\n",
      "[LightGBM] [Info] Number of data points in the train set: 8198890, number of used features: 48\n",
      "[LightGBM] [Info] [binary:BoostFromScore]: pavg=0.069787 -> initscore=-2.589971\n",
      "[LightGBM] [Info] Start training from score -2.589971\n",
      "Training until validation scores don't improve for 100 rounds\n",
      "[1000]\tvalid_0's binary_logloss: 0.0536334\tvalid_1's binary_logloss: 0.0689871\n",
      "[2000]\tvalid_0's binary_logloss: 0.0410906\tvalid_1's binary_logloss: 0.0653699\n",
      "[3000]\tvalid_0's binary_logloss: 0.0324979\tvalid_1's binary_logloss: 0.0629025\n",
      "[4000]\tvalid_0's binary_logloss: 0.0260475\tvalid_1's binary_logloss: 0.0614182\n",
      "[5000]\tvalid_0's binary_logloss: 0.0210445\tvalid_1's binary_logloss: 0.0603183\n",
      "[6000]\tvalid_0's binary_logloss: 0.0171145\tvalid_1's binary_logloss: 0.0596606\n",
      "[7000]\tvalid_0's binary_logloss: 0.0140789\tvalid_1's binary_logloss: 0.059254\n",
      "[8000]\tvalid_0's binary_logloss: 0.0116081\tvalid_1's binary_logloss: 0.0590505\n",
      "Early stopping, best iteration is:\n",
      "[8002]\tvalid_0's binary_logloss: 0.0116019\tvalid_1's binary_logloss: 0.0590482\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "min_data_in_leaf, val_score: 0.058919:  80%|########  | 4/5 [1:47:56<27:09, 1629.27s/it]\u001b[32m[I 2022-06-01 11:34:21,411]\u001b[0m Trial 66 finished with value: 0.05904816193246766 and parameters: {'min_child_samples': 50}. Best is trial 64 with value: 0.05891947114051528.\u001b[0m\n",
      "min_data_in_leaf, val_score: 0.058919:  80%|########  | 4/5 [1:47:56<27:09, 1629.27s/it]/home/kaggler/.local/lib/python3.8/site-packages/lightgbm/engine.py:181: UserWarning: 'early_stopping_rounds' argument is deprecated and will be removed in a future release of LightGBM. Pass 'early_stopping()' callback via 'callbacks' argument instead.\n",
      "  _log_warning(\"'early_stopping_rounds' argument is deprecated and will be removed in a future release of LightGBM. \"\n",
      "/home/kaggler/.local/lib/python3.8/site-packages/lightgbm/engine.py:239: UserWarning: 'verbose_eval' argument is deprecated and will be removed in a future release of LightGBM. Pass 'log_evaluation()' callback via 'callbacks' argument instead.\n",
      "  _log_warning(\"'verbose_eval' argument is deprecated and will be removed in a future release of LightGBM. \"\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[LightGBM] [Info] Number of positive: 572173, number of negative: 7626717\n",
      "[LightGBM] [Warning] Auto-choosing row-wise multi-threading, the overhead of testing was 0.238754 seconds.\n",
      "You can set `force_row_wise=true` to remove the overhead.\n",
      "And if memory is not enough, you can set `force_col_wise=true`.\n",
      "[LightGBM] [Info] Total Bins 7249\n",
      "[LightGBM] [Info] Number of data points in the train set: 8198890, number of used features: 48\n",
      "[LightGBM] [Info] [binary:BoostFromScore]: pavg=0.069787 -> initscore=-2.589971\n",
      "[LightGBM] [Info] Start training from score -2.589971\n",
      "Training until validation scores don't improve for 100 rounds\n",
      "[1000]\tvalid_0's binary_logloss: 0.0535656\tvalid_1's binary_logloss: 0.0689529\n",
      "[2000]\tvalid_0's binary_logloss: 0.0410048\tvalid_1's binary_logloss: 0.0653877\n",
      "[3000]\tvalid_0's binary_logloss: 0.0326255\tvalid_1's binary_logloss: 0.0630991\n",
      "[4000]\tvalid_0's binary_logloss: 0.026076\tvalid_1's binary_logloss: 0.0614398\n",
      "[5000]\tvalid_0's binary_logloss: 0.0209884\tvalid_1's binary_logloss: 0.0602763\n",
      "[6000]\tvalid_0's binary_logloss: 0.017075\tvalid_1's binary_logloss: 0.0595389\n",
      "[7000]\tvalid_0's binary_logloss: 0.0140207\tvalid_1's binary_logloss: 0.0590957\n",
      "[8000]\tvalid_0's binary_logloss: 0.0116259\tvalid_1's binary_logloss: 0.0588608\n",
      "Early stopping, best iteration is:\n",
      "[8160]\tvalid_0's binary_logloss: 0.0113074\tvalid_1's binary_logloss: 0.0588265\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "min_data_in_leaf, val_score: 0.058827: 100%|##########| 5/5 [2:18:23<00:00, 1700.75s/it]\u001b[32m[I 2022-06-01 12:04:48,894]\u001b[0m Trial 67 finished with value: 0.058826521802674524 and parameters: {'min_child_samples': 5}. Best is trial 67 with value: 0.058826521802674524.\u001b[0m\n",
      "min_data_in_leaf, val_score: 0.058827: 100%|##########| 5/5 [2:18:23<00:00, 1660.76s/it]\n"
     ]
    }
   ],
   "source": [
    "train_origin = pd.read_csv(INPUT_DIR + \"train.csv\")\n",
    "train_origin = preprocess(train_origin)\n",
    "\n",
    "# trainデータの分割\n",
    "kf = GroupKFold(n_splits=2)\n",
    "for i, (trn_idx, val_idx) in enumerate(kf.split(train_origin, train_origin['point_of_interest'], train_origin['point_of_interest'])):\n",
    "    train_origin.loc[val_idx, \"set\"] = i\n",
    "\n",
    "# 1st stage\n",
    "train = pd.concat([\n",
    "    extract_candidate(train_origin[train_origin[\"set\"]==0]), \n",
    "    extract_candidate(train_origin[train_origin[\"set\"]==1]), \n",
    "])\n",
    "train = add_orgin_data(train, train_origin)\n",
    "stage1_max_score = calc_max_score(train, train_origin)\n",
    "\n",
    "# 2nd stage\n",
    "# create target\n",
    "train['target'] = (train['point_of_interest'] == train['match_point_of_interest']).values.astype(int)\n",
    "train[\"target\"] = train[\"target\"].fillna(0)\n",
    "\n",
    "train = add_distance_features(train)\n",
    "\n",
    "not_use_cols = ['match_state_leven', 'address', 'categories', 'point_of_interest', 'match_address_leven',\n",
    "                'city', 'match_point_of_interest', 'match_name', 'match_categories_leven', 'match_longitude',\n",
    "                'target', 'match_city_leven', 'zip', 'match_categories', 'match_city', 'match_latitude',\n",
    "                'match_zip', 'match_url', 'id', 'match_set', 'country', 'match_state', 'match_address',\n",
    "                'match_name_leven', 'match_id', 'latitude', 'url', 'set', 'name', 'phone', 'longitude',\n",
    "                'match_url_leven', 'state', 'match_phone', 'match_country']\n",
    "                \n",
    "features = [c for c in train.columns if c not in not_use_cols]\n",
    "with open(OUTPUT_DIR + f'{exp_name}/features.pickle', 'wb') as f:\n",
    "    pickle.dump(features, f)\n",
    "\n",
    "train = reduce_data_size(train, features)\n",
    "\n",
    "params = {\n",
    "     'objective': 'binary',\n",
    "     'boosting': 'gbdt',\n",
    "     'learning_rate': 0.1,\n",
    "     'metric': 'binary_logloss',\n",
    "     'seed': SEED\n",
    "    }\n",
    "\n",
    "# split folds\n",
    "kf = StratifiedKFold(n_splits=N_SPLITS, shuffle=True, random_state=SEED)\n",
    "for i, (trn_idx, val_idx) in enumerate(kf.split(train, train[\"target\"], train[\"target\"])):\n",
    "    train.loc[val_idx, \"fold\"] = i\n",
    "\n",
    "for i in range(N_SPLITS):\n",
    "    print('fold : ' + str(i))\n",
    "    tr_idx = train[train['fold'] != i].index\n",
    "    vl_idx = train[train['fold'] == i].index\n",
    "    tr_x, tr_y = train.loc[tr_idx, features], train.loc[tr_idx, 'target']\n",
    "    vl_x, vl_y = train.loc[vl_idx, features], train.loc[vl_idx, 'target']\n",
    "    tr_data = lgb.Dataset(tr_x, label=tr_y)\n",
    "    vl_data = lgb.Dataset(vl_x, label=vl_y)\n",
    "    break\n",
    "\n",
    "model = optuna_lgb.train(params, tr_data, valid_sets=[tr_data, vl_data],\n",
    "                         num_boost_round=20000, early_stopping_rounds=100, verbose_eval=1000)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 58,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'objective': 'binary', 'boosting': 'gbdt', 'learning_rate': 0.1, 'metric': 'binary_logloss', 'seed': 42, 'feature_pre_filter': False, 'lambda_l1': 0.5745709668124809, 'lambda_l2': 0.5123383865042099, 'num_leaves': 239, 'feature_fraction': 0.784, 'bagging_fraction': 1.0, 'bagging_freq': 0, 'min_child_samples': 5, 'num_iterations': 20000, 'early_stopping_round': 100}\n"
     ]
    }
   ],
   "source": [
    "print(model.params)"
   ]
  }
 ],
 "metadata": {
  "interpreter": {
   "hash": "d4d1e4263499bec80672ea0156c357c1ee493ec2b1c70f0acce89fc37c4a6abe"
  },
  "kernelspec": {
   "display_name": "Python 3.8.12 ('base')",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.12"
  },
  "orig_nbformat": 4
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
