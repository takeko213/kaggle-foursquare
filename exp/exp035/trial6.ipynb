{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# exp034_trial\n",
    "tfidf類似度計算の高速化"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "metadata": {},
   "outputs": [],
   "source": [
    "MODE = 'local_train'\n",
    "#MODE = 'kaggle_inference'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "metadata": {},
   "outputs": [],
   "source": [
    "exp_name = 'exp032'\n",
    "memo = 'exp030 dist10+tfidf_name_sim10の1st'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "The Cython extension is already loaded. To reload it, use:\n",
      "  %reload_ext Cython\n"
     ]
    }
   ],
   "source": [
    "import os\n",
    "import sys\n",
    "import gc\n",
    "\n",
    "if MODE == 'local_train':\n",
    "    sys.path.append('/home/kaggler/.local/lib/python3.8/site-packages')\n",
    "    from dotenv import load_dotenv\n",
    "    load_dotenv\n",
    "    sys.path.append(os.getenv('UTILS_PATH'))\n",
    "    import line_notify\n",
    "    import slack_notify\n",
    "    \n",
    "if MODE == \"kaggle_inference\":\n",
    "    from cuml import ForestInference\n",
    "    import treelite\n",
    "    \n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt; plt.style.use(\"ggplot\")\n",
    "import seaborn as sns\n",
    "from sklearn.metrics.pairwise import haversine_distances\n",
    "from sklearn.model_selection import GroupKFold, KFold, StratifiedKFold\n",
    "from sklearn.metrics import accuracy_score\n",
    "from sklearn.decomposition import TruncatedSVD\n",
    "import lightgbm as lgb\n",
    "import itertools\n",
    "from scipy.spatial.distance import canberra\n",
    "from sklearn.neighbors import KNeighborsRegressor\n",
    "import functools\n",
    "import multiprocessing\n",
    "import Levenshtein\n",
    "import difflib\n",
    "import pickle\n",
    "from tqdm import tqdm\n",
    "%load_ext Cython\n",
    "\n",
    "from transformers import DistilBertModel, DistilBertTokenizer\n",
    "from torch.utils.data import DataLoader, Dataset\n",
    "import torch.nn.functional as F\n",
    "import torch.nn as nn\n",
    "import torch\n",
    "\n",
    "from sklearn.metrics.pairwise import cosine_similarity\n",
    "from sklearn.feature_extraction.text import TfidfVectorizer\n",
    "\n",
    "from cuml.feature_extraction.text import TfidfVectorizer as TfidfVectorizer_gpu\n",
    "import cudf, cuml, cupy\n",
    "from cuml.neighbors import NearestNeighbors as NearestNeighbors_gpu"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 42,
   "metadata": {},
   "outputs": [],
   "source": [
    "# directry_setting\n",
    "if MODE == 'local_train':\n",
    "    INPUT_DIR = os.getenv('INPUT_DIR')\n",
    "    OUTPUT_DIR = os.getenv('OUTPUT_DIR')\n",
    "    MODEL_DIR = os.getenv('OUTPUT_DIR')\n",
    "    BERT_MODEL = \"distilbert-base-multilingual-cased\"\n",
    "    os.makedirs(OUTPUT_DIR + exp_name, exist_ok=True)\n",
    "\n",
    "elif MODE == 'kaggle_inference':\n",
    "    INPUT_DIR = '/kaggle/input/foursquare-location-matching/'\n",
    "    OUTPUT_DIR = './'\n",
    "    MODEL_DIR = f'../input/fs{exp_name}/'\n",
    "    BERT_MODEL = \"../input/distilbertbaseuncased\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 43,
   "metadata": {},
   "outputs": [],
   "source": [
    "# CONFIG\n",
    "SEED = 42\n",
    "N_NEIGHBORS = 10\n",
    "N_SPLITS = 5\n",
    "PROB_TH = 0.5\n",
    "MAX_LEN = 32\n",
    "BS = 512\n",
    "NW = 2\n",
    "SVD_N_COMP = 50"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 44,
   "metadata": {},
   "outputs": [],
   "source": [
    "class Cat2VecModel(nn.Module):\n",
    "    def __init__(self):\n",
    "        super(Cat2VecModel, self).__init__()\n",
    "        self.distill_bert = DistilBertModel.from_pretrained(BERT_MODEL)\n",
    "        \n",
    "    def forward(self, ids, mask):\n",
    "        x = self.distill_bert(ids, mask)[0]\n",
    "        x = F.normalize((x[:, 1:, :]*mask[:, 1:, None]).mean(axis=1))\n",
    "        return x\n",
    "\n",
    "class InferenceDataset(Dataset):\n",
    "    \n",
    "    def __init__(self, df, max_len, col):\n",
    "        super().__init__()\n",
    "        self.df = df.reset_index(drop=True)\n",
    "        self.max_len = max_len\n",
    "        self.tokenizer = DistilBertTokenizer.from_pretrained(BERT_MODEL, do_lower_case=True)\n",
    "        self.col = col\n",
    "\n",
    "    def __getitem__(self, index):\n",
    "        row = self.df.iloc[index]\n",
    "        \n",
    "        inputs = self.tokenizer.encode_plus(\n",
    "            row[self.col],\n",
    "            None,\n",
    "            add_special_tokens=True,\n",
    "            max_length=MAX_LEN,\n",
    "            padding=\"max_length\",\n",
    "            return_token_type_ids=True,\n",
    "            truncation=True\n",
    "        )\n",
    "        ids = torch.LongTensor(inputs['input_ids'])\n",
    "        mask = torch.LongTensor(inputs['attention_mask'])\n",
    "\n",
    "        return ids, mask\n",
    "\n",
    "    def __len__(self):\n",
    "        return self.df.shape[0]\n",
    "\n",
    "def inference(ds):\n",
    "    cat2vec_model = Cat2VecModel()\n",
    "    cat2vec_model = cat2vec_model.cuda()\n",
    "    \n",
    "    loader = DataLoader(ds, batch_size=BS, shuffle=False, num_workers=NW,\n",
    "                        pin_memory=False, drop_last=False)\n",
    "    \n",
    "    vs = []\n",
    "    with torch.no_grad():\n",
    "        for idx, (ids, masks) in enumerate(loader):\n",
    "            v = cat2vec_model(ids.cuda(), masks.cuda()).detach().cpu().numpy()\n",
    "            vs.append(v)\n",
    "    return np.concatenate(vs)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 45,
   "metadata": {},
   "outputs": [],
   "source": [
    "def make_bert_vec(df, col):\n",
    "    cat_df = df[[col]].drop_duplicates()\n",
    "    cat_df[col] = cat_df[col].fillna(\"null\")\n",
    "\n",
    "    cat_ds = InferenceDataset(cat_df, max_len=MAX_LEN, col=col)\n",
    "    V = inference(cat_ds)\n",
    "    #svd = TruncatedSVD(n_components=SVD_N_COMP, random_state=SEED)\n",
    "    #V = svd.fit_transform(V)\n",
    "    V = V.astype(\"float16\")\n",
    "    bert_vec = {k:v for k,v in zip(cat_df[col].values, V)}\n",
    "    return bert_vec"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 46,
   "metadata": {},
   "outputs": [],
   "source": [
    "def preprocess(df):\n",
    "    columns = ['id', 'name', 'address', 'city', 'state',\n",
    "        'zip', 'country', 'url', 'phone', 'categories']\n",
    "    for c in columns:\n",
    "        if c != \"id\":\n",
    "            df[c] = df[c].astype(str).str.lower()\n",
    "\n",
    "    df[[\"latitude\", \"longitude\"]] = np.deg2rad(df[[\"latitude\", \"longitude\"]])\n",
    "    \n",
    "    return df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 47,
   "metadata": {},
   "outputs": [],
   "source": [
    "def extract_candidate_dist(df):\n",
    "    dfs = []\n",
    "    for country, country_df in tqdm(df.groupby(\"country\")):\n",
    "        country_df = country_df.reset_index(drop=True)\n",
    "        \n",
    "        knn = KNeighborsRegressor(n_neighbors=min(len(country_df), N_NEIGHBORS), \n",
    "                                    metric='haversine', n_jobs=-1)\n",
    "        knn.fit(country_df[['latitude','longitude']], country_df.index)\n",
    "        nears = knn.kneighbors(country_df[['latitude','longitude']], return_distance=False)\n",
    "        \n",
    "        k = min(len(country_df), N_NEIGHBORS)\n",
    "        country_df['match_id'] = country_df['id'].values[nears[:, :k]].tolist()\n",
    "        country_df = country_df.explode(['match_id'])\n",
    "        country_df = country_df.loc[country_df['id'] != country_df['match_id']].copy()\n",
    "        dfs.append(country_df)\n",
    "    df = pd.concat(dfs).reset_index(drop=True)\n",
    "    return df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 48,
   "metadata": {},
   "outputs": [],
   "source": [
    "def extract_candidate_tfidf_sim(df, col):\n",
    "    dfs = []\n",
    "    for country, country_df in tqdm(df.groupby(\"country\")):\n",
    "        country_df = country_df[country_df[col]!=\"nan\"].copy()\n",
    "        if len(country_df) < 2:\n",
    "            continue\n",
    "\n",
    "        country_df = country_df.reset_index(drop=True)\n",
    "        \n",
    "        model = TfidfVectorizer_gpu(stop_words='english', binary=True)\n",
    "        text_embeddings = model.fit_transform(cudf.from_pandas(country_df[col]))\n",
    "\n",
    "        model = NearestNeighbors_gpu(n_neighbors=min(len(country_df), N_NEIGHBORS), algorithm=\"brute\")\n",
    "        model.fit(text_embeddings)\n",
    "        nears = model.kneighbors(text_embeddings, return_distance=False)\n",
    "        \n",
    "        k = min(len(country_df), N_NEIGHBORS)\n",
    "        country_df['match_id'] = country_df['id'].values[nears[:, :k].get()].tolist()\n",
    "        country_df = country_df.explode(['match_id'])\n",
    "        country_df = country_df.loc[country_df['id'] != country_df['match_id']].copy()\n",
    "        dfs.append(country_df)\n",
    "    df = pd.concat(dfs).reset_index(drop=True)\n",
    "    return df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 49,
   "metadata": {},
   "outputs": [],
   "source": [
    "def add_orgin_data(df, org_df):\n",
    "    df = df.merge(org_df.add_prefix('match_'), on='match_id')\n",
    "    df = df.reset_index(drop=True)\n",
    "    return df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 50,
   "metadata": {},
   "outputs": [],
   "source": [
    "%%cython\n",
    "def LCS(str S, str T):\n",
    "    cdef int i, j\n",
    "    cdef list dp = [[0] * (len(T) + 1) for _ in range(len(S) + 1)]\n",
    "    for i in range(len(S)):\n",
    "        for j in range(len(T)):\n",
    "            dp[i + 1][j + 1] = max(dp[i][j] + (S[i] == T[j]), dp[i + 1][j], dp[i][j + 1], dp[i + 1][j + 1])\n",
    "    return dp[len(S)][len(T)]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 51,
   "metadata": {},
   "outputs": [],
   "source": [
    "%%cython\n",
    "# Optimized version\n",
    "import numpy as np\n",
    "cimport numpy as np\n",
    "\n",
    "import cython\n",
    "from libc.stdlib cimport malloc, free\n",
    "\n",
    "\n",
    "@cython.boundscheck(False) # turn off bounds-checking for entire function\n",
    "@cython.wraparound(False)  # turn off negative index wrapping for entire function\n",
    "def fast_LCS(str S, str T):\n",
    "    if len(S) < len(T):\n",
    "        S, T = T, S\n",
    "\n",
    "    cdef int i, j\n",
    "    cdef np.uint16_t[:] dp_prev, dp_curr\n",
    "    \n",
    "    dp_prev = np.zeros(len(T) + 1, dtype=np.uint16)\n",
    "    dp_curr = np.zeros(len(T) + 1, dtype=np.uint16)\n",
    "\n",
    "    for i in range(len(S)):\n",
    "        for j in range(len(T)):\n",
    "            dp_curr[j + 1]  = max(dp_prev[j] + (1 if S[i] == T[j] else 0), dp_curr[j], dp_prev[j + 1])\n",
    "        dp_prev, dp_curr = dp_curr, dp_prev\n",
    "    return dp_prev[len(T)]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 52,
   "metadata": {},
   "outputs": [],
   "source": [
    "def make_tfidf_vec(df, col):\n",
    "    df_ = pd.concat([df[col], df[\"match_\" + col]]).drop_duplicates().to_frame()\n",
    "    df_ = df_.reset_index(drop=True)\n",
    "    df_.columns = [col]\n",
    "\n",
    "    tfidf_vectorizer = TfidfVectorizer(ngram_range=(1,2), stop_words='english')\n",
    "    tfidf_vec = tfidf_vectorizer.fit_transform(df_[col].tolist())\n",
    "    tfidf_vec = {k:v for k,v in zip(df_[col].values, tfidf_vec)}\n",
    "    return tfidf_vec"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 53,
   "metadata": {},
   "outputs": [],
   "source": [
    "def _add_distance_features(args):\n",
    "    _, df = args\n",
    "\n",
    "    columns = ['name', 'address', 'city', 'state',\n",
    "               'zip', 'url', 'phone', 'categories']\n",
    "\n",
    "    for c in columns:\n",
    "        geshs = []\n",
    "        levens = []\n",
    "        jaros = []\n",
    "        lcss = []\n",
    "        tfidf_sims = []\n",
    "\n",
    "        if c in [\"name\", \"categories\"]:\n",
    "            tfidf_vec = make_tfidf_vec(df, c)\n",
    "\n",
    "\n",
    "        for str1, str2 in df[[f\"{c}\", f\"match_{c}\"]].values.astype(str):\n",
    "            if str1 != \"nan\" and str2 != \"nan\":\n",
    "                #geshs.append(difflib.SequenceMatcher(None, str1, str2).ratio())\n",
    "                #levens.append(Levenshtein.distance(str1, str2))\n",
    "                #jaros.append(Levenshtein.jaro_winkler(str1, str2))\n",
    "                #lcss.append(LCS(str(str1), str(str2)))\n",
    "\n",
    "                if c in [\"name\", \"categories\"]:\n",
    "                    sim = cosine_similarity(tfidf_vec[str1].reshape(1, -1), tfidf_vec[str2].reshape(1, -1))[0][0]\n",
    "                    tfidf_sims.append(sim)\n",
    "\n",
    "            else:\n",
    "                #geshs.append(-1)\n",
    "                #levens.append(-1)\n",
    "                #jaros.append(-1)\n",
    "                #lcss.append(-1)\n",
    "                \n",
    "                if c in [\"name\", \"categories\"]:\n",
    "                    tfidf_sims.append(-1)\n",
    "\n",
    "\n",
    "        #df[f\"match_{c}_gesh\"] = geshs\n",
    "        #df[f\"match_{c}_gesh\"] = df[f\"match_{c}_gesh\"].astype(np.float16)\n",
    "        #df[f\"match_{c}_leven\"] = levens\n",
    "        #df[f\"match_{c}_leven\"] = df[f\"match_{c}_leven\"].astype(np.float16)\n",
    "        #df[f\"match_{c}_jaro\"] = jaros\n",
    "        #df[f\"match_{c}_jaro\"] = df[f\"match_{c}_jaro\"].astype(np.float16)\n",
    "        #df[f\"match_{c}_lcs\"] = lcss\n",
    "        #df[f\"match_{c}_lcs\"] = df[f\"match_{c}_lcs\"].astype(np.float16)\n",
    "            \n",
    "        \n",
    "        #if not c in ['country', \"phone\", \"zip\"]:\n",
    "        #    df[f\"match_{c}_len\"] = df[f\"match_{c}\"].astype(str).map(len)\n",
    "        #    df[f\"{c}_len\"] = df[f\"{c}\"].astype(str).map(len)\n",
    "        #    df[f\"match_{c}_nleven\"] = df[f\"match_{c}_leven\"] / df[[f\"match_{c}_len\", f\"{c}_len\"]].max(axis=1)\n",
    "        #    df[f\"match_{c}_nleven\"] = df[f\"match_{c}_nleven\"].astype(np.float16)\n",
    "        #    df[f\"match_{c}_nlcsi\"] = df[f\"match_{c}_lcs\"] / df[f\"match_{c}_len\"]\n",
    "        #    df[f\"match_{c}_nlcs0\"] = df[f\"match_{c}_lcs\"] / df[f\"{c}_len\"]\n",
    "        #    df[f\"match_{c}_nlcsi\"] = df[f\"match_{c}_nlcsi\"].astype(np.float16)\n",
    "        #    df[f\"match_{c}_nlcs0\"] = df[f\"match_{c}_nlcs0\"].astype(np.float16)\n",
    "        #    df.drop(f'{c}_len',axis=1, inplace = True)\n",
    "        #    df.drop(f\"match_{c}_len\",axis=1, inplace = True)\n",
    "\n",
    "        if c in [\"name\", \"categories\"]:\n",
    "            df[f\"tfidf_sim_{c}\"] = tfidf_sims\n",
    "            df[f\"tfidf_sim_{c}\"] = df[f\"tfidf_sim_{c}\"].astype(np.float16)\n",
    "\n",
    "    return df\n",
    "\n",
    "\n",
    "def add_distance_features(df):\n",
    "    processes = multiprocessing.cpu_count()\n",
    "    with multiprocessing.Pool(processes=processes) as pool:\n",
    "        dfs = pool.imap_unordered(_add_distance_features, df.groupby('country'))\n",
    "        dfs = tqdm(dfs)\n",
    "        dfs = list(dfs)\n",
    "    df = pd.concat(dfs)\n",
    "    return df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 54,
   "metadata": {},
   "outputs": [],
   "source": [
    "def _add_distance_features_new(args):\n",
    "    _, df = args\n",
    "\n",
    "    columns = ['name', 'address', 'city', 'state',\n",
    "               'zip', 'url', 'phone', 'categories']\n",
    "\n",
    "    for c in columns:\n",
    "        geshs = []\n",
    "        levens = []\n",
    "        jaros = []\n",
    "        lcss = []\n",
    "        tfidf_sims = []\n",
    "\n",
    "        #if c in [\"name\", \"categories\"]:\n",
    "        #    tfidf_vec = make_tfidf_vec(df, c)\n",
    "\n",
    "\n",
    "        for str1, str2 in df[[f\"{c}\", f\"match_{c}\"]].values.astype(str):\n",
    "            if str1 != \"nan\" and str2 != \"nan\":\n",
    "                geshs.append(difflib.SequenceMatcher(None, str1, str2).ratio())\n",
    "                levens.append(Levenshtein.distance(str1, str2))\n",
    "                jaros.append(Levenshtein.jaro_winkler(str1, str2))\n",
    "                lcss.append(fast_LCS(str(str1), str(str2)))\n",
    "\n",
    "                #if c in [\"name\", \"categories\"]:\n",
    "                #    sim = cosine_similarity(tfidf_vec[str1].reshape(1, -1), tfidf_vec[str2].reshape(1, -1))[0][0]\n",
    "                #    tfidf_sims.append(sim)\n",
    "\n",
    "            else:\n",
    "                geshs.append(-1)\n",
    "                levens.append(-1)\n",
    "                jaros.append(-1)\n",
    "                lcss.append(-1)\n",
    "                \n",
    "                #if c in [\"name\", \"categories\"]:\n",
    "                #    tfidf_sims.append(-1)\n",
    "\n",
    "\n",
    "        df[f\"match_{c}_gesh\"] = geshs\n",
    "        df[f\"match_{c}_gesh\"] = df[f\"match_{c}_gesh\"].astype(np.float16)\n",
    "        df[f\"match_{c}_leven\"] = levens\n",
    "        #df[f\"match_{c}_leven\"] = df[f\"match_{c}_leven\"].astype(np.float16)\n",
    "        df[f\"match_{c}_jaro\"] = jaros\n",
    "        df[f\"match_{c}_jaro\"] = df[f\"match_{c}_jaro\"].astype(np.float16)\n",
    "        df[f\"match_{c}_lcs\"] = lcss\n",
    "        #df[f\"match_{c}_lcs\"] = df[f\"match_{c}_lcs\"].astype(np.float16)\n",
    "            \n",
    "        if not c in ['country', \"phone\", \"zip\"]:\n",
    "            df[f\"match_{c}_len\"] = df[f\"match_{c}\"].astype(str).map(len)\n",
    "            df[f\"{c}_len\"] = df[f\"{c}\"].astype(str).map(len)\n",
    "            df[f\"match_{c}_nleven\"] = df[f\"match_{c}_leven\"] / df[[f\"match_{c}_len\", f\"{c}_len\"]].max(axis=1)\n",
    "            df[f\"match_{c}_nleven\"] = df[f\"match_{c}_nleven\"].astype(np.float16)\n",
    "            df[f\"match_{c}_nlcsi\"] = df[f\"match_{c}_lcs\"] / df[f\"match_{c}_len\"]\n",
    "            df[f\"match_{c}_nlcs0\"] = df[f\"match_{c}_lcs\"] / df[f\"{c}_len\"]\n",
    "            df[f\"match_{c}_nlcsi\"] = df[f\"match_{c}_nlcsi\"].astype(np.float16)\n",
    "            df[f\"match_{c}_nlcs0\"] = df[f\"match_{c}_nlcs0\"].astype(np.float16)\n",
    "            df.drop(f'{c}_len',axis=1, inplace = True)\n",
    "            df.drop(f\"match_{c}_len\",axis=1, inplace = True)\n",
    "\n",
    "        #if c in [\"name\", \"categories\"]:\n",
    "        #    df[f\"tfidf_sim_{c}\"] = tfidf_sims\n",
    "        #    df[f\"tfidf_sim_{c}\"] = df[f\"tfidf_sim_{c}\"].astype(np.float16)\n",
    "\n",
    "    return df\n",
    "\n",
    "\n",
    "def add_distance_features_new(df):\n",
    "    processes = multiprocessing.cpu_count()\n",
    "    with multiprocessing.Pool(processes=processes) as pool:\n",
    "        dfs = pool.imap_unordered(_add_distance_features_new, df.groupby('country'))\n",
    "        dfs = tqdm(dfs)\n",
    "        dfs = list(dfs)\n",
    "    df = pd.concat(dfs)\n",
    "    return df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 55,
   "metadata": {},
   "outputs": [],
   "source": [
    "def add_vec_sim_features(df, vec, vec_name, col):\n",
    "    sim_list = []\n",
    "    for str1, str2 in tqdm(df[[f\"{col}\", f\"match_{col}\"]].values.astype(str)):\n",
    "        if str1!=\"nan\" and str2!=\"nan\":\n",
    "            sim = cosine_similarity(vec[str1].reshape(1, -1), vec[str2].reshape(1, -1))[0][0]\n",
    "        else:\n",
    "            sim = -1\n",
    "        sim_list.append(sim)\n",
    "    df[f\"{vec_name}_sim_{col}\"] = sim_list\n",
    "    df[f\"{vec_name}_sim_{col}\"] = df[f\"{vec_name}_sim_{col}\"].astype(np.float16)\n",
    "    return df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 56,
   "metadata": {},
   "outputs": [],
   "source": [
    "def add_vec_sim_features_new(df, vec, vec_name, col):\n",
    "    sim_list = []\n",
    "    for str1, str2 in tqdm(df[[f\"{col}\", f\"match_{col}\"]].values.astype(str)):\n",
    "        if str1!=\"nan\" and str2!=\"nan\":\n",
    "            sim = dot(vec[str1], vec[str2]) / (norm(vec[str1])*norm(vec[str2]))\n",
    "        else:\n",
    "            sim = -1\n",
    "        sim_list.append(sim)\n",
    "    df[f\"{vec_name}_sim_{col}\"] = sim_list\n",
    "    return df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 64,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_origin = pd.read_csv(INPUT_DIR + \"train.csv\")\n",
    "train_origin = preprocess(train_origin)\n",
    "train_origin = train_origin[train_origin[\"country\"]==\"us\"]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 65,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>id</th>\n",
       "      <th>name</th>\n",
       "      <th>latitude</th>\n",
       "      <th>longitude</th>\n",
       "      <th>address</th>\n",
       "      <th>city</th>\n",
       "      <th>state</th>\n",
       "      <th>zip</th>\n",
       "      <th>country</th>\n",
       "      <th>url</th>\n",
       "      <th>phone</th>\n",
       "      <th>categories</th>\n",
       "      <th>point_of_interest</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>8</th>\n",
       "      <td>E_00007dcd2bb53f</td>\n",
       "      <td>togo's sandwiches</td>\n",
       "      <td>0.667725</td>\n",
       "      <td>-2.130429</td>\n",
       "      <td>1380 holiday ln., ste. b</td>\n",
       "      <td>fairfield</td>\n",
       "      <td>ca</td>\n",
       "      <td>94534</td>\n",
       "      <td>us</td>\n",
       "      <td>https://locations.togos.com/ll/us/ca/fairfield...</td>\n",
       "      <td>7074394747</td>\n",
       "      <td>sandwich places</td>\n",
       "      <td>P_aae7505da98d46</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>13</th>\n",
       "      <td>E_0000d9e584ed9f</td>\n",
       "      <td>signature properties savannah</td>\n",
       "      <td>0.558725</td>\n",
       "      <td>-1.415692</td>\n",
       "      <td>100 commercial ct ste c</td>\n",
       "      <td>savannah</td>\n",
       "      <td>ga</td>\n",
       "      <td>31406</td>\n",
       "      <td>us</td>\n",
       "      <td>http://www.oursignatureproperties.com</td>\n",
       "      <td>9126292700</td>\n",
       "      <td>real estate offices</td>\n",
       "      <td>P_af856e3abdcebc</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>16</th>\n",
       "      <td>E_00011cca3f0bd6</td>\n",
       "      <td>inner workings</td>\n",
       "      <td>0.622854</td>\n",
       "      <td>-1.848987</td>\n",
       "      <td>serving general area</td>\n",
       "      <td>santa fe</td>\n",
       "      <td>nm</td>\n",
       "      <td>87501</td>\n",
       "      <td>us</td>\n",
       "      <td>http://innerworkingsmassage.com</td>\n",
       "      <td>(505) 850-0872</td>\n",
       "      <td>massage studios</td>\n",
       "      <td>P_ea131770ce8f01</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>19</th>\n",
       "      <td>E_00015cd7e0227f</td>\n",
       "      <td>lake destiny</td>\n",
       "      <td>0.499925</td>\n",
       "      <td>-1.420508</td>\n",
       "      <td>nan</td>\n",
       "      <td>maitland</td>\n",
       "      <td>fl</td>\n",
       "      <td>32751</td>\n",
       "      <td>us</td>\n",
       "      <td>nan</td>\n",
       "      <td>nan</td>\n",
       "      <td>lakes</td>\n",
       "      <td>P_dc0c160f3fcef8</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>23</th>\n",
       "      <td>E_0001968548bde2</td>\n",
       "      <td>champion</td>\n",
       "      <td>0.591404</td>\n",
       "      <td>-1.470774</td>\n",
       "      <td>3700 dekalb technology pkwy</td>\n",
       "      <td>atlanta</td>\n",
       "      <td>ga</td>\n",
       "      <td>30340</td>\n",
       "      <td>us</td>\n",
       "      <td>nan</td>\n",
       "      <td>nan</td>\n",
       "      <td>nan</td>\n",
       "      <td>P_2dd4afac52e8b8</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>...</th>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1138789</th>\n",
       "      <td>E_fffe88fc908438</td>\n",
       "      <td>jerns funeral chapel</td>\n",
       "      <td>0.851201</td>\n",
       "      <td>-2.137385</td>\n",
       "      <td>800 e sunset dr</td>\n",
       "      <td>bellingham</td>\n",
       "      <td>wa</td>\n",
       "      <td>98225</td>\n",
       "      <td>us</td>\n",
       "      <td>nan</td>\n",
       "      <td>+13607340070</td>\n",
       "      <td>funeral homes</td>\n",
       "      <td>P_9d7d732431075d</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1138797</th>\n",
       "      <td>E_ffff59c16a4c36</td>\n",
       "      <td>walgreens</td>\n",
       "      <td>0.675700</td>\n",
       "      <td>-1.578709</td>\n",
       "      <td>12509 dorsett rd</td>\n",
       "      <td>maryland heights</td>\n",
       "      <td>mo</td>\n",
       "      <td>63043</td>\n",
       "      <td>us</td>\n",
       "      <td>https://www.walgreens.com/locator/walgreens-12...</td>\n",
       "      <td>3144344224</td>\n",
       "      <td>pharmacies, convenience stores, photography labs</td>\n",
       "      <td>P_525103ba9777c2</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1138798</th>\n",
       "      <td>E_ffff62eb945bd0</td>\n",
       "      <td>mi casita</td>\n",
       "      <td>0.847041</td>\n",
       "      <td>-2.147166</td>\n",
       "      <td>nan</td>\n",
       "      <td>friday harbor</td>\n",
       "      <td>wa</td>\n",
       "      <td>98250</td>\n",
       "      <td>us</td>\n",
       "      <td>nan</td>\n",
       "      <td>nan</td>\n",
       "      <td>bars</td>\n",
       "      <td>P_af5567d03a4472</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1138806</th>\n",
       "      <td>E_ffff989ae206f8</td>\n",
       "      <td>cricket wireless authorized retailer</td>\n",
       "      <td>0.630598</td>\n",
       "      <td>-1.513473</td>\n",
       "      <td>825c murfreesboro pike</td>\n",
       "      <td>nashville</td>\n",
       "      <td>tn</td>\n",
       "      <td>37217</td>\n",
       "      <td>us</td>\n",
       "      <td>https://www.cricketwireless.com/stores/ll/us/t...</td>\n",
       "      <td>+16156799940</td>\n",
       "      <td>mobile phone shops</td>\n",
       "      <td>P_caad31263268d4</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1138808</th>\n",
       "      <td>E_ffffbf9a83e0ba</td>\n",
       "      <td>deshon place</td>\n",
       "      <td>0.713353</td>\n",
       "      <td>-1.395309</td>\n",
       "      <td>325 new castle rd</td>\n",
       "      <td>butler</td>\n",
       "      <td>pa</td>\n",
       "      <td>16001</td>\n",
       "      <td>us</td>\n",
       "      <td>nan</td>\n",
       "      <td>nan</td>\n",
       "      <td>housing developments</td>\n",
       "      <td>P_db0abc418e7365</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>245284 rows × 13 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "                       id                                  name  latitude  \\\n",
       "8        E_00007dcd2bb53f                     togo's sandwiches  0.667725   \n",
       "13       E_0000d9e584ed9f         signature properties savannah  0.558725   \n",
       "16       E_00011cca3f0bd6                        inner workings  0.622854   \n",
       "19       E_00015cd7e0227f                          lake destiny  0.499925   \n",
       "23       E_0001968548bde2                              champion  0.591404   \n",
       "...                   ...                                   ...       ...   \n",
       "1138789  E_fffe88fc908438                  jerns funeral chapel  0.851201   \n",
       "1138797  E_ffff59c16a4c36                             walgreens  0.675700   \n",
       "1138798  E_ffff62eb945bd0                             mi casita  0.847041   \n",
       "1138806  E_ffff989ae206f8  cricket wireless authorized retailer  0.630598   \n",
       "1138808  E_ffffbf9a83e0ba                          deshon place  0.713353   \n",
       "\n",
       "         longitude                      address              city state  \\\n",
       "8        -2.130429     1380 holiday ln., ste. b         fairfield    ca   \n",
       "13       -1.415692      100 commercial ct ste c          savannah    ga   \n",
       "16       -1.848987         serving general area          santa fe    nm   \n",
       "19       -1.420508                          nan          maitland    fl   \n",
       "23       -1.470774  3700 dekalb technology pkwy           atlanta    ga   \n",
       "...            ...                          ...               ...   ...   \n",
       "1138789  -2.137385              800 e sunset dr        bellingham    wa   \n",
       "1138797  -1.578709             12509 dorsett rd  maryland heights    mo   \n",
       "1138798  -2.147166                          nan     friday harbor    wa   \n",
       "1138806  -1.513473       825c murfreesboro pike         nashville    tn   \n",
       "1138808  -1.395309            325 new castle rd            butler    pa   \n",
       "\n",
       "           zip country                                                url  \\\n",
       "8        94534      us  https://locations.togos.com/ll/us/ca/fairfield...   \n",
       "13       31406      us              http://www.oursignatureproperties.com   \n",
       "16       87501      us                    http://innerworkingsmassage.com   \n",
       "19       32751      us                                                nan   \n",
       "23       30340      us                                                nan   \n",
       "...        ...     ...                                                ...   \n",
       "1138789  98225      us                                                nan   \n",
       "1138797  63043      us  https://www.walgreens.com/locator/walgreens-12...   \n",
       "1138798  98250      us                                                nan   \n",
       "1138806  37217      us  https://www.cricketwireless.com/stores/ll/us/t...   \n",
       "1138808  16001      us                                                nan   \n",
       "\n",
       "                  phone                                        categories  \\\n",
       "8            7074394747                                   sandwich places   \n",
       "13           9126292700                               real estate offices   \n",
       "16       (505) 850-0872                                   massage studios   \n",
       "19                  nan                                             lakes   \n",
       "23                  nan                                               nan   \n",
       "...                 ...                                               ...   \n",
       "1138789    +13607340070                                     funeral homes   \n",
       "1138797      3144344224  pharmacies, convenience stores, photography labs   \n",
       "1138798             nan                                              bars   \n",
       "1138806    +16156799940                                mobile phone shops   \n",
       "1138808             nan                              housing developments   \n",
       "\n",
       "        point_of_interest  \n",
       "8        P_aae7505da98d46  \n",
       "13       P_af856e3abdcebc  \n",
       "16       P_ea131770ce8f01  \n",
       "19       P_dc0c160f3fcef8  \n",
       "23       P_2dd4afac52e8b8  \n",
       "...                   ...  \n",
       "1138789  P_9d7d732431075d  \n",
       "1138797  P_525103ba9777c2  \n",
       "1138798  P_af5567d03a4472  \n",
       "1138806  P_caad31263268d4  \n",
       "1138808  P_db0abc418e7365  \n",
       "\n",
       "[245284 rows x 13 columns]"
      ]
     },
     "execution_count": 65,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "train_origin"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 66,
   "metadata": {},
   "outputs": [],
   "source": [
    "col = \"name\"\n",
    "df_ = train_origin[[\"name\"]].drop_duplicates().reset_index(drop=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 67,
   "metadata": {},
   "outputs": [],
   "source": [
    "tfidf_vectorizer = TfidfVectorizer(ngram_range=(1,2), stop_words='english')\n",
    "tfidf_vec = tfidf_vectorizer.fit_transform(df_[col].tolist())\n",
    "tfidf_vec = {k:v for k,v in zip(df_[col].values, tfidf_vec)}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 73,
   "metadata": {},
   "outputs": [],
   "source": [
    "str1 = \"lake destiny\"\n",
    "str2 = \"lake lenape\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 74,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0.10350873637967552"
      ]
     },
     "execution_count": 74,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "cosine_similarity(tfidf_vec[str1].reshape(1, -1), tfidf_vec[str2].reshape(1, -1))[0][0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 78,
   "metadata": {},
   "outputs": [],
   "source": [
    "from scipy.sparse.linalg import norm as norm_sparse"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 93,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0.10350873637967552"
      ]
     },
     "execution_count": 93,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "(tfidf_vec[str1].dot(tfidf_vec[str2].T) / (norm_sparse(tfidf_vec[str1]) * norm_sparse(tfidf_vec[str2]))).toarray()[0][0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 94,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "463 µs ± 2 µs per loop (mean ± std. dev. of 7 runs, 1000 loops each)\n"
     ]
    }
   ],
   "source": [
    "%%timeit\n",
    "sim = cosine_similarity(tfidf_vec[str1].reshape(1, -1), tfidf_vec[str2].reshape(1, -1))[0][0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 95,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "517 µs ± 1.93 µs per loop (mean ± std. dev. of 7 runs, 1000 loops each)\n"
     ]
    }
   ],
   "source": [
    "%%timeit\n",
    "sim = (tfidf_vec[str1].dot(tfidf_vec[str2].T) / (norm_sparse(tfidf_vec[str1]) * norm_sparse(tfidf_vec[str2]))).toarray()[0][0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([0., 0., 0., ..., 0., 0., 0.])"
      ]
     },
     "execution_count": 30,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "tfidf_vec[str2].toarray()[0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 63,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0.0"
      ]
     },
     "execution_count": 63,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {},
   "outputs": [
    {
     "ename": "ValueError",
     "evalue": "dimension mismatch",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mValueError\u001b[0m                                Traceback (most recent call last)",
      "\u001b[0;32m/tmp/ipykernel_4454/1316587351.py\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[0;32m----> 1\u001b[0;31m \u001b[0mtfidf_vec\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mstr2\u001b[0m\u001b[0;34m]\u001b[0m \u001b[0;34m*\u001b[0m \u001b[0mtfidf_vec\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mstr2\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m",
      "\u001b[0;32m/opt/conda/lib/python3.8/site-packages/scipy/sparse/base.py\u001b[0m in \u001b[0;36m__mul__\u001b[0;34m(self, other)\u001b[0m\n\u001b[1;32m    477\u001b[0m         \u001b[0;32mif\u001b[0m \u001b[0missparse\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mother\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    478\u001b[0m             \u001b[0;32mif\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mshape\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;36m1\u001b[0m\u001b[0;34m]\u001b[0m \u001b[0;34m!=\u001b[0m \u001b[0mother\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mshape\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;36m0\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 479\u001b[0;31m                 \u001b[0;32mraise\u001b[0m \u001b[0mValueError\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m'dimension mismatch'\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    480\u001b[0m             \u001b[0;32mreturn\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_mul_sparse_matrix\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mother\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    481\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mValueError\u001b[0m: dimension mismatch"
     ]
    }
   ],
   "source": [
    "tfidf_vec[str2] * tfidf_vec[str2]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [],
   "source": [
    "from numpy import dot\n",
    "from numpy.linalg import norm"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 43,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "16.6 µs ± 20.8 ns per loop (mean ± std. dev. of 7 runs, 100000 loops each)\n"
     ]
    }
   ],
   "source": [
    "%%timeit\n",
    "sim = dot(bert_vec_name[str1], bert_vec_name[str2]) / (norm(bert_vec_name[str1])*norm(bert_vec_name[str2]))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 56,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 151/151 [00:04<00:00, 35.83it/s]\n"
     ]
    }
   ],
   "source": [
    "# trainデータの分割\n",
    "kf = GroupKFold(n_splits=20)\n",
    "for i, (trn_idx, val_idx) in enumerate(kf.split(train_origin, train_origin['point_of_interest'], train_origin['point_of_interest'])):\n",
    "    train_origin.loc[val_idx, \"set\"] = i\n",
    "\n",
    "# 1st stage\n",
    "train = extract_candidate_dist(train_origin[train_origin[\"set\"]==0])\n",
    "train = add_orgin_data(train, train_origin)\n",
    "\n",
    "old = train.copy()\n",
    "new = train.copy()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 57,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 511642/511642 [01:02<00:00, 8229.64it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "CPU times: user 1min 2s, sys: 565 ms, total: 1min 3s\n",
      "Wall time: 1min 2s\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    }
   ],
   "source": [
    "%%time\n",
    "old = add_vec_sim_features(old, bert_vec_name, \"bert\", \"name\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 58,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 511642/511642 [00:09<00:00, 51399.36it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "CPU times: user 10.2 s, sys: 95.8 ms, total: 10.3 s\n",
      "Wall time: 10.2 s\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    }
   ],
   "source": [
    "%%time\n",
    "new = add_vec_sim_features_new(new, bert_vec_name, \"bert\", \"name\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 59,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0         1.000000\n",
       "1         0.415771\n",
       "2         0.461182\n",
       "3         0.461182\n",
       "4         0.529785\n",
       "            ...   \n",
       "511637    0.563965\n",
       "511638    0.545410\n",
       "511639    0.472656\n",
       "511640    0.451660\n",
       "511641    0.451660\n",
       "Name: bert_sim_name, Length: 511642, dtype: float16"
      ]
     },
     "execution_count": 59,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "old[\"bert_sim_name\"]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 60,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0         1.000000\n",
       "1         0.415771\n",
       "2         0.461182\n",
       "3         0.461182\n",
       "4         0.529785\n",
       "            ...   \n",
       "511637    0.563965\n",
       "511638    0.545410\n",
       "511639    0.472656\n",
       "511640    0.451660\n",
       "511641    0.451660\n",
       "Name: bert_sim_name, Length: 511642, dtype: float16"
      ]
     },
     "execution_count": 60,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "new[\"bert_sim_name\"]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "interpreter": {
   "hash": "d4d1e4263499bec80672ea0156c357c1ee493ec2b1c70f0acce89fc37c4a6abe"
  },
  "kernelspec": {
   "display_name": "Python 3.8.12 ('base')",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.12"
  },
  "orig_nbformat": 4
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
