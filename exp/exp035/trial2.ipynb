{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# exp034_trial\n",
    "距離計算の高速化検討"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "MODE = 'local_train'\n",
    "#MODE = 'kaggle_inference'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "exp_name = 'exp032'\n",
    "memo = 'exp030 dist10+tfidf_name_sim10の1st'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import sys\n",
    "import gc\n",
    "\n",
    "if MODE == 'local_train':\n",
    "    sys.path.append('/home/kaggler/.local/lib/python3.8/site-packages')\n",
    "    from dotenv import load_dotenv\n",
    "    load_dotenv\n",
    "    sys.path.append(os.getenv('UTILS_PATH'))\n",
    "    import line_notify\n",
    "    import slack_notify\n",
    "    \n",
    "if MODE == \"kaggle_inference\":\n",
    "    from cuml import ForestInference\n",
    "    import treelite\n",
    "    \n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt; plt.style.use(\"ggplot\")\n",
    "import seaborn as sns\n",
    "from sklearn.metrics.pairwise import haversine_distances\n",
    "from sklearn.model_selection import GroupKFold, KFold, StratifiedKFold\n",
    "from sklearn.metrics import accuracy_score\n",
    "from sklearn.decomposition import TruncatedSVD\n",
    "import lightgbm as lgb\n",
    "import itertools\n",
    "from scipy.spatial.distance import canberra\n",
    "from sklearn.neighbors import KNeighborsRegressor\n",
    "import functools\n",
    "import multiprocessing\n",
    "import Levenshtein\n",
    "import difflib\n",
    "import pickle\n",
    "from tqdm import tqdm\n",
    "%load_ext Cython\n",
    "\n",
    "from transformers import DistilBertModel, DistilBertTokenizer\n",
    "from torch.utils.data import DataLoader, Dataset\n",
    "import torch.nn.functional as F\n",
    "import torch.nn as nn\n",
    "import torch\n",
    "\n",
    "from sklearn.metrics.pairwise import cosine_similarity\n",
    "from sklearn.feature_extraction.text import TfidfVectorizer\n",
    "\n",
    "from cuml.feature_extraction.text import TfidfVectorizer as TfidfVectorizer_gpu\n",
    "import cudf, cuml, cupy\n",
    "from cuml.neighbors import NearestNeighbors as NearestNeighbors_gpu"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "# directry_setting\n",
    "if MODE == 'local_train':\n",
    "    INPUT_DIR = os.getenv('INPUT_DIR')\n",
    "    OUTPUT_DIR = os.getenv('OUTPUT_DIR')\n",
    "    MODEL_DIR = os.getenv('OUTPUT_DIR')\n",
    "    BERT_MODEL = \"distilbert-base-multilingual-cased\"\n",
    "    os.makedirs(OUTPUT_DIR + exp_name, exist_ok=True)\n",
    "\n",
    "elif MODE == 'kaggle_inference':\n",
    "    INPUT_DIR = '/kaggle/input/foursquare-location-matching/'\n",
    "    OUTPUT_DIR = './'\n",
    "    MODEL_DIR = f'../input/fs{exp_name}/'\n",
    "    BERT_MODEL = \"../input/distilbertbaseuncased\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "# CONFIG\n",
    "SEED = 42\n",
    "N_NEIGHBORS = 10\n",
    "N_SPLITS = 5\n",
    "PROB_TH = 0.5\n",
    "MAX_LEN = 32\n",
    "BS = 512\n",
    "NW = 2\n",
    "SVD_N_COMP = 50"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "class Cat2VecModel(nn.Module):\n",
    "    def __init__(self):\n",
    "        super(Cat2VecModel, self).__init__()\n",
    "        self.distill_bert = DistilBertModel.from_pretrained(BERT_MODEL)\n",
    "        \n",
    "    def forward(self, ids, mask):\n",
    "        x = self.distill_bert(ids, mask)[0]\n",
    "        x = F.normalize((x[:, 1:, :]*mask[:, 1:, None]).mean(axis=1))\n",
    "        return x\n",
    "\n",
    "class InferenceDataset(Dataset):\n",
    "    \n",
    "    def __init__(self, df, max_len, col):\n",
    "        super().__init__()\n",
    "        self.df = df.reset_index(drop=True)\n",
    "        self.max_len = max_len\n",
    "        self.tokenizer = DistilBertTokenizer.from_pretrained(BERT_MODEL, do_lower_case=True)\n",
    "        self.col = col\n",
    "\n",
    "    def __getitem__(self, index):\n",
    "        row = self.df.iloc[index]\n",
    "        \n",
    "        inputs = self.tokenizer.encode_plus(\n",
    "            row[self.col],\n",
    "            None,\n",
    "            add_special_tokens=True,\n",
    "            max_length=MAX_LEN,\n",
    "            padding=\"max_length\",\n",
    "            return_token_type_ids=True,\n",
    "            truncation=True\n",
    "        )\n",
    "        ids = torch.LongTensor(inputs['input_ids'])\n",
    "        mask = torch.LongTensor(inputs['attention_mask'])\n",
    "\n",
    "        return ids, mask\n",
    "\n",
    "    def __len__(self):\n",
    "        return self.df.shape[0]\n",
    "\n",
    "def inference(ds):\n",
    "    cat2vec_model = Cat2VecModel()\n",
    "    cat2vec_model = cat2vec_model.cuda()\n",
    "    \n",
    "    loader = DataLoader(ds, batch_size=BS, shuffle=False, num_workers=NW,\n",
    "                        pin_memory=False, drop_last=False)\n",
    "    \n",
    "    vs = []\n",
    "    with torch.no_grad():\n",
    "        for idx, (ids, masks) in enumerate(loader):\n",
    "            v = cat2vec_model(ids.cuda(), masks.cuda()).detach().cpu().numpy()\n",
    "            vs.append(v)\n",
    "    return np.concatenate(vs)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "def make_bert_vec(df, col):\n",
    "    cat_df = df[[col]].drop_duplicates()\n",
    "    cat_df[col] = cat_df[col].fillna(\"null\")\n",
    "\n",
    "    cat_ds = InferenceDataset(cat_df, max_len=MAX_LEN, col=col)\n",
    "    V = inference(cat_ds)\n",
    "    #svd = TruncatedSVD(n_components=SVD_N_COMP, random_state=SEED)\n",
    "    #V = svd.fit_transform(V)\n",
    "    V = V.astype(\"float16\")\n",
    "    bert_vec = {k:v for k,v in zip(cat_df[col].values, V)}\n",
    "    return bert_vec"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "def preprocess(df):\n",
    "    columns = ['id', 'name', 'address', 'city', 'state',\n",
    "        'zip', 'country', 'url', 'phone', 'categories']\n",
    "    for c in columns:\n",
    "        if c != \"id\":\n",
    "            df[c] = df[c].astype(str).str.lower()\n",
    "\n",
    "    df[[\"latitude\", \"longitude\"]] = np.deg2rad(df[[\"latitude\", \"longitude\"]])\n",
    "    \n",
    "    return df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "def extract_candidate_dist(df):\n",
    "    dfs = []\n",
    "    for country, country_df in tqdm(df.groupby(\"country\")):\n",
    "        country_df = country_df.reset_index(drop=True)\n",
    "        \n",
    "        knn = KNeighborsRegressor(n_neighbors=min(len(country_df), N_NEIGHBORS), \n",
    "                                    metric='haversine', n_jobs=-1)\n",
    "        knn.fit(country_df[['latitude','longitude']], country_df.index)\n",
    "        nears = knn.kneighbors(country_df[['latitude','longitude']], return_distance=False)\n",
    "        \n",
    "        k = min(len(country_df), N_NEIGHBORS)\n",
    "        country_df['match_id'] = country_df['id'].values[nears[:, :k]].tolist()\n",
    "        country_df = country_df.explode(['match_id'])\n",
    "        country_df = country_df.loc[country_df['id'] != country_df['match_id']].copy()\n",
    "        dfs.append(country_df)\n",
    "    df = pd.concat(dfs).reset_index(drop=True)\n",
    "    return df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "def extract_candidate_tfidf_sim(df, col):\n",
    "    dfs = []\n",
    "    for country, country_df in tqdm(df.groupby(\"country\")):\n",
    "        country_df = country_df[country_df[col]!=\"nan\"].copy()\n",
    "        if len(country_df) < 2:\n",
    "            continue\n",
    "\n",
    "        country_df = country_df.reset_index(drop=True)\n",
    "        \n",
    "        model = TfidfVectorizer_gpu(stop_words='english', binary=True)\n",
    "        text_embeddings = model.fit_transform(cudf.from_pandas(country_df[col]))\n",
    "\n",
    "        model = NearestNeighbors_gpu(n_neighbors=min(len(country_df), N_NEIGHBORS), algorithm=\"brute\")\n",
    "        model.fit(text_embeddings)\n",
    "        nears = model.kneighbors(text_embeddings, return_distance=False)\n",
    "        \n",
    "        k = min(len(country_df), N_NEIGHBORS)\n",
    "        country_df['match_id'] = country_df['id'].values[nears[:, :k].get()].tolist()\n",
    "        country_df = country_df.explode(['match_id'])\n",
    "        country_df = country_df.loc[country_df['id'] != country_df['match_id']].copy()\n",
    "        dfs.append(country_df)\n",
    "    df = pd.concat(dfs).reset_index(drop=True)\n",
    "    return df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "def add_orgin_data(df, org_df):\n",
    "    df = df.merge(org_df.add_prefix('match_'), on='match_id')\n",
    "    df = df.reset_index(drop=True)\n",
    "    return df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "%%cython\n",
    "def LCS(str S, str T):\n",
    "    cdef int i, j\n",
    "    cdef list dp = [[0] * (len(T) + 1) for _ in range(len(S) + 1)]\n",
    "    for i in range(len(S)):\n",
    "        for j in range(len(T)):\n",
    "            dp[i + 1][j + 1] = max(dp[i][j] + (S[i] == T[j]), dp[i + 1][j], dp[i][j + 1], dp[i + 1][j + 1])\n",
    "    return dp[len(S)][len(T)]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "def make_tfidf_vec(df, col):\n",
    "    df_ = pd.concat([df[col], df[\"match_\" + col]]).drop_duplicates().to_frame()\n",
    "    df_ = df_.reset_index(drop=True)\n",
    "    df_.columns = [col]\n",
    "\n",
    "    tfidf_vectorizer = TfidfVectorizer(ngram_range=(1,2), stop_words='english')\n",
    "    tfidf_vec = tfidf_vectorizer.fit_transform(df_[col].tolist())\n",
    "    tfidf_vec = {k:v for k,v in zip(df_[col].values, tfidf_vec)}\n",
    "    return tfidf_vec"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "def _add_distance_features(args):\n",
    "    _, df = args\n",
    "\n",
    "    columns = ['name', 'address', 'city', 'state',\n",
    "               'zip', 'url', 'phone', 'categories']\n",
    "\n",
    "    for c in columns:\n",
    "        geshs = []\n",
    "        levens = []\n",
    "        jaros = []\n",
    "        lcss = []\n",
    "        tfidf_sims = []\n",
    "\n",
    "        #if c in [\"name\", \"categories\"]:\n",
    "        #    tfidf_vec = make_tfidf_vec(df, c)\n",
    "\n",
    "\n",
    "        for str1, str2 in df[[f\"{c}\", f\"match_{c}\"]].values.astype(str):\n",
    "            if str1 != \"nan\" and str2 != \"nan\":\n",
    "                geshs.append(difflib.SequenceMatcher(None, str1, str2).ratio())\n",
    "                levens.append(Levenshtein.distance(str1, str2))\n",
    "                jaros.append(Levenshtein.jaro_winkler(str1, str2))\n",
    "                lcss.append(LCS(str(str1), str(str2)))\n",
    "\n",
    "                #if c in [\"name\", \"categories\"]:\n",
    "                #    sim = cosine_similarity(tfidf_vec[str1].reshape(1, -1), tfidf_vec[str2].reshape(1, -1))[0][0]\n",
    "                #    tfidf_sims.append(sim)\n",
    "\n",
    "            else:\n",
    "                geshs.append(-1)\n",
    "                levens.append(-1)\n",
    "                jaros.append(-1)\n",
    "                lcss.append(-1)\n",
    "                \n",
    "                #if c in [\"name\", \"categories\"]:\n",
    "                #    tfidf_sims.append(-1)\n",
    "\n",
    "\n",
    "        df[f\"match_{c}_gesh\"] = geshs\n",
    "        df[f\"match_{c}_gesh\"] = df[f\"match_{c}_gesh\"].astype(np.float16)\n",
    "        df[f\"match_{c}_leven\"] = levens\n",
    "        #df[f\"match_{c}_leven\"] = df[f\"match_{c}_leven\"].astype(np.float16)\n",
    "        df[f\"match_{c}_jaro\"] = jaros\n",
    "        df[f\"match_{c}_jaro\"] = df[f\"match_{c}_jaro\"].astype(np.float16)\n",
    "        df[f\"match_{c}_lcs\"] = lcss\n",
    "        #df[f\"match_{c}_lcs\"] = df[f\"match_{c}_lcs\"].astype(np.float16)\n",
    "            \n",
    "        if not c in ['country', \"phone\", \"zip\"]:\n",
    "            df[f\"match_{c}_len\"] = df[f\"match_{c}\"].astype(str).map(len)\n",
    "            df[f\"{c}_len\"] = df[f\"{c}\"].astype(str).map(len)\n",
    "            df[f\"match_{c}_nleven\"] = df[f\"match_{c}_leven\"] / df[[f\"match_{c}_len\", f\"{c}_len\"]].max(axis=1)\n",
    "            df[f\"match_{c}_nleven\"] = df[f\"match_{c}_nleven\"].astype(np.float16)\n",
    "            df[f\"match_{c}_nlcsi\"] = df[f\"match_{c}_lcs\"] / df[f\"match_{c}_len\"]\n",
    "            df[f\"match_{c}_nlcs0\"] = df[f\"match_{c}_lcs\"] / df[f\"{c}_len\"]\n",
    "            df[f\"match_{c}_nlcsi\"] = df[f\"match_{c}_nlcsi\"].astype(np.float16)\n",
    "            df[f\"match_{c}_nlcs0\"] = df[f\"match_{c}_nlcs0\"].astype(np.float16)\n",
    "            df.drop(f'{c}_len',axis=1, inplace = True)\n",
    "            df.drop(f\"match_{c}_len\",axis=1, inplace = True)\n",
    "\n",
    "        #if c in [\"name\", \"categories\"]:\n",
    "        #    df[f\"tfidf_sim_{c}\"] = tfidf_sims\n",
    "        #    df[f\"tfidf_sim_{c}\"] = df[f\"tfidf_sim_{c}\"].astype(np.float16)\n",
    "\n",
    "    return df\n",
    "\n",
    "\n",
    "def add_distance_features(df):\n",
    "    processes = multiprocessing.cpu_count()\n",
    "    with multiprocessing.Pool(processes=processes) as pool:\n",
    "        dfs = pool.imap_unordered(_add_distance_features, df.groupby('country'))\n",
    "        dfs = tqdm(dfs)\n",
    "        dfs = list(dfs)\n",
    "    df = pd.concat(dfs)\n",
    "    return df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [],
   "source": [
    "def _calc_gesh(row, col1, col2):\n",
    "    return difflib.SequenceMatcher(None, row[col1], row[col2]).ratio()\n",
    "\n",
    "def _calc_leven(row, col1, col2):\n",
    "    return Levenshtein.distance(row[col1], row[col2])\n",
    "\n",
    "def _calc_jaro(row, col1, col2):\n",
    "    return Levenshtein.jaro_winkler(row[col1], row[col2])\n",
    "\n",
    "def _calc_lcs(row, col1, col2):\n",
    "    return LCS(row[col1], row[col2])\n",
    "\n",
    "def _add_distance_features_new(args):\n",
    "    _, df = args\n",
    "\n",
    "    columns = ['name', 'address', 'city', 'state',\n",
    "               'zip', 'url', 'phone', 'categories']\n",
    "\n",
    "    for c in columns:\n",
    "        df[f\"match_{c}_gesh\"] = df.apply(_calc_gesh, args=(c, f\"match_{c}\"), axis=1)\n",
    "        df[f\"match_{c}_leven\"] = df.apply(_calc_leven, args=(c, f\"match_{c}\"), axis=1)\n",
    "        df[f\"match_{c}_jaro\"] = df.apply(_calc_jaro, args=(c, f\"match_{c}\"), axis=1)\n",
    "        df[f\"match_{c}_lcs\"] = df.apply(_calc_lcs, args=(c, f\"match_{c}\"), axis=1)\n",
    "\n",
    "        df[f\"match_{c}_gesh\"] = df[f\"match_{c}_gesh\"].astype(np.float16)\n",
    "        df[f\"match_{c}_jaro\"] = df[f\"match_{c}_jaro\"].astype(np.float16)\n",
    "\n",
    "        df.loc[(df[c]==\"nan\") | (df[f\"match_{c}\"]==\"nan\"), [f\"match_{c}_gesh\", f\"match_{c}_leven\", f\"match_{c}_lcs\", f\"match_{c}_jaro\"]] = -1\n",
    "            \n",
    "        if not c in ['country', \"phone\", \"zip\"]:\n",
    "            df[f\"match_{c}_len\"] = df[f\"match_{c}\"].astype(str).map(len)\n",
    "            df[f\"{c}_len\"] = df[f\"{c}\"].astype(str).map(len)\n",
    "            df[f\"match_{c}_nleven\"] = df[f\"match_{c}_leven\"] / df[[f\"match_{c}_len\", f\"{c}_len\"]].max(axis=1)\n",
    "            df[f\"match_{c}_nleven\"] = df[f\"match_{c}_nleven\"].astype(np.float16)\n",
    "            df[f\"match_{c}_nlcsi\"] = df[f\"match_{c}_lcs\"] / df[f\"match_{c}_len\"]\n",
    "            df[f\"match_{c}_nlcs0\"] = df[f\"match_{c}_lcs\"] / df[f\"{c}_len\"]\n",
    "            df[f\"match_{c}_nlcsi\"] = df[f\"match_{c}_nlcsi\"].astype(np.float16)\n",
    "            df[f\"match_{c}_nlcs0\"] = df[f\"match_{c}_nlcs0\"].astype(np.float16)\n",
    "            df.drop(f'{c}_len',axis=1, inplace = True)\n",
    "            df.drop(f\"match_{c}_len\",axis=1, inplace = True)\n",
    "\n",
    "        #if c in [\"name\", \"categories\"]:\n",
    "        #    df[f\"tfidf_sim_{c}\"] = tfidf_sims\n",
    "        #    df[f\"tfidf_sim_{c}\"] = df[f\"tfidf_sim_{c}\"].astype(np.float16)\n",
    "\n",
    "    return df\n",
    "\n",
    "\n",
    "def add_distance_features_new(df):\n",
    "    processes = multiprocessing.cpu_count()\n",
    "    with multiprocessing.Pool(processes=processes) as pool:\n",
    "        dfs = pool.imap_unordered(_add_distance_features_new, df.groupby('country'))\n",
    "        dfs = tqdm(dfs)\n",
    "        dfs = list(dfs)\n",
    "    df = pd.concat(dfs)\n",
    "    return df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 151/151 [00:04<00:00, 35.18it/s]\n"
     ]
    }
   ],
   "source": [
    "train_origin = pd.read_csv(INPUT_DIR + \"train.csv\")\n",
    "train_origin = preprocess(train_origin)\n",
    "\n",
    "# trainデータの分割\n",
    "kf = GroupKFold(n_splits=20)\n",
    "for i, (trn_idx, val_idx) in enumerate(kf.split(train_origin, train_origin['point_of_interest'], train_origin['point_of_interest'])):\n",
    "    train_origin.loc[val_idx, \"set\"] = i\n",
    "\n",
    "# 1st stage\n",
    "train = extract_candidate_dist(train_origin[train_origin[\"set\"]==0])\n",
    "train = add_orgin_data(train, train_origin)\n",
    "\n",
    "old = train.copy()\n",
    "new = train.copy()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "136it [00:35,  3.81it/s]\n"
     ]
    }
   ],
   "source": [
    "old = add_distance_features(old)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "136it [01:16,  1.77it/s]\n"
     ]
    }
   ],
   "source": [
    "new = add_distance_features_new(new)"
   ]
  }
 ],
 "metadata": {
  "interpreter": {
   "hash": "d4d1e4263499bec80672ea0156c357c1ee493ec2b1c70f0acce89fc37c4a6abe"
  },
  "kernelspec": {
   "display_name": "Python 3.8.12 ('base')",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.12"
  },
  "orig_nbformat": 4
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
